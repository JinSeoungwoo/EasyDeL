{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EasyDeL \ud83d\udd2e","text":"<p>EasyDeL, an open-source library, is specifically designed to enhance and streamline the training process of machine learning models. It focuses primarily on Jax/Flax and aims to provide convenient and effective solutions for training Flax/Jax Models on TPU/GPU for both Serving and Training purposes. Additionally, EasyDeL will support mojo and will be rewritten for mojo as well.</p> <p>Some of the key features provided by EasyDeL include:</p> <ul> <li>Support for 8, 6, and 4 BIT inference and training in JAX</li> <li>Integration of flashAttention in JAX for GPUs and TPUs</li> <li>Automatic serving of LLMs with mid and high-level APIs in both JAX and PyTorch</li> <li>LLM Trainer and fine-tuner in JAX</li> <li>RLHF (presumably Reinforcement Learning with Hybrid Functions) in Jax</li> <li>And various other features to enhance the training process and optimize performance.</li> </ul> <p>These features collectively aim to simplify and accelerate the training of machine learning models, making it more efficient and accessible for developers working with Jax/Flax.</p>"},{"location":"#what-makes-easydel-special","title":"What Makes EasyDeL \ud83d\udd2e Special","text":"<p>EasyDeL is built up on JAX and Flax and that's why EasyDeL can perform as fast and as easy as possible</p> <p>When comparing JAX to PyTorch and TensorFlow, there are several benefits to using JAX that are worth considering.</p> <ol> <li> <p>Performance: JAX provides excellent performance through its XLA (Accelerated Linear Algebra) backend, which can    optimize and compile your code for various hardware accelerators such as GPUs and TPUs. This can lead to significant    speed improvements for certain types of computations.</p> </li> <li> <p>Automatic Differentiation: JAX offers a powerful and flexible automatic differentiation system, which is    essential for training machine learning models. It allows for both forward-mode and reverse-mode automatic    differentiation, giving you more options for gradient computation.</p> </li> <li> <p>Functional Programming: JAX is built around functional programming concepts, which can lead to more composable    and modular code. This can make it easier to reason about your code and to create abstractions that are reusable    across different parts of your project.</p> </li> <li> <p>Interoperability with NumPy: JAX is designed to be compatible with NumPy, which means that you can often take    existing NumPy code and run it with minimal changes on JAX. This can be a significant advantage when transitioning    existing codebases to use JAX.</p> </li> <li> <p>Flexibility: JAX provides a high degree of flexibility, allowing you to drop down to lower-level abstractions    when needed. This can be particularly useful when implementing custom operations or experimenting with new research    ideas.</p> </li> </ol> <p>While JAX offers these benefits, it's important to note that PyTorch and TensorFlow have large and active communities, extensive libraries, and a wide range of pre-trained models, which can be advantageous in certain scenarios. Additionally, the choice of framework often depends on the specific requirements of the project and the familiarity of the team with a particular toolset.</p>"},{"location":"AvailableModels/","title":"Available Models","text":"Models FP16/FP32/BF16 DP FSDP MP FlashAttn Gradient Checkpointing 8/6/4Bit Interface and Training Llama \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Mistral \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Llama2 \u2705 \u2705 \u2705 \u2705 \u274c \u2705 \u2705 GPT-J \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 LT \u2705 \u2705 \u2705 \u2705 \u274c \u2705 \u274c MosaicMPT \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 GPTNeoX-J \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c Falcon \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Palm \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \ud83c\udf2a\ufe0f T5 \u2705 \u2705 \u2705 \u2705 \u274c \u2705 \ud83c\udf2a\ufe0f OPT \u2705 \u2705 \u2705 \u2705 \u274c \u2705 \u274c <p>you can also tell me the model you want in Flax/Jax version and ill try my best to build it ;)</p>"},{"location":"AvailableModels/#current-update","title":"Current Update","text":"<p>Some of the models supported by EasyDel will support 8,6,4 bit interface and Train these following models will be supported</p> <ul> <li>[X] Llama (Supported via <code>LlamaConfig(bits=8)</code> or <code>LlamaConfig(bits=4)</code>)</li> <li>[X] Falcon (Supported via <code>FalconConfig(bits=8)</code> or <code>FalconConfig(bits=4)</code>)</li> <li>[X] Mistral (Supported via <code>MistalConfig(bits=8)</code> or <code>MistalConfig(bits=4)</code>)</li> <li>[ ] Palm</li> <li>[ ] T5</li> <li>[X] MosaicGPT / MPT (Supported via <code>MptConfig(bits=8)</code> or <code>MptConfig(bits=4)</code>)</li> <li>[X] GPT-J (Supported via <code>GPTJConfig(bits=8)</code> or <code>GPTJConfig(bits=4)</code>)</li> </ul> <p>all the models in future will have the 8,6 and 4 bit Inference and Training</p>"},{"location":"Bits/","title":"EasyBIT","text":""},{"location":"Bits/#about-bits-in-easydel","title":"About Bits in EasyDel","text":"<p>In easydel bits are totally different from huggingface and in EasyDel training model with 8 bit is supported too without needs to change the code just change the bit and that's all you have todo but by the way you still have to pass the dtype and param_dtype cause unlike the transformers and bitsandbytes which store parameters in int8 and do operations in float16, bfloat16, float32 we don't do that like this in Jax we still store parameters as float16,bfloat16 or float32 and do operations in bits like 8 6 4, and you can still train your model in this way and make it much more accurate than bitsandbytes or peft fine-tuning</p> <p>Right now im looking to make EasyBITs in EasyDel work on TPU-v3 cause on low amp GPUs and old TPUs it might now work as good as it does on TPU-v4/5</p>"},{"location":"DataProcessing/","title":"DataProcessing","text":""},{"location":"DataProcessing/#data-processing","title":"Data Processing","text":"<p>here in this case you will see an example data required by EasyDel to pre-train or fine-tune models</p> <pre><code>from datasets import load_dataset\nfrom EasyDel.data_preprocessing import DataProcessor, DataProcessorArguments\nfrom transformers import LlamaTokenizerFast\n\n\ndef main():\n    tokenizer = LlamaTokenizerFast.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n    dataset = load_dataset(\"erfanzar/orca-lite\")\n    print(dataset)\n\n    #     DatasetDict({\n    #         train: Dataset({\n    #             features: ['user', 'gpt', 'system', 'llama_2_prompt_style', 'prompt_length'],\n    #             num_rows: 101397\n    #         })\n    #     })\n\n    processor_arguments = DataProcessorArguments(\n        max_position_embeddings=2048,\n        num_proc=6,\n        prompt_field='llama_2_prompt_style',\n\n    )\n\n    easydel_dataset = DataProcessor.process_data(\n        data=dataset['train'],\n        tokenizer=tokenizer,\n        arguments=processor_arguments,\n        field='train'\n    )\n    print(easydel_dataset)\n    # DatasetDict({\n    #     train: Dataset({\n    #         features: ['input_ids', 'attention_mask'],\n    #         num_rows: 101397\n    #     })\n    # })\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>now you can pass this data to Trainer and train your model \ud83d\ude07.</p>"},{"location":"Falcon/","title":"About Falcon Models","text":"<p>Sure, here is a document about Falcon Models:</p> <p>Falcon Models</p> <p>Falcon Models is a family of large language models (LLMs) developed by the Technology Innovation Institute (TII) in Abu Dhabi. The models are trained on a massive dataset of text and code, and can be used for a variety of tasks, including</p> <ul> <li>Natural language understanding (NLU)</li> <li>Natural language generation (NLG)</li> <li>Machine translation</li> <li>Text summarization</li> <li>Question answering</li> <li>Code generation</li> </ul> <p>The Falcon models are available under the Apache 2.0 license, which means that they can be freely used, modified, and redistributed.</p> <p>Falcon-40B</p> <p>The Falcon-40B is the largest model in the Falcon family. It has 40 billion parameters, and is trained on a dataset of 500 billion words. The model is capable of state-of-the-art performance on a variety of NLP tasks.</p> <p>Falcon-7B</p> <p>The Falcon-7B is a smaller version of the Falcon-40B. It has 7 billion parameters, and is trained on a dataset of 100 billion words. The model is still capable of achieving strong performance on NLP tasks, but it is more efficient to train and deploy.</p> <p>Falcon-180B</p> <p>The Falcon-180B is the newest model in the Falcon family. It has 180 billion parameters, and is trained on a dataset of 2 trillion words. The model is the largest openly available LLM, and it is capable of achieving state-of-the-art performance on a variety of NLP tasks.</p> <p>Use Cases</p> <p>The Falcon models can be used for a variety of tasks, including:</p> <ul> <li>Natural language understanding (NLU): The Falcon models can be used to understand the meaning of text, such as   identifying the entities and relationships in a sentence.</li> <li>Natural language generation (NLG): The Falcon models can be used to generate text, such as writing different kinds of   creative content, like poems, code, scripts, musical pieces, email, letters, etc.</li> <li>Machine translation: The Falcon models can be used to translate text from one language to another.</li> <li>Text summarization: The Falcon models can be used to summarize a text document into a shorter, more concise version.</li> <li>Question answering: The Falcon models can be used to answer questions about a text document.</li> <li>Code generation: The Falcon models can be used to generate code, such as Python scripts or Java classes.</li> </ul> <p>Availability</p> <p>The Falcon models are available through the Hugging Face Hub. The models are also available in the TensorFlow Hub and the PyTorch Hub ( and EasyDel).</p> <p>Conclusion</p> <p>The Falcon models are a powerful family of LLMs that can be used for a variety of tasks. The models are open source and available for free, making them a valuable resource for researchers and developers.</p>"},{"location":"Falcon/#how-to-useload-them-in-easydel","title":"How to Use/Load Them in EasyDel","text":"<pre><code>import jax\nfrom EasyDel.transform import falcon_from_pretrained\n\nparams, config = falcon_from_pretrained(\n    'tiiuae/falcon-7b',\n    device  # Offload on CPU\n)\n</code></pre> <p>also keep that in mind that returned <code>config</code> includes <code>.get_partition_rules(fsdp=True)</code></p>"},{"location":"Falcon/#use-with-jaxserver","title":"Use With JaxServer","text":"<pre><code>from EasyDel.serve import JAXServer\nfrom EasyDel.modules import FlaxFalconForCausalLM\nimport jax\nfrom EasyDel.transform import falcon_from_pretrained\nfrom transformers import AutoTokenizer\n\nparams, config = falcon_from_pretrained(\n    'tiiuae/falcon-7b',\n    device  # Offload on CPU\n)\n\n\nclass FalconJaxServer(JAXServer):\n    ...\n    # You have to Custom this one yourself as you \n    # need read JaxServer Documents inorder to learn how\n\n\nserver = FalconJaxServer.load_from_params(\n    params=params,\n    model=FlaxFalconForCausalLM(\n        config=config,\n        dtype=jax.numpy.bfloat16,  # Im on TPUs\n        param_dtype=jax.numpy.bfloat16,  # Im on TPUs\n        precision=jax.lax.Precision('fastest'),\n        _do_init=False,\n        input_shape=(1, 1024)\n    ),\n    config_model=config,\n    add_params_field=True,\n    tokenizer=AutoTokenizer.from_pretrained('tiiuae/falcon-7b'),\n    verbose=False,\n    do_memory_log=True,\n    config={\n        \"max_length\": 2048,\n        \"max_new_tokens\": 2048,\n        \"max_stream_tokens\": 64,\n        \"dtype\": 'bf16',\n        \"use_prefix_tokenizer\": True,\n        'pre_compile': True\n    }\n)\n\nserver.fire()  # Launch FastAPI functions\n\nshared_urls = server.launch(\n    share_chat=True,\n    share_inst=True\n)\n</code></pre> <p>Done \ud83d\ude07 this method can be used for all the Falcon models</p>"},{"location":"Install/","title":"Installing EasyDeL","text":"<p>EasyDeL uses FJFormer and JAX as main dependecies in order to run the scripts but there are some things that needs to be installed such as GO-lang to JAX specific platform installations, but you can simply install EasyDeL via pip:</p> <pre><code>pip install easydel\n</code></pre>"},{"location":"Install/#installing-jax","title":"Installing Jax","text":"<p>JAX uses XLA to compile and run your NumPy programs on GPUs and TPUs. Compilation happens under the hood by default, with library calls getting just-in-time compiled and executed. But JAX also lets you just-in-time compile your own Python functions into XLA-optimized kernels using a one-function API, jit.</p> <p>you can install other version too but easydel required at least version of 0.4.16</p>"},{"location":"Install/#tpu","title":"TPU","text":"<pre><code>!pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html -q\n</code></pre>"},{"location":"Install/#gpu","title":"GPU","text":""},{"location":"Install/#cuda-12","title":"CUDA-12","text":"<pre><code>pip install --upgrade pip\n# CUDA 12 installation\n# Note: wheels only available on linux.\npip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n</code></pre>"},{"location":"Install/#cuda-11","title":"CUDA-11","text":"<pre><code>pip install --upgrade pip\n# CUDA 11 installation\n# Note: wheels only available on linux.\npip install --upgrade \"jax[cuda11_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n</code></pre>"},{"location":"Install/#installing-go","title":"Installing GO","text":""},{"location":"Install/#note-this-library-needs-golang-to-run-for-some-tracking-stuff-on-tpugpucpu","title":"Note this Library needs golang to run (for some tracking stuff on TPU/GPU/CPU)","text":""},{"location":"Install/#ubuntu-go-installation","title":"Ubuntu GO installation","text":"<pre><code>sudo apt-get update &amp;&amp; apt-get upgrade -y\nsudo apt-get install golang -y \n</code></pre>"},{"location":"Install/#manjaroarch-go-installation","title":"Manjaro/Arch GO installation","text":"<pre><code>sudo pacman -Syyuu go\n</code></pre>"},{"location":"JAXServer/","title":"JAXServer","text":""},{"location":"JAXServer/#jaxserver","title":"JAXServer \ud83e\uddec","text":"<p><code>JAXServer</code> is one of offered utilities by EasyDel, and it's help hosting using and doing process with LLMs and its also hackable, so you can override your own method in it and use it support both mid-level and high-level apis and also give you a Gradio Chat and Instruct Pre-build and ready to use page</p> <ul> <li>Supported Models are:<ul> <li>EveryModel that have <code>transformers.FlaxPretrainedModel</code> as their Parent :)</li> </ul> </li> </ul>"},{"location":"JAXServer/#input-configs","title":"Input Configs","text":"<p>The config input is a dictionary that contains the following keys:</p> <ul> <li><code>port</code>: The port number that the server will listen on.<ul> <li>Default Value has been set to  <code>2059</code></li> </ul> </li> <li><code>batch_size</code>: The batch size for training.<ul> <li>Default Value has been set to  <code>1</code></li> </ul> </li> <li><code>max_length</code>: The maximum length of a sequence.<ul> <li>Default Value has been set to  <code>2048</code></li> </ul> </li> <li><code>max_new_tokens</code>: The maximum number of new tokens generated by the model in a single step.<ul> <li>Default Value has been set to  <code>2048</code></li> </ul> </li> <li><code>max_stream_tokens</code>: The maximum number of tokens that can be streamed to the model in a single batch.<ul> <li>Default Value has been set to  <code>32</code></li> </ul> </li> <li><code>temperature</code>: The temperature parameter for sampling from the model's output distribution.<ul> <li>Default Value has been set to  <code>0.1</code></li> </ul> </li> <li><code>top_p</code>: The top-p parameter for sampling from the model's output distribution.<ul> <li>Default Value has been set to  <code>0.95</code></li> </ul> </li> <li><code>top_k</code>: The top-k parameter for sampling from the model's output distribution.<ul> <li>Default Value has been set to  <code>50</code></li> </ul> </li> <li><code>mesh_axes_shape</code>: The shape of the mesh axes for distributed training.<ul> <li>Default Value has been set to  <code>(1, -1, 1, 1)</code></li> </ul> </li> <li><code>host</code>: The host address for the server.<ul> <li>Default Value has been set to  <code>'0.0.0.0'</code></li> </ul> </li> <li><code>dtype</code>: The data type for the model's parameters.<ul> <li>Default Value has been set to  <code>'fp16'</code></li> </ul> </li> <li><code>mesh_axes_names</code>: The names of the mesh axes for distributed training.<ul> <li>Default Value has been set to  <code>(\"dp\", \"fsdp\", \"tp\", \"mp\")</code></li> </ul> </li> <li><code>logging</code>: Whether the model should log its training progress.:<ul> <li>Default Value has been set to  <code>True</code></li> </ul> </li> <li><code>stream_tokens_for_gradio</code>: Whether the model should stream tokens to Gradio.<ul> <li>Default Value has been set to  <code>True</code></li> </ul> </li> <li><code>use_prefix_tokenizer</code>: Whether the model should use a prefix tokenizer.<ul> <li>Default Value has been set to  <code>True</code></li> </ul> </li> <li><code>pre_compile</code>: Whether the model should be pre-compiled.<ul> <li>Default Value has been set to  <code>True</code></li> </ul> </li> </ul>"},{"location":"JAXServer/#jaxserver-functions","title":"JAXServer Functions","text":"<p><code>JAXServer</code> has <code>format_chat</code> and <code>format_instruct</code> funcs that you have to implement them to prompt your model</p> <pre><code>@staticmethod\ndef format_instruct(system: str, instruction: str)-&gt;str:\n    \"\"\"\n    Here you will get the system and instruction from user, and you can apply your prompting style\n    \"\"\"\n    raise NotImplementedError()\n\n@staticmethod\ndef format_chat(history: typing.List[str], prompt: str, system: typing.Union[str, None])-&gt;str:\n    \"\"\"\n    Here you will get the system, prompt and history from user, and you can apply your prompting style\n    \"\"\"\n    raise NotImplementedError()\n</code></pre> <p><code>JAXServer</code> Contains a method named <code>.process</code> and with using <code>process</code> method you can generate text from text</p> <p>what does this do and how this works ? here's the inputs that <code>process</code> function takes in</p> <pre><code>def process(self,\n            string,\n            *,\n            greedy: bool = False,\n            max_new_tokens: int = None,\n            **kwargs\n            ) -&gt; [str, int]:\n    ...\n</code></pre> <ul> <li>Arguments:<ul> <li>string : String to be tokenized <code>(String)</code></li> <li>Greedy : Use Greedy Search Method or NO <code>(Bool)</code></li> <li>Max New Tokens : Number Of new Tokens to be Generated <code>(Int)</code></li> </ul> </li> <li>Yields:<ul> <li>String : Next Tokens Predicted to String <code>(String)</code></li> <li>Number of Used Tokens : Number of Used Tokens to generate answer <code>(Int)</code></li> </ul> </li> </ul> <p>you can use this function outside the class like this</p> <pre><code>for string, num_used_tokens in server.process(\n        'im a string',\n        greedy=False,\n        max_new_tokens=256  # or None to use Maximum numbers passed in Config\n):\n    print(f'\\r{num_used_tokens}: {string}', end='')\n</code></pre>"},{"location":"JAXServer/#gradio-functions","title":"Gradio Functions \ud83e\udd16","text":"<p>if you want to change gradio response functions you can override them like this</p>"},{"location":"JAXServer/#chat-gradio-function","title":"Chat Gradio Function","text":"<p>this is the default gradio functions and this is how it looks :</p> <pre><code>def process_gradio_chat(self, prompt, history, max_new_tokens, system, greedy):\n    string = self.chat_format(history=history, prompt=prompt, system=system)\n\n    if not self.config.stream_tokens_for_gradio:\n        response = ''\n        for response, _ in self.process(\n                string=string,\n                greedy=greedy,\n                max_new_tokens=max_new_tokens,\n        ):\n            ...\n        history.append([prompt, response])\n    else:\n        history.append([prompt, ''])\n        for response, _ in self.process(\n                string=string,\n                greedy=greedy,\n                max_new_tokens=max_new_tokens,\n        ):\n            history[-1][-1] = response\n            yield '', history\n    return '', history\n</code></pre> <p>and here's a example of changing that in order to use Llama Models</p> <pre><code>def process_gradio_chat(self, prompt, history, max_new_tokens, system, greedy):\n    def prompt_llama2_model(message: str, chat_history,\n                            system_prompt: str) -&gt; str:\n\n        do_strip = False\n        texts = [f'&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\n{system_prompt}\\n&lt;&lt;/SYS&gt;&gt;\\n\\n']\n        for user_input, response in chat_history:\n            user_input = user_input.strip() if do_strip else user_input\n            do_strip = True\n            texts.append(f'{user_input} [/INST] {response.strip()} &lt;/s&gt;&lt;s&gt;[INST] ')\n        message = message.strip() if do_strip else message\n        texts.append(f'{message} [/INST]')\n        return ''.join(texts)\n\n    string = prompt_llama2_model(\n        message=prompt,\n        chat_history=history or [],\n        system_prompt=system\n    )\n    if not self.config.stream_tokens_for_gradio:\n        response = ''\n        for response, _ in self.process(\n                string=string,\n                greedy=greedy,\n                max_new_tokens=max_new_tokens,\n        ):\n            ...\n        history.append([prompt, response])\n    else:\n        history.append([prompt, ''])\n        for response, _ in self.process(\n                string=string,\n                greedy=greedy,\n                max_new_tokens=max_new_tokens\n        ):\n            history[-1][-1] = response\n            yield '', history\n\n    return '', history\n\n</code></pre> <p>as you see you can easily override the functions just like how you want and use them with some simple changes, and you can Also Use Their <code>Gradio Client</code> or use <code>JAXServer</code> <code>FastAPI</code> builtin methods</p>"},{"location":"JAXServer/#fastapi","title":"FastAPI \ud83c\udf2a","text":""},{"location":"JAXServer/#instruct-api","title":"Instruct API","text":"<p>to Override this api you have to code <code>forward_instruct</code> just like what you want the default implementation of this function is</p> <pre><code>def forward_instruct(self, data: InstructRequest):\n    if not self._funcs_generated:\n        return {\n            'status': \"down\"\n        }\n\n    string = self.config.instruct_format.format(instruct=data.prompt, system=data.system)\n    response, used_tokens = [None] * 2\n    for response, used_tokens in self.process(\n            string=string,\n            greedy=data.greedy,\n            max_new_tokens=None\n    ):\n        ...\n    self.number_of_served_request_until_last_up_time += 1\n    return {\n        'input': f'{string}',\n        'response': response,\n        'tokens_used': used_tokens,\n    }\n</code></pre> <ul> <li>BaseModel Class For PYData in FastAPI :</li> </ul> <pre><code>class InstructRequest(BaseModel):\n    prompt: str\n    system: Optional[str] = None\n    temperature: Optional[float] = None\n    greedy: Optional[bool] = False\n</code></pre> <ul> <li>And here's an example of using this api via python and creating a simple client with using <code>requests</code> library in   python :</li> </ul> <pre><code>import requests\n\ncontent = {\n    'prompt': 'can you code a simple neural network in c++ for me',\n    'system': 'You are an AI assistant generate short and useful response',\n    'temperature': 0.1,\n    'greedy': False\n}\n\nresponse = requests.post(\n    url='http://ip:port/instruct',\n    json=content\n).json()\n\nprint(response['response'])\n# Response of model\nprint(response['input'])\n# The input passed to the model\n\n</code></pre>"},{"location":"JAXServer/#chat-api","title":"Chat API","text":"<p>to Override this api you have to code <code>forward_chat</code> just like what you want the default implementation of this function is</p> <pre><code>def forward_chat(self, data: ChatRequest):\n    if not self._funcs_generated:\n        return {\n            'status': \"down\"\n        }\n\n    history = self.process_chat_history(data.history or [])\n    history += self.config.prompt_prefix_chat + data.prompt + self.config.prompt_postfix_chat\n\n    response, used_tokens = [None] * 2\n    for response, used_tokens in self.process(\n            string=history,\n            greedy=data.greedy,\n            max_new_tokens=None\n    ):\n        ...\n    self.number_of_served_request_until_last_up_time += 1\n    return {\n        'input': f'{history}',\n        'response': response,\n        'tokens_used': used_tokens,\n    }\n</code></pre> <ul> <li>BaseModel Class For PYData in FastAPI :</li> </ul> <pre><code>class ChatRequest(BaseModel):\n    prompt: str\n    history: Union[List[List], None] = None\n    temperature: Optional[float] = None\n    greedy: Optional[bool] = False\n</code></pre> <ul> <li>And here's an example of using this api via python and creating a simple client with using <code>requests</code> library in   python :</li> </ul> <pre><code>import requests\n\ncontent = {\n    'prompt': 'can you code a simple neural network in c++ for me',\n    'history': [\n        ['hello how are you', 'Hello\\nthanks, im here to assist you you have any question that i could help you with']\n    ],\n    'temperature': 0.1,\n    'greedy': False\n}\n\nresponse = requests.post(\n    url='http://ip:port/chat',\n    json=content\n).json()\n\nprint(response['response'])\n# Response of model\nprint(response['input'])\n# The input passed to the model\n\n</code></pre>"},{"location":"JAXServer/#status","title":"Status \ud83d\udce3","text":"<p>Simply by sending a get API to <code>https://ip:port/status</code> you will receive base information about the server and how it being run, num cores in use, number of generated prompt , number of request and ...</p>"},{"location":"Llama/","title":"About Llama Models","text":"<ul> <li>Introduction</li> </ul> <p>Llama models are a family of large language models (LLMs) developed by Meta AI. They are trained on a massive dataset of text and code, and they can be used for a variety of tasks, such as text generation, translation, summarization, question answering, code generation, and natural language inference.</p> <ul> <li>Model Architecture</li> </ul> <p>Llama models are based on the Transformer architecture, which is a neural network architecture that has been shown to be very effective for natural language processing tasks. The Transformer architecture uses self-attention to learn long-range dependencies between words in a sentence.</p> <ul> <li>Training Data</li> </ul> <p>Llama models are trained on a massive dataset of text and code. The text dataset includes text from a variety of sources, such as books, articles, and websites. The code dataset includes code from a variety of programming languages, such as Python, Java, and C++.</p> <ul> <li>Fine-tuning</li> </ul> <p>After being pre-trained on a massive dataset, Llama models can be fine-tuned for specific tasks. Fine-tuning involves training the model on a smaller dataset of data that is relevant to the specific task.</p> <ul> <li>Applications</li> </ul> <p>Llama models can be used for a variety of tasks, such as:</p> <pre><code>* Text generation: Llama models can be used to generate text, such as poems, code, scripts, and musical pieces.\n* Translation: Llama models can be used to translate text from one language to another.\n* Summarization: Llama models can be used to summarize text.\n* Question answering: Llama models can be used to answer questions about text.\n* Code generation: Llama models can be used to generate code.\n* Natural language inference: Llama models can be used to determine the relationship between two sentences.\n</code></pre> <ul> <li>Availability</li> </ul> <p>Llama models are available for free for research and commercial use. They can be downloaded from the Hugging Face Hub.</p> <ul> <li>Limitations</li> </ul> <p>Llama models are still under development, and they have some limitations. For example, they can sometimes generate incorrect or misleading text. They can also be biased, reflecting the biases that are present in the training data.</p> <ul> <li>Future Work</li> </ul> <p>Llama models are a promising new technology with the potential to be used for a variety of applications. Future work on Llama models will focus on improving their accuracy, reducing their bias, and making them more robust to errors.</p> <ul> <li>Text generation</li> <li>Translation</li> <li>Summarization</li> <li>Question answering</li> <li>Code generation</li> <li>Natural language inference</li> </ul> <p>Here is a table comparing the different sizes of Llama models:</p> Model Parameters Llama 7B 7 billion Llama 13B 13 billion Llama 33B 33 billion Llama 65B 65 billion Llama 70B 70 billion"},{"location":"Llama/#how-to-useload-them-in-easydel","title":"How to Use/Load Them in EasyDel","text":"<pre><code>import jax\nfrom EasyDel.transform import llama_from_pretrained\n\nparams, config = llama_from_pretrained(\n    'meta-llama/Llama-2-7b',\n    device  # Offload on CPU\n)\n</code></pre> <p>also keep that in mind that returned <code>config</code> includes <code>.get_partition_rules(fsdp=True)</code></p>"},{"location":"Llama/#use-with-jaxserver","title":"Use With JaxServer","text":"<pre><code>from EasyDel.serve import JAXServer\nfrom EasyDel.modules.llama import FlaxLlamaForCausalLM\nimport jax\nfrom EasyDel.transform import llama_from_pretrained\nfrom transformers import AutoTokenizer\n\nparams, config = llama_from_pretrained(\n    'meta-llama/Llama-2-7b',\n    device  # Offload on CPU\n)\n\nDEFAULT_SYSTEM_PROMPT = \"You are a helpful, respectful and honest assistant and act as wanted\"\n\n\nclass Llama2JaxServer(JAXServer):\n    def process_gradio_chat(self, prompt, history, max_new_tokens, system, greedy):\n\n        system = None if system == '' else system\n        string = self.prompt_llama2_model(\n            message=prompt,\n            chat_history=history or [],\n            system_prompt=system or DEFAULT_SYSTEM_PROMPT\n        )\n        if not self.config.stream_tokens_for_gradio:\n            response = ''\n            for response, _ in self.process(\n                    string=string,\n                    greedy=greedy,\n                    max_new_tokens=max_new_tokens,\n            ):\n                ...\n            history.append([prompt, response])\n        else:\n            history.append([prompt, ''])\n            for response, _ in self.process(\n                    string=string,\n                    greedy=greedy,\n                    max_new_tokens=max_new_tokens\n            ):\n                history[-1][-1] = response\n                yield '', history\n\n        return '', history\n\n    def process_gradio_instruct(self, prompt, system, max_new_tokens, greedy):\n        string = self.prompt_llama2_model(system_prompt=DEFAULT_SYSTEM_PROMPT, message=prompt, chat_history=[])\n        if not self.config.stream_tokens_for_gradio:\n            response = ''\n            for response, _ in self.process(\n                    string=string,\n                    greedy=greedy,\n                    max_new_tokens=max_new_tokens,\n            ):\n                pass\n        else:\n            response = ''\n            for response, _ in self.process(\n                    string=string,\n                    greedy=greedy,\n                    max_new_tokens=max_new_tokens,\n                    stream=True\n            ):\n                yield '', response\n        return '', response\n\n    @staticmethod\n    def prompt_llama2_model(message: str, chat_history,\n                            system_prompt: str) -&gt; str:\n\n        do_strip = False\n        texts = [f'&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\n{system_prompt}\\n&lt;&lt;/SYS&gt;&gt;\\n\\n']\n        for user_input, response in chat_history:\n            user_input = user_input.strip() if do_strip else user_input\n            do_strip = True\n            texts.append(f'{user_input} [/INST] {response.strip()} &lt;/s&gt;&lt;s&gt;[INST] ')\n        message = message.strip() if do_strip else message\n        texts.append(f'{message} [/INST]')\n        return ''.join(texts)\n\n\nserver = Llama2JaxServer.load_from_params(\n    params=params,\n    model=FlaxLlamaForCausalLM(\n        config=config,\n        dtype=jax.numpy.bfloat16,  # Im on TPUs\n        param_dtype=jax.numpy.bfloat16,  # Im on TPUs\n        precision=jax.lax.Precision('fastest'),\n        _do_init=False,\n        input_shape=(1, 1024)\n    ),\n    config_model=config,\n    add_params_field=True,\n    tokenizer=AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b'),\n    verbose=False,\n    do_memory_log=True,\n    config={\n        \"max_length\": 4096,\n        \"max_new_tokens\": 4096,\n        \"max_stream_tokens\": 64,\n        \"dtype\": 'bf16',\n        \"use_prefix_tokenizer\": True,\n        'pre_compile': True\n    }\n)\n\nserver.fire()  # Launch FastAPI functions\n\nshared_urls = server.launch(\n    share_chat=True,\n    share_inst=True\n)\n</code></pre> <p>Done \ud83d\ude07 this method can be used for all the llama models</p>"},{"location":"Llama2/","title":"Llama2 Models","text":""},{"location":"Llama2/#about-llama2-models","title":"About Llama2 Models","text":"<p>Llama2 Models</p> <p>Llama2 Models is a family of pretrained and fine-tuned large language models (LLMs) developed by Meta AI. The models are trained on a massive dataset of text and code, and can be used for a variety of tasks, including</p> <ul> <li>Natural language understanding (NLU)</li> <li>Natural language generation (NLG)</li> <li>Machine translation</li> <li>Text summarization</li> <li>Question answering</li> <li>Code generation</li> </ul> <p>The Llama2 models are available under the Apache 2.0 license, which means that they can be freely used, modified, and redistributed.</p> <p>Model Architecture</p> <p>The Llama2 models are based on the Transformer architecture, which is a neural network architecture that has been shown to be very effective for NLP tasks. The models are trained using a technique called masked language modeling, which involves predicting the missing words in a sequence of text.</p> <p>Model Sizes</p> <p>The Llama2 models come in a variety of sizes, ranging from 7 billion to 70 billion parameters. The larger models have more capacity to learn complex patterns in language, but they are also more computationally expensive to train and deploy.</p> <p>Fine-tuning</p> <p>The Llama2 models are pretrained on a massive dataset of text and code, but they can be further fine-tuned on a specific task to improve their performance. Fine-tuning involves training the model on a dataset of labeled data for the specific task.</p> <p>Use Cases</p> <p>The Llama2 models can be used for a variety of tasks, including:</p> <ul> <li>Natural language understanding (NLU): The Llama2 models can be used to understand the meaning of text, such as   identifying the entities and relationships in a sentence.</li> <li>Natural language generation (NLG): The Llama2 models can be used to generate text, such as writing different kinds of   creative content, like poems, code, scripts, musical pieces, email, letters, etc.</li> <li>Machine translation: The Llama2 models can be used to translate text from one language to another.</li> <li>Text summarization: The Llama2 models can be used to summarize a text document into a shorter, more concise version.</li> <li>Question answering: The Llama2 models can be used to answer questions about a text document.</li> <li>Code generation: The Llama2 models can be used to generate code, such as Python scripts or Java classes.</li> </ul> <p>Availability</p> <p>The Llama2 models are available through the Hugging Face Hub. The models are also available in the TensorFlow Hub , the PyTorch Hub and EasyDel.</p> <p>Conclusion</p> <p>The Llama2 models are a powerful family of LLMs that can be used for a variety of tasks. The models are open source and available for free, making them a valuable resource for researchers and developers.</p>"},{"location":"Llama2/#how-to-useload-them-in-easydel","title":"How to Use/Load Them in EasyDel","text":"<pre><code>import jax\nfrom EasyDel.transform import llama_from_pretrained\n\nparams, config = llama_from_pretrained(\n    'meta-llama/Llama-2-7b',\n    device=jax.devices('cpu')[0]  # Offload on CPU\n)\n</code></pre> <p>also keep that in mind that returned <code>config</code> includes <code>.get_partition_rules(fsdp=True)</code></p>"},{"location":"Llama2/#use-with-jaxserver","title":"Use With JaxServer","text":"<pre><code>from EasyDel.modules import FlaxLlamaForCausalLM\nfrom EasyDel.serve import JAXServer\nimport jax\nfrom EasyDel.transform import llama_from_pretrained\nfrom transformers import AutoTokenizer\n\nparams, config = llama_from_pretrained(\n    'meta-llama/Llama-2-7b',\n    device=jax.devices('cpu')[0]  # Offload on CPU\n)\n\nDEFAULT_SYSTEM_PROMPT = \"You are a helpful, respectful and honest assistant and act as wanted\"\n\n\nclass Llama2JaxServer(JAXServer):\n    def process_gradio_chat(self, prompt, history, max_new_tokens, system, greedy):\n\n        system = None if system == '' else system\n        string = self.prompt_llama2_model(\n            message=prompt,\n            chat_history=history or [],\n            system_prompt=system or DEFAULT_SYSTEM_PROMPT\n        )\n        if not self.config.stream_tokens_for_gradio:\n            response = ''\n            for response, _ in self.process(\n                    string=string,\n                    greedy=greedy,\n                    max_new_tokens=max_new_tokens,\n            ):\n                ...\n            history.append([prompt, response])\n        else:\n            history.append([prompt, ''])\n            for response, _ in self.process(\n                    string=string,\n                    greedy=greedy,\n                    max_new_tokens=max_new_tokens\n            ):\n                history[-1][-1] = response\n                yield '', history\n\n        return '', history\n\n    def process_gradio_instruct(self, prompt, system, max_new_tokens, greedy):\n        string = self.prompt_llama2_model(system_prompt=DEFAULT_SYSTEM_PROMPT, message=prompt, chat_history=[])\n        if not self.config.stream_tokens_for_gradio:\n            response = ''\n            for response, _ in self.process(\n                    string=string,\n                    greedy=greedy,\n                    max_new_tokens=max_new_tokens,\n            ):\n                pass\n        else:\n            response = ''\n            for response, _ in self.process(\n                    string=string,\n                    greedy=greedy,\n                    max_new_tokens=max_new_tokens,\n                    stream=True\n            ):\n                yield '', response\n        return '', response\n\n    @staticmethod\n    def prompt_llama2_model(message: str, chat_history,\n                            system_prompt: str) -&gt; str:\n\n        do_strip = False\n        texts = [f'&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\n{system_prompt}\\n&lt;&lt;/SYS&gt;&gt;\\n\\n']\n        for user_input, response in chat_history:\n            user_input = user_input.strip() if do_strip else user_input\n            do_strip = True\n            texts.append(f'{user_input} [/INST] {response.strip()} &lt;/s&gt;&lt;s&gt;[INST] ')\n        message = message.strip() if do_strip else message\n        texts.append(f'{message} [/INST]')\n        return ''.join(texts)\n\n\nserver = Llama2JaxServer.load_from_params(\n    params=params,\n    model=FlaxLlamaForCausalLM(\n        config=config,\n        dtype=jax.numpy.bfloat16,  # Im on TPUs\n        param_dtype=jax.numpy.bfloat16,  # Im on TPUs\n        precision=jax.lax.Precision('fastest'),\n        _do_init=False,\n        input_shape=(1, 1024)\n    ),\n    config_model=config,\n    add_params_field=True,\n    tokenizer=AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b'),\n    verbose=False,\n    do_memory_log=True,\n    config={\n        \"max_length\": 4096,\n        \"max_new_tokens\": 4096,\n        \"max_stream_tokens\": 64,\n        \"dtype\": 'bf16',\n        \"use_prefix_tokenizer\": True,\n        'pre_compile': True\n    }\n)\n\nserver.fire()  # Launch FastAPI functions\n\nshared_urls = server.launch(\n    share_chat=True,\n    share_inst=True\n)\n</code></pre> <p>Done \ud83d\ude07 this method can be used for all the llama2 models</p>"},{"location":"Mistral/","title":"Mistral Models","text":""},{"location":"Mistral/#mistral-models","title":"Mistral Models","text":"<p>Mistral LLM models. Mistral AI is a French startup that develops large language models (LLMs). Mistral's first LLM, Mistral-7B-v0.1, was released in October 2023. It is a 7 billion parameter decoder-based LM with a number of architectural innovations, including sliding window attention, grouped query attention, and byte-fallback BPE tokenizer. Mistral-7B-v0.1 has been shown to achieve state-of-the-art performance on a number of NLP benchmarks, including GLUE, SuperGLUE, and the Stanford Question Answering Dataset.</p> <p>Mistral AI has not yet released a commercial version of Mistral-7B-v0.1, but it is available for free download and evaluation. The company is also working on developing larger and more powerful LLMs, including a 100 billion parameter model.</p> <p>Mistral's LLMs have been praised for their ability to generate creative and informative text, as well as their ability to perform a wide range of NLP tasks, such as translation, question answering, and summarization. However, some concerns have been raised about the potential for Mistral's LLMs to be used to generate harmful content, such as instructions on how to make bombs or how to self-harm.</p> <p>Overall, Mistral AI is a promising startup in the field of LLM development. Its LLMs have the potential to be used in a wide range of applications, such as customer service, education, and creative writing. However, it is important to be aware of the potential risks associated with using LLMs, such as the risk of generating harmful content.</p> <p>README.md</p> <p>Mistral LLM models</p> <p>Mistral LLM models are a set of large language models (LLMs) developed by Mistral AI, a French startup. Mistral's LLMs are trained on massive datasets of text and code, and can be used to perform a variety of NLP tasks, including:</p> <ul> <li>Text generation</li> <li>Translation</li> <li>Question answering</li> <li>Summarization</li> <li>Code generation</li> <li>Creative writing</li> </ul> <p>Mistral-7B-v0.1 is the first LLM released by Mistral AI. It is a 7 billion parameter decoder-based LM with a number of architectural innovations, including sliding window attention, grouped query attention, and byte-fallback BPE tokenizer. Mistral-7B-v0.1 has been shown to achieve state-of-the-art performance on a number of NLP benchmarks, including GLUE, SuperGLUE, and the Stanford Question Answering Dataset.</p> <p>To use a Mistral LLM model:</p> <ol> <li>Download the model weights from the Mistral AI website: https://mistral.ai/.</li> <li>Install the necessary dependencies, such as the Transformers library.</li> <li>Load the model weights into a Python script or notebook.</li> <li>Call the model's <code>generate()</code> method to generate text, translate languages, answer questions, or perform other NLP    tasks.</li> </ol> <p>Here is an example of how to generate text with Mistral-7B-v0.1:</p> <pre><code>import transformers\n\n# Load the model weights\nmodel = transformers.BartModel.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n\n# Generate text\nprompt = \"Write a poem about a cat.\"\ngenerated_text = model.generate(prompt, max_length=100)\n\n# Print the generated text\nprint(generated_text)\n</code></pre> <p>Output:</p> <pre><code>A furry friend, a playful elf,\nA curious cat, a cuddly elf.\nWith paws so soft and eyes so bright,\nA cat brings joy both day and night.\n</code></pre> <p>Mistral LLM models are still under development, but they have the potential to be used in a wide range of applications. If you are interested in using Mistral's LLMs, please visit the Mistral AI website: https://mistral.ai/ for more information.</p>"},{"location":"Mistral/#mistral-model-in-easydel","title":"Mistral Model In EasyDel","text":"<p>using Mistral Models are the same as all the other models in EasyDel Collection but let take a look at how can we train or finetune a Mistral model</p> <pre><code>from EasyDel.trainer import TrainArguments, CausalLMTrainer\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom jax import numpy as jnp\nimport flax\nimport EasyDel\nfrom EasyDel.transform import llama_from_pretrained, mistral_from_pretrained\n\nmodel_id = 'mistralai/Mistral-7B-v0.1'\ndataset_train = load_dataset('&lt;TOKENIZED_MISTRAL_DATASET_AT_HUGGINGFACE&gt;')\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\nparams, config = mistral_from_pretrained(model_id)\n\nconfig.freq_max_position_embeddings = config.max_position_embeddings  # 32768\nconfig.max_position_embeddings = 4096  # Let use context length of 4096 for training\nconfig.c_max_position_embeddings = config.max_position_embeddings\n\nmax_length = config.max_position_embeddings\n\ntrain_args = TrainArguments(\n    model_class=EasyDel.FlaxMistralForCausalLM,\n    configs_to_init_model_class={\n        'config': config,\n        'dtype': jnp.bfloat16,\n        'param_dtype': jnp.bfloat16,\n        'input_shape': (1, 1)\n    },\n    custom_rule=config.get_partition_rules(True),\n    model_name='Test',\n    num_train_epochs=2,\n    learning_rate=4e-5,\n    learning_rate_end=5e-6,\n    optimizer='adamw',\n    scheduler='cosine',\n    weight_decay=0.01,\n    total_batch_size=32,\n    max_steps=None,\n    do_train=True,\n    do_eval=False,\n    backend='tpu',\n    max_length=max_length,\n    gradient_checkpointing='nothing_saveable',\n    sharding_array=(1, -1, 1, 1),\n    use_pjit_attention_force=False,\n    gradient_accumulation_steps=8,\n    remove_ckpt_after_load=True,\n    ids_to_pop_from_dataset=['token_type_ids'],\n    loss_remat='',\n    dtype=jnp.bfloat16\n)\n\ntrainer = CausalLMTrainer(\n    train_args,\n    dataset_train['train'],\n    ckpt_path=None\n)\n\noutput = trainer.train(flax.core.FrozenDict({'params': params}))\n# And Here were EasyDel goes brrrrrr and start training \n</code></pre>"},{"location":"MosaicMPT/","title":"About MosaicMPT Models","text":"<p>MosaicMPT Models</p> <p>MosaicMPT Models is a family of large language models (LLMs) developed by MosaicML. The models are trained on a massive dataset of text and code, and can be used for a variety of tasks, including</p> <ul> <li>Natural language understanding (NLU)</li> <li>Natural language generation (NLG)</li> <li>Machine translation</li> <li>Text summarization</li> <li>Question answering</li> <li>Code generation</li> </ul> <p>The MosaicMPT models are available under the Apache 2.0 license, which means that they can be freely used, modified, and redistributed.</p> <p>Model Architecture</p> <p>The MosaicMPT models are based on the Transformer architecture, which is a neural network architecture that has been shown to be very effective for NLP tasks. The models are trained using a technique called masked language modeling, which involves predicting the missing words in a sequence of text.</p> <p>Model Sizes</p> <p>The MosaicMPT models come in a variety of sizes, ranging from 7 billion to 70 billion parameters. The larger models have more capacity to learn complex patterns in language, but they are also more computationally expensive to train and deploy.</p> <p>MosaicPretrainedTransformer (MPT) Architecture</p> <p>The MosaicPretrainedTransformer (MPT) architecture is a modified transformer architecture that is optimized for efficient training and inference. The MPT architecture includes the following changes:</p> <ul> <li>Performance-optimized layer implementations</li> <li>Architecture changes that provide greater training stability</li> <li>Elimination of context length limits by replacing positional embeddings with Attention with Linear Biases (ALiBi)</li> </ul> <p>Thanks to these modifications, MPT models can be trained with high throughput efficiency and stable convergence. MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer.</p> <p>Use Cases</p> <p>The MosaicMPT models can be used for a variety of tasks, including:</p> <ul> <li>Natural language understanding (NLU): The MosaicMPT models can be used to understand the meaning of text, such as   identifying the entities and relationships in a sentence.</li> <li>Natural language generation (NLG): The MosaicMPT models can be used to generate text, such as writing different kinds   of creative content, like poems, code, scripts, musical pieces, email, letters, etc.</li> <li>Machine translation: The MosaicMPT models can be used to translate text from one language to another.</li> <li>Text summarization: The MosaicMPT models can be used to summarize a text document into a shorter, more concise   version.</li> <li>Question answering: The MosaicMPT models can be used to answer questions about a text document.</li> <li>Code generation: The MosaicMPT models can be used to generate code, such as Python scripts or Java classes.</li> </ul> <p>Availability</p> <p>The MosaicMPT models are available through the Hugging Face Hub. The models are also available in the TensorFlow Hub, the PyTorch Hub and EasyDel.</p> <p>Conclusion</p> <p>The MosaicMPT models are a powerful family of LLMs that can be used for a variety of tasks. The models are open source and available for free, making them a valuable resource for researchers and developers.</p>"},{"location":"MosaicMPT/#how-to-useload-them-in-easydel","title":"How to Use/Load Them in EasyDel","text":"<pre><code>import jax\nfrom EasyDel.transform import mpt_from_pretrained\n\nparams, config = mpt_from_pretrained(\n  'mosaicml/mpt-7b',\n  device=jax.devices('cpu')[0]  # Offload on CPU\n)\n</code></pre> <p>also keep that in mind that returned <code>config</code> includes <code>.get_partition_rules(fsdp=True)</code></p>"},{"location":"MosaicMPT/#use-with-jaxserver","title":"Use With JaxServer","text":"<pre><code>from EasyDel import JAXServer, FlaxMptForCausalLM\nimport jax\nfrom EasyDel.transform import mpt_from_pretrained\nfrom transformers import AutoTokenizer\n\nparams, config = mpt_from_pretrained(\n  'mosaicml/mpt-7b',\n  device=jax.devices('cpu')[0]  # Offload on CPU\n)\n\n\nclass MPTJaxServer(JAXServer):\n  ...\n  # You have to Custom this one yourself as you \n  # need read JaxServer Documents inorder to learn how\n\n\nserver = MPTJaxServer.load_from_params(\n  params=params,\n  model=FlaxMptForCausalLM(\n    config=config,\n    dtype=jax.numpy.bfloat16,  # Im on TPUs\n    param_dtype=jax.numpy.bfloat16,  # Im on TPUs\n    precision=jax.lax.Precision('fastest'),\n    _do_init=False,\n    input_shape=(1, 1024)\n  ),\n  config_model=config,\n  add_params_field=True,\n  tokenizer=AutoTokenizer.from_pretrained('mosaicml/mpt-7b'),\n  verbose=False,\n  do_memory_log=True,\n  config={\n    \"max_length\": 2048,\n    \"max_new_tokens\": 2048,\n    \"max_stream_tokens\": 64,\n    \"dtype\": 'bf16',\n    \"use_prefix_tokenizer\": True,\n    'pre_compile': True\n  }\n)\n\nserver.fire()  # Launch FastAPI functions\n\nshared_urls = server.launch(\n  share_chat=True,\n  share_inst=True\n)\n</code></pre> <p>Done \ud83d\ude07 this method can be used for all the MosaicMPT models</p>"},{"location":"PyTorchServer/","title":"PytorchServer","text":""},{"location":"PyTorchServer/#pytorchserver","title":"PyTorchServer \ud83e\uddec","text":"<p><code>PyTorchServer</code> is one of offered utilities by EasyDel, and it's help hosting using and doing process with LLMs and its also hackable, so you can override your own method in it and use it support both mid-level and high-level apis and also give you a Gradio Chat and Instruct Pre-build and ready to use page</p> <ul> <li>Supported Models are:<ul> <li>EveryModel that have <code>transformers.PretrainedModel</code> as their Parent :)</li> </ul> </li> </ul> <p>Documents are On The Way Amigos...</p>"},{"location":"TrainingExample/","title":"About EasyDel.CausalLMTrainer","text":"<p>What Will we cover in this tutorial</p> <ul> <li>Inputs args/kwargs for CausalLMTrainer</li> <li>How to use</li> <li>Example of Training a Llama2 Model with CausalLMTrainer with jax on TPU/CPU/GPU</li> <li>Reconvert Model to HuggingFace PyTorch</li> </ul>"},{"location":"TrainingExample/#what-do-we-need-to-start","title":"What Do we need to start","text":"<p>if you want to run this code in Kaggle or GoogleCloud you are fine to just run this Code to install dependencies</p> <pre><code>pip install fjformer==0.0.7 datasets gradio wandb -U -q\n# EasyDel Can work with any version of JAX &gt;= 0.4.4\npip install jax[tpu]==0.4.14 -f https://storage.googleapis.com/jax-releases/libtpu_releases.html -q\npython -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('HF_TOKEN_Here')\"\nwandb login WANDB_TOKEN_HERE\napt-get update &amp;&amp; apt-get upgrade -y -q\napt-get install golang -y -q\n</code></pre>"},{"location":"TrainingExample/#inputs-to-causallmtrainer","title":"Inputs to CausalLMTrainer","text":"<p><code>arguments: TrainArguments</code></p> <p><code>dataset_train: Dataset</code></p> <p><code>dataset_eval: Dataset = None</code></p> <p><code>finetune: bool = True</code></p> <p><code>ckpt_path: typing.Union[str, os.PathLike] = None</code></p> <p><code>_do_init_fns: bool = True</code></p>"},{"location":"TrainingExample/#arguments","title":"arguments","text":"<p>trainer takes in a TrainArguments to initialize wandb, utilities, funcs, partitions, optimizers and extra</p> <pre><code>class TrainArguments(\n    OrderedDict\n):\n    def __init__(\n            self,\n            model_name: str,  # Name For Model \n            num_train_epochs: int,  # Number of Total Epochs for Train\n            model_id: str = None,  # ID For model [Optional for load Model From HuggingFace Repo]\n            model_class=None,\n            # Model class to initialize the Model or you can pass the HuggingFace Repo ID to `model_id` field\n            total_batch_size: int = 32,\n            # Total Batch size without counting gradient_accumulation_steps\n            max_steps: Union[int, None] = None,\n            # Max Number of Train Steps\n            optimizer: str = 'lion',\n            # Optimizer For Model\n            scheduler: str = 'linear',\n            # Scheduler For Model\n            learning_rate: Union[int, float] = 5e-5,\n            # Start OF Learning Rate\n            learning_rate_end: Union[None, float] = 5e-6,\n            # End OF Learning Rate in case of using scheduler='linear'\n            gradient_accumulation_steps: int = 1,\n            # Gradient Accumulation Steps\n            weight_decay: float = 0.01,\n            gradient_checkpointing: str = 'nothing_saveable',\n            max_length: Union[int, None] = 4096,\n            sharding_array: Union[tuple, int] = (1, -1, 1, 1),\n            # PJIT Partition Sharding Mesh Size for (DP,FSDP,MP) \n            is_fine_tuning: bool = True,\n            do_train: bool = True,\n            do_eval: bool = False,\n            do_test: Union[bool, None] = False,\n            backend: Union[str, None] = None,\n            extra_optimizer_kwargs: dict = None,\n            # Extra kwargs to be passed to optimizer for initialization\n            save_steps: Union[int, None] = None,\n            save_dir: str = 'easydel_ckpt',\n            use_pjit_attention_force: bool = False,\n            dtype=jnp.bfloat16,\n            param_dtype=jnp.bfloat16,\n            fully_fsdp=True,\n            use_wandb: bool = True,\n            custom_rule=None,\n            # Use Custom Partition Rules or use Default Ones \n            # [Do not use Custom Partitioning Rules in case that you haven't dealt with Jax Mesh]\n            extra_configs=None,\n            ids_to_pop_from_dataset: list = None,\n            # Ids to pop from dataset in training Loop for example ids_to_pop_from_dataset=['token_type_ids'], \n            remove_ckpt_after_load: bool = False,\n            # For saving Memory and Disk Space\n            configs_to_init_model_class=None,\n            # config to be passed to model for initialization of model\n            do_last_save: bool = True,\n            model_parameters=None,\n            do_shard_fns: bool = True,\n            track_memory: bool = True,\n            loss_remat: str = '',\n            loss_chunk: int = 1024,\n            is_left_padded: bool = False,\n            # Is model using Left padded or right padded dataset\n            warmup_steps: int = 500,\n            **kwargs\n    ):\n        ...\n</code></pre>"},{"location":"TrainingExample/#dataset_train","title":"dataset_train","text":"<p>Dataset for Train model which should only contain numerical information for model such as Input_ids and attention_mask</p>"},{"location":"TrainingExample/#dataset_eval","title":"dataset_eval","text":"<p>Dataset for Evaluate the model which should only contain numerical information for model such as Input_ids and attention_mask</p>"},{"location":"TrainingExample/#finetune","title":"finetune","text":"<p>fine-tune is a boolean field which stand for should EasyDel initialize the params for pre-training the model, or you will pass the model parameters later in training section</p>"},{"location":"TrainingExample/#ckpt_path","title":"ckpt_path","text":"<p>path to Read Params from Checkpoint in case of passing string you are reading from file and in case of passing none you can pass params manually to Tariner</p>"},{"location":"TrainingExample/#finetune-a-llama-model","title":"FineTune A LLama Model","text":"<p>here's a script</p> <p>here in this script we will see an example for how to finetune models in EasyDel</p> <pre><code>from EasyDel import TrainArguments, CausalLMTrainer, AutoEasyDelModelForCausalLM\nimport jax\nimport flax\nfrom datasets import load_dataset\n\nmodel_id = 'meta-llama/Llama-2-13b-hf'\ndataset_train = load_dataset('REPO_ID_PATH_TO_DATASET')\n# dataset should only contain numerical information for Model such as input_id, attention_mask , ...\nmodel, params = AutoEasyDelModelForCausalLM.from_pretrained(\n    model_id,\n    dtype=jax.numpy.bfloat16,\n    param_dtype=jax.numpy.bfloat16,\n    precision=jax.lax.Precision('fastest'),\n    device=jax.devices('cpu')[0],  # Load JAX Model and initialize or load Parameters on CPU \n    # The Rest of kwargs here will be passed to AutoModelForCausalLM huggingface such as this device_map\n    device_map='auto'\n)\nconfig = model.config\n\n# this part of code is only for making model faster and more optimized \nconfig.freq_max_position_embeddings = config.max_position_embeddings\nconfig.max_position_embeddings = 4096\nconfig.c_max_position_embeddings = config.max_position_embeddings\nconfig.use_pjit_attention_force = False  # disable pjit attention force is recommended in case of using MP = 1 in sharding Mesh\n\nmax_length = config.max_position_embeddings\n\nconfigs_to_init_model_class = {\n    'config': config,\n    'dtype': jax.numpy.bfloat16,\n    'param_dtype': jax.numpy.bfloat16,\n    'input_shape': (1, 1)\n}\n\ntrain_args = TrainArguments(\n    model_class=type(model),\n    configs_to_init_model_class=configs_to_init_model_class,\n    custom_rule=config.get_partition_rules(True),\n    model_name='EasyDelLLama2',\n    num_train_epochs=2,\n    learning_rate=2e-4,\n    learning_rate_end=5e-6,\n    optimizer='adamw',\n    scheduler='linear',\n    weight_decay=0.02,\n    total_batch_size=64,\n    max_steps=None,\n    do_train=True,\n    do_eval=False,\n    backend='tpu',\n    max_length=max_length,\n    gradient_checkpointing='nothing_saveable',\n    sharding_array=(1, -1, 1, 1),\n    use_pjit_attention_force=False,\n    gradient_accumulation_steps=1,\n    remove_ckpt_after_load=True,\n    ids_to_pop_from_dataset=['token_type_ids'],\n    loss_remat='',\n    dtype=jax.numpy.bfloat16\n)\n\ntrainer = CausalLMTrainer(\n    train_args,\n    dataset_train,\n    ckpt_path=None\n)\n\noutput = trainer.train(flax.core.FrozenDict({'params': params}))\n\nsaved_model_location = f\"{str(train_args.get_path())}/{output.last_save_file_name}\"\n\nprint(\"Hey im Here in case you want to load me :\", saved_model_location)\n\n### Let Convert Model TO HF/PyTorch\n\nfrom EasyDel.transform import llama_easydel_to_hf\n\nconfig.rope_theta = 10000\nconfig.attention_bias = False\nmodel = llama_easydel_to_hf(saved_model_location, config=config)\n\n# Here's your Huggingface Torch Llama\nmodel = model.half()\n\nmodel.push_to_hub(\"REPO_ID_TO_PUSH\")\n</code></pre>"},{"location":"lib-python-EasyDel-_rl_trainer-core/","title":"rl_trainer.core","text":""},{"location":"lib-python-EasyDel-_rl_trainer-core/#lib.python.EasyDel.rl_trainer.core.add_suffix","title":"<code>add_suffix(input_dict, suffix)</code>","text":"<p>Add suffix to dict keys.</p> Source code in <code>lib/python/EasyDel/rl_trainer/core.py</code> <pre><code>def add_suffix(input_dict, suffix):\n    \"\"\"Add suffix to dict keys.\"\"\"\n    return dict((k + suffix, v) for k, v in input_dict.items())\n</code></pre>"},{"location":"lib-python-EasyDel-_rl_trainer-core/#lib.python.EasyDel.rl_trainer.core.multinomial","title":"<code>multinomial(logits, num_samples, replacement=False)</code>","text":"<p>Implements the <code>torch.multinomial</code> function in JAX.</p> <p>Args:     logits (jnp.array): The unnormalized log probabilities of the events.     num_samples (int): The number of samples to draw.     replacement (bool): Don't use this ; Returns:     jnp.array: A matrix of shape (num_samples, batch_size) containing the         sampled indices.</p> Source code in <code>lib/python/EasyDel/rl_trainer/core.py</code> <pre><code>def multinomial(logits, num_samples: int, replacement: bool = False):\n    \"\"\"\n    Implements the `torch.multinomial` function in JAX.\n\n    Args:\n        logits (jnp.array): The unnormalized log probabilities of the events.\n        num_samples (int): The number of samples to draw.\n        replacement (bool): Don't use this ;\\\n\n    Returns:\n        jnp.array: A matrix of shape (num_samples, batch_size) containing the\n            sampled indices.\n    \"\"\"\n    logits = jax.nn.log_softmax(logits, axis=-1)\n    if replacement:\n        return jax.random.categorical(logits, num_samples)\n    else:\n        samples = []\n        for _ in range(num_samples):\n            sample = jax.random.categorical(logits, 1)\n            samples.append(sample[0])\n            logits.at[sample[0]].set(-jnp.inf)\n        return jnp.array(samples)\n</code></pre>"},{"location":"lib-python-EasyDel-_rl_trainer-models-modelling_base/","title":"rl_trainer.models.modelling_base","text":""},{"location":"lib-python-EasyDel-_rl_trainer-models-modelling_base/#lib.python.EasyDel.rl_trainer.models.modelling_base.FlaxPreTrainedModelWrapper","title":"<code>FlaxPreTrainedModelWrapper</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>lib/python/EasyDel/rl_trainer/models/modelling_base.py</code> <pre><code>class FlaxPreTrainedModelWrapper(nn.Module):\n    pretrained_model: FlaxPreTrainedModel\n    transformers_parent_class = None\n    supported_args = None\n    supported_modules = (\"v_head\",)\n    supported_rm_modules = (\"score\",)\n    supported_pretrained_model_architectures = FlaxPreTrainedModel\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, from_pt: bool = True, *model_args, **kwargs):\n\n        \"\"\"\n        The from_pretrained function is used to instantiate a model from a pretrained checkpoint.\n\n        :param cls: Refer to the class that called this function\n        :param pretrained_model_name_or_path: Specify the path to the pretrained model\n        :param from_pt: bool: Determine whether to load the model from a pytorch checkpoint or not\n        :param *model_args: Pass the positional arguments of the model\n        :param **kwargs: Pass keyworded, variable-length argument list\n        :return: A model with the state_dict loaded from a file\n\n        \"\"\"\n        if kwargs is not None:\n            reward_adapter = kwargs.pop(\"reward_adapter\", None)\n            is_trainable = kwargs.pop(\"is_trainable\", False)\n            trl_model_args, pretrained_kwargs, peft_quantization_kwargs = cls._split_kwargs(kwargs)\n            token = pretrained_kwargs.get(\"token\", None)\n        else:\n            reward_adapter = None\n            is_trainable = False\n            trl_model_args = {}\n            pretrained_kwargs = {}\n            token = None\n\n        if reward_adapter is not None and not isinstance(reward_adapter, str):\n            raise ValueError(\n                \"The `reward_adapter` argument should be \"\n                \"a string representing the name of local path or the\"\n                \" Hub id to the Reward Modeling adapter.\"\n            )\n\n        if isinstance(pretrained_model_name_or_path, str):\n\n            remote_adapter_config = None\n\n            local_adapter_present = os.path.exists(os.path.join(pretrained_model_name_or_path, \"adapter_config.json\"))\n\n            pretrained_model = cls.transformers_parent_class.from_pretrained(\n                pretrained_model_name_or_path, *model_args, **pretrained_kwargs\n            )\n\n        elif isinstance(pretrained_model_name_or_path, cls.supported_pretrained_model_architectures):\n            pretrained_model = pretrained_model_name_or_path\n\n        else:\n            raise ValueError(\n                \"pretrained_model_name_or_path should be a string or a PreTrainedModel, \"\n                f\"but is {type(pretrained_model_name_or_path)}\"\n            )\n\n        model = cls(pretrained_model, **trl_model_args)\n        is_resuming_training = True\n        if isinstance(pretrained_model_name_or_path, str):\n            safe_filename = os.path.join(pretrained_model_name_or_path, \"model.safetensors\")\n            filename = os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin\")\n\n            sharded_index_filename = os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin.index.json\")\n            safe_sharded_index_filename = os.path.join(pretrained_model_name_or_path, \"model.safetensors.index.json\")\n            is_sharded = False\n            use_safe = os.path.exists(safe_filename)\n\n            if not (os.path.exists(filename) or os.path.exists(safe_filename)):\n                filename, files_to_download, is_sharded, is_resuming_training = cls._get_checkpoint_from_hub(\n                    pretrained_model,\n                    pretrained_model_name_or_path,\n                    sharded_index_filename,\n                    token=token,\n                )\n                if filename is None and files_to_download is None:\n                    safe_filename, files_to_download, is_sharded, is_resuming_training = cls._get_checkpoint_from_hub(\n                        pretrained_model,\n                        pretrained_model_name_or_path,\n                        safe_sharded_index_filename,\n                        token=token,\n                        model_name=\"model.safetensors\",\n                        model_index_name=\"model.safetensors.index.json\",\n                    )\n                    use_safe = True\n                else:\n                    use_safe = False\n            if from_pt:\n                loading_func = load_file\n                load_kwargs = {}\n            else:\n                def loading_func(file_name: str, *args, **kwargs_):\n                    tensors = {}\n                    with open(file_name, 'rb') as stream:\n                        unpacker = msgpack.Unpacker(stream, read_size=83886080, max_buffer_size=0)\n                        for key, value in unpacker:\n                            key = tuple(key)\n                            tensor = from_bytes(None, value)\n                            tensors[key] = tensor\n                    return tensors\n\n            if is_resuming_training:\n                if is_sharded:\n                    state_dict = {}\n\n                    for shard_file in files_to_download:\n                        filename = hf_hub_download(\n                            pretrained_model_name_or_path,\n                            shard_file,\n                            token=token,\n                        )\n                        state_dict.update(loading_func(filename, **load_kwargs))\n                else:\n                    state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n\n        else:\n            state_dict = pretrained_model_name_or_path.state_dict()\n\n        if from_pt:\n            lw = len('.weight')\n            with jax.default_device(cls._get_current_device()):\n                flax_dict = {}\n                for key, tensor in state_dict.items():\n                    if match_keywords(key, ['kernel'], ['none']):\n                        if len(tensor.shape) == 2:\n                            tensor = tensor.transpose(0, 1)\n                    if key.endswith('.weight'):\n                        key = key[:-lw] + '.kernel'\n                    key_tuple = key.split('.')\n                    key_names = ()\n                    tensor = tensor.detach().cpu().numpy()\n                    for k in key_tuple:\n                        key_names += k,\n                    flax_dict[key_names] = tensor\n\n        model.is_peft_model = False\n        model.current_device = cls._get_current_device()\n\n        if is_resuming_training:\n            model.post_init(state_dict=state_dict)\n\n        model.supports_rm_adapter = False\n\n        return model\n\n    @classmethod\n    def _get_checkpoint_from_hub(\n            cls,\n            pretrained_model,\n            pretrained_model_name_or_path,\n            index_filename,\n            token=None,\n            model_name=\"pytorch_model.bin\",\n            model_index_name=\"pytorch_model.bin.index.json\",\n    ):\n        \"\"\"\n        The _get_checkpoint_from_hub function is used to download a pretrained model from the Hugging Face Hub.\n        It will first attempt to download the entire model, and if that fails it will try downloading just the v_head weights.\n        If neither of those attempts succeed, it will return None for all outputs.\n\n        :param cls: Specify the class of the model\n        :param pretrained_model: Load the pretrained model\n        :param pretrained_model_name_or_path: Load the pretrained model from a checkpoint\n        :param index_filename: Load the index file for sharded models\n        :param token: Authenticate with the hugging face model hub\n        :param model_name: Specify the name of the model file to be downloaded\n        :param model_index_name: Specify the name of the index file\n        :param : Load the pretrained model\n        :return: A tuple of four elements:\n\n        \"\"\"\n        files_to_download = None\n        filename = None\n        is_resuming_training = True\n        is_sharded = False\n\n        try:\n            filename = hf_hub_download(\n                pretrained_model_name_or_path,\n                model_name,\n                token=token,\n            )\n        # sharded\n        except (EntryNotFoundError, LocalEntryNotFoundError, HFValidationError):\n            index_file_name = ''\n            if os.path.exists(index_filename):\n                index_file_name = index_filename\n            else:\n                try:\n                    index_file_name = hf_hub_download(\n                        pretrained_model_name_or_path,\n                        model_index_name,\n                        token=token,\n                    )\n                except (EntryNotFoundError, LocalEntryNotFoundError, HFValidationError):\n                    # not continue training, do not have v_head weight\n                    is_resuming_training = False\n                    logging.warning(\n                        f\"A {type(pretrained_model)} model is loaded from '{pretrained_model_name_or_path}', \"\n                        f\"and no v_head weight is found. This IS expected if you are not resuming PPO training.\"\n                    )\n            # load json\n            if is_resuming_training:\n                with open(index_file_name, \"r\") as f:\n                    index = json.load(f)\n                files_to_download = set()\n                for k, v in index[\"weight_map\"].items():\n                    if any([module in k for module in cls.supported_modules]):\n                        files_to_download.add(v)\n                is_sharded = True\n\n        return filename, files_to_download, is_sharded, is_resuming_training\n\n    @classmethod\n    def _get_current_device(cls):\n        \"\"\"\n        The _get_current_device function is a class method that returns the current device.\n\n        :param cls: Indicate that the function is a method of the class\n        :return: The current device\n\n        \"\"\"\n        return jax.devices()[0]\n\n    @classmethod\n    def _split_kwargs(cls, kwargs):\n        \"\"\"\n        The _split_kwargs function is used to split the kwargs into three categories:\n            1. supported_kwargs - These are the arguments that are supported by this class and will be passed on to the parent class.\n            2. unsupported_kwargs - These are arguments that aren't supported by this class, but may be useful for other classes in a chain of inheritance (e.g., if you're using multiple mixins).\n            3. peft_kwargs - These are arguments specific to PEFT and will not be passed on to any other classes.\n\n        :param cls: Refer to the class itself\n        :param kwargs: Pass keyword arguments to the function\n        :return: A tuple of three dictionaries\n\n        \"\"\"\n        supported_kwargs = {}\n        unsupported_kwargs = {}\n        peft_kwargs = {}\n\n        for key, value in kwargs.items():\n            if key in cls.supported_args:\n                supported_kwargs[key] = value\n            else:\n                unsupported_kwargs[key] = value\n\n        return supported_kwargs, unsupported_kwargs, peft_kwargs\n\n    def push_to_hub(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def save_pretrained(self, *args, **kwargs):\n        state_dict = kwargs.get(\"state_dict\")\n        if state_dict is None:\n            state_dict = self.state_dict()\n            kwargs[\"state_dict\"] = state_dict\n\n        return self.pretrained_model.save_pretrained(*args, **kwargs)\n\n    def state_dict(self, *args, **kwargs):\n        r\"\"\"\n        Return the state_dict of the pretrained model.\n        \"\"\"\n        raise NotImplementedError\n\n    def post_init(self, *args, **kwargs):\n        r\"\"\"\n        Post initialization method. This method is called after the model is\n        instantiated and loaded from a checkpoint. It can be used to perform\n        additional operations such as loading the state_dict.\n        \"\"\"\n        raise NotImplementedError\n\n    def compute_reward_score(self, input_ids, attention_mask=None, ppo_adapter_name=\"default\", **kwargs):\n\n        \"\"\"\n        The compute_reward_score function is used to compute the reward score for a given input.\n        The function takes in an input_ids tensor and returns a tensor of scores. The shape of the returned\n        tensor will be (batch_size, sequence_length). The higher the score, the more likely that token should be kept.\n\n        :param self: Represent the instance of the class\n        :param input_ids: Pass the input tokens to the model\n        :param attention_mask: Indicate which tokens are padding\n        :param ppo_adapter_name: Set the adapter back to its original state\n        :param **kwargs: Pass a variable number of arguments to a function\n        :return: The scores for the given input_ids\n\n        \"\"\"\n        if not self.supports_rm_adapter:\n            raise ValueError(\"This model does not support reward modeling adapter.\")\n\n        # enable rm adapter\n        self.pretrained_model.set_adapter(self.rm_adapter_name)\n        self.pretrained_model.eval()\n\n        base_model_output = self.pretrained_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=True,\n            return_dict=True,\n            **kwargs,\n        )\n\n        last_hidden_states = base_model_output.hidden_states[-1]\n        scores = self.score(last_hidden_states)\n\n        self.pretrained_model.set_adapter(ppo_adapter_name)\n        self.pretrained_model.train()\n\n        return scores\n</code></pre>"},{"location":"lib-python-EasyDel-_rl_trainer-models-modelling_base/#lib.python.EasyDel.rl_trainer.models.modelling_base.FlaxPreTrainedModelWrapper.compute_reward_score","title":"<code>compute_reward_score(input_ids, attention_mask=None, ppo_adapter_name='default', **kwargs)</code>","text":"<p>The compute_reward_score function is used to compute the reward score for a given input. The function takes in an input_ids tensor and returns a tensor of scores. The shape of the returned tensor will be (batch_size, sequence_length). The higher the score, the more likely that token should be kept.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <p>Pass the input tokens to the model</p> required <code>attention_mask</code> <p>Indicate which tokens are padding</p> <code>None</code> <code>ppo_adapter_name</code> <p>Set the adapter back to its original state</p> <code>'default'</code> <code>**kwargs</code> <p>Pass a variable number of arguments to a function</p> <code>{}</code> <p>Returns:</p> Type Description <p>The scores for the given input_ids</p> Source code in <code>lib/python/EasyDel/rl_trainer/models/modelling_base.py</code> <pre><code>def compute_reward_score(self, input_ids, attention_mask=None, ppo_adapter_name=\"default\", **kwargs):\n\n    \"\"\"\n    The compute_reward_score function is used to compute the reward score for a given input.\n    The function takes in an input_ids tensor and returns a tensor of scores. The shape of the returned\n    tensor will be (batch_size, sequence_length). The higher the score, the more likely that token should be kept.\n\n    :param self: Represent the instance of the class\n    :param input_ids: Pass the input tokens to the model\n    :param attention_mask: Indicate which tokens are padding\n    :param ppo_adapter_name: Set the adapter back to its original state\n    :param **kwargs: Pass a variable number of arguments to a function\n    :return: The scores for the given input_ids\n\n    \"\"\"\n    if not self.supports_rm_adapter:\n        raise ValueError(\"This model does not support reward modeling adapter.\")\n\n    # enable rm adapter\n    self.pretrained_model.set_adapter(self.rm_adapter_name)\n    self.pretrained_model.eval()\n\n    base_model_output = self.pretrained_model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        output_hidden_states=True,\n        return_dict=True,\n        **kwargs,\n    )\n\n    last_hidden_states = base_model_output.hidden_states[-1]\n    scores = self.score(last_hidden_states)\n\n    self.pretrained_model.set_adapter(ppo_adapter_name)\n    self.pretrained_model.train()\n\n    return scores\n</code></pre>"},{"location":"lib-python-EasyDel-_rl_trainer-models-modelling_base/#lib.python.EasyDel.rl_trainer.models.modelling_base.FlaxPreTrainedModelWrapper.from_pretrained","title":"<code>from_pretrained(pretrained_model_name_or_path, from_pt=True, *model_args, **kwargs)</code>  <code>classmethod</code>","text":"<p>The from_pretrained function is used to instantiate a model from a pretrained checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>Refer to the class that called this function</p> required <code>pretrained_model_name_or_path</code> <p>Specify the path to the pretrained model</p> required <code>from_pt</code> <code>bool</code> <p>bool: Determine whether to load the model from a pytorch checkpoint or not</p> <code>True</code> <code>*model_args</code> <p>Pass the positional arguments of the model</p> <code>()</code> <code>**kwargs</code> <p>Pass keyworded, variable-length argument list</p> <code>{}</code> <p>Returns:</p> Type Description <p>A model with the state_dict loaded from a file</p> Source code in <code>lib/python/EasyDel/rl_trainer/models/modelling_base.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, pretrained_model_name_or_path, from_pt: bool = True, *model_args, **kwargs):\n\n    \"\"\"\n    The from_pretrained function is used to instantiate a model from a pretrained checkpoint.\n\n    :param cls: Refer to the class that called this function\n    :param pretrained_model_name_or_path: Specify the path to the pretrained model\n    :param from_pt: bool: Determine whether to load the model from a pytorch checkpoint or not\n    :param *model_args: Pass the positional arguments of the model\n    :param **kwargs: Pass keyworded, variable-length argument list\n    :return: A model with the state_dict loaded from a file\n\n    \"\"\"\n    if kwargs is not None:\n        reward_adapter = kwargs.pop(\"reward_adapter\", None)\n        is_trainable = kwargs.pop(\"is_trainable\", False)\n        trl_model_args, pretrained_kwargs, peft_quantization_kwargs = cls._split_kwargs(kwargs)\n        token = pretrained_kwargs.get(\"token\", None)\n    else:\n        reward_adapter = None\n        is_trainable = False\n        trl_model_args = {}\n        pretrained_kwargs = {}\n        token = None\n\n    if reward_adapter is not None and not isinstance(reward_adapter, str):\n        raise ValueError(\n            \"The `reward_adapter` argument should be \"\n            \"a string representing the name of local path or the\"\n            \" Hub id to the Reward Modeling adapter.\"\n        )\n\n    if isinstance(pretrained_model_name_or_path, str):\n\n        remote_adapter_config = None\n\n        local_adapter_present = os.path.exists(os.path.join(pretrained_model_name_or_path, \"adapter_config.json\"))\n\n        pretrained_model = cls.transformers_parent_class.from_pretrained(\n            pretrained_model_name_or_path, *model_args, **pretrained_kwargs\n        )\n\n    elif isinstance(pretrained_model_name_or_path, cls.supported_pretrained_model_architectures):\n        pretrained_model = pretrained_model_name_or_path\n\n    else:\n        raise ValueError(\n            \"pretrained_model_name_or_path should be a string or a PreTrainedModel, \"\n            f\"but is {type(pretrained_model_name_or_path)}\"\n        )\n\n    model = cls(pretrained_model, **trl_model_args)\n    is_resuming_training = True\n    if isinstance(pretrained_model_name_or_path, str):\n        safe_filename = os.path.join(pretrained_model_name_or_path, \"model.safetensors\")\n        filename = os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin\")\n\n        sharded_index_filename = os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin.index.json\")\n        safe_sharded_index_filename = os.path.join(pretrained_model_name_or_path, \"model.safetensors.index.json\")\n        is_sharded = False\n        use_safe = os.path.exists(safe_filename)\n\n        if not (os.path.exists(filename) or os.path.exists(safe_filename)):\n            filename, files_to_download, is_sharded, is_resuming_training = cls._get_checkpoint_from_hub(\n                pretrained_model,\n                pretrained_model_name_or_path,\n                sharded_index_filename,\n                token=token,\n            )\n            if filename is None and files_to_download is None:\n                safe_filename, files_to_download, is_sharded, is_resuming_training = cls._get_checkpoint_from_hub(\n                    pretrained_model,\n                    pretrained_model_name_or_path,\n                    safe_sharded_index_filename,\n                    token=token,\n                    model_name=\"model.safetensors\",\n                    model_index_name=\"model.safetensors.index.json\",\n                )\n                use_safe = True\n            else:\n                use_safe = False\n        if from_pt:\n            loading_func = load_file\n            load_kwargs = {}\n        else:\n            def loading_func(file_name: str, *args, **kwargs_):\n                tensors = {}\n                with open(file_name, 'rb') as stream:\n                    unpacker = msgpack.Unpacker(stream, read_size=83886080, max_buffer_size=0)\n                    for key, value in unpacker:\n                        key = tuple(key)\n                        tensor = from_bytes(None, value)\n                        tensors[key] = tensor\n                return tensors\n\n        if is_resuming_training:\n            if is_sharded:\n                state_dict = {}\n\n                for shard_file in files_to_download:\n                    filename = hf_hub_download(\n                        pretrained_model_name_or_path,\n                        shard_file,\n                        token=token,\n                    )\n                    state_dict.update(loading_func(filename, **load_kwargs))\n            else:\n                state_dict = loading_func(filename if not use_safe else safe_filename, **load_kwargs)\n\n    else:\n        state_dict = pretrained_model_name_or_path.state_dict()\n\n    if from_pt:\n        lw = len('.weight')\n        with jax.default_device(cls._get_current_device()):\n            flax_dict = {}\n            for key, tensor in state_dict.items():\n                if match_keywords(key, ['kernel'], ['none']):\n                    if len(tensor.shape) == 2:\n                        tensor = tensor.transpose(0, 1)\n                if key.endswith('.weight'):\n                    key = key[:-lw] + '.kernel'\n                key_tuple = key.split('.')\n                key_names = ()\n                tensor = tensor.detach().cpu().numpy()\n                for k in key_tuple:\n                    key_names += k,\n                flax_dict[key_names] = tensor\n\n    model.is_peft_model = False\n    model.current_device = cls._get_current_device()\n\n    if is_resuming_training:\n        model.post_init(state_dict=state_dict)\n\n    model.supports_rm_adapter = False\n\n    return model\n</code></pre>"},{"location":"lib-python-EasyDel-_rl_trainer-models-modelling_base/#lib.python.EasyDel.rl_trainer.models.modelling_base.FlaxPreTrainedModelWrapper.post_init","title":"<code>post_init(*args, **kwargs)</code>","text":"<p>Post initialization method. This method is called after the model is instantiated and loaded from a checkpoint. It can be used to perform additional operations such as loading the state_dict.</p> Source code in <code>lib/python/EasyDel/rl_trainer/models/modelling_base.py</code> <pre><code>def post_init(self, *args, **kwargs):\n    r\"\"\"\n    Post initialization method. This method is called after the model is\n    instantiated and loaded from a checkpoint. It can be used to perform\n    additional operations such as loading the state_dict.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"lib-python-EasyDel-_rl_trainer-models-modelling_base/#lib.python.EasyDel.rl_trainer.models.modelling_base.FlaxPreTrainedModelWrapper.state_dict","title":"<code>state_dict(*args, **kwargs)</code>","text":"<p>Return the state_dict of the pretrained model.</p> Source code in <code>lib/python/EasyDel/rl_trainer/models/modelling_base.py</code> <pre><code>def state_dict(self, *args, **kwargs):\n    r\"\"\"\n    Return the state_dict of the pretrained model.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"lib-python-EasyDel-_rl_trainer-models-modelling_value_head/","title":"rl_trainer.models.modelling_value_head","text":""},{"location":"lib-python-EasyDel-_rl_trainer-models-modelling_value_head/#lib.python.EasyDel.rl_trainer.models.modelling_value_head.FlaxAutoModelForCausalLMWithValueHead","title":"<code>FlaxAutoModelForCausalLMWithValueHead</code>","text":"<p>             Bases: <code>FlaxPreTrainedModelWrapper</code></p> Source code in <code>lib/python/EasyDel/rl_trainer/models/modelling_value_head.py</code> <pre><code>class FlaxAutoModelForCausalLMWithValueHead(FlaxPreTrainedModelWrapper):\n    pretrained_model: Type[FlaxPreTrainedModel] = FlaxLlamaForCausalLM\n    transformers_parent_class: Type[FlaxPreTrainedModel] = FlaxPreTrainedModel\n    lm_head_namings = [\"lm_head\", \"embed_out\"]\n    supported_args = (\n        \"summary_dropout_prob\",\n        \"v_head_initializer_range\",\n        \"v_head_init_strategy\",\n    )\n\n    def setup(self):\n        if not any(hasattr(self.pretrained_model, attribute) for attribute in self.lm_head_namings):\n            raise ValueError(\"The model does not have a language model head, please use a model that has one.\")\n\n        self.v_head = ValueHead(self.pretrained_model.config)\n\n    def __call__(\n            self,\n            input_ids=None,\n            past_key_values=None,\n            attention_mask=None,\n            **kwargs,\n    ):\n        \"\"\"\n        The __call__ function is the main function of a Flax model.\n        It takes in input_ids, attention_mask, and past_key_values as arguments.\n        The output is a tuple containing lm logits and value.\n\n        :param self: Represent the instance of the class\n        :param input_ids: Pass the input to the model\n        :param past_key_values: Pass the past key values to the model\n        :param attention_mask: Mask out the padding tokens\n        :param **kwargs: Pass in the past_key_values parameter\n        :param : Pass the past key values to the model\n        :return: The logits and the value\n\n        \"\"\"\n        kwargs[\"output_hidden_states\"] = True\n        kwargs[\"past_key_values\"] = past_key_values\n\n        base_model_output: FlaxCausalLMOutput = self.pretrained_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=True,\n            **kwargs,\n        )\n\n        last_hidden_state = base_model_output.hidden_states[-1]\n        lm_logits = base_model_output.logits\n\n        value = self.v_head(last_hidden_state).squeeze(-1)\n\n        return lm_logits, value\n\n    def generate(self, *args, **kwargs):\n        return self.pretrained_model.generate(*args, **kwargs)\n\n    def push_to_hub(self, *args, **kwargs):\n        \"\"\"\n        The push_to_hub function is used to push the model to a remote location.\n\n        :param self: Represent the instance of the class\n        :param *args: Send a non-keyworded variable length argument list to the function\n        :param **kwargs: Pass keyworded, variable-length argument list to a function\n        :return: The pretrained model\n\n        \"\"\"\n        setattr(self.pretrained_model, \"v_head\", self.v_head)\n\n        return self.pretrained_model.push_to_hub(*args, **kwargs)\n</code></pre>"},{"location":"lib-python-EasyDel-_rl_trainer-models-modelling_value_head/#lib.python.EasyDel.rl_trainer.models.modelling_value_head.FlaxAutoModelForCausalLMWithValueHead.__call__","title":"<code>__call__(input_ids=None, past_key_values=None, attention_mask=None, **kwargs)</code>","text":"<p>The call function is the main function of a Flax model. It takes in input_ids, attention_mask, and past_key_values as arguments. The output is a tuple containing lm logits and value.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <p>Pass the input to the model</p> <code>None</code> <code>past_key_values</code> <p>Pass the past key values to the model</p> <code>None</code> <code>attention_mask</code> <p>Mask out the padding tokens</p> <code>None</code> <code>**kwargs</code> <p>Pass in the past_key_values parameter</p> <code>{}</code> <code></code> <p>Pass the past key values to the model</p> required <p>Returns:</p> Type Description <p>The logits and the value</p> Source code in <code>lib/python/EasyDel/rl_trainer/models/modelling_value_head.py</code> <pre><code>def __call__(\n        self,\n        input_ids=None,\n        past_key_values=None,\n        attention_mask=None,\n        **kwargs,\n):\n    \"\"\"\n    The __call__ function is the main function of a Flax model.\n    It takes in input_ids, attention_mask, and past_key_values as arguments.\n    The output is a tuple containing lm logits and value.\n\n    :param self: Represent the instance of the class\n    :param input_ids: Pass the input to the model\n    :param past_key_values: Pass the past key values to the model\n    :param attention_mask: Mask out the padding tokens\n    :param **kwargs: Pass in the past_key_values parameter\n    :param : Pass the past key values to the model\n    :return: The logits and the value\n\n    \"\"\"\n    kwargs[\"output_hidden_states\"] = True\n    kwargs[\"past_key_values\"] = past_key_values\n\n    base_model_output: FlaxCausalLMOutput = self.pretrained_model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        return_dict=True,\n        **kwargs,\n    )\n\n    last_hidden_state = base_model_output.hidden_states[-1]\n    lm_logits = base_model_output.logits\n\n    value = self.v_head(last_hidden_state).squeeze(-1)\n\n    return lm_logits, value\n</code></pre>"},{"location":"lib-python-EasyDel-_rl_trainer-models-modelling_value_head/#lib.python.EasyDel.rl_trainer.models.modelling_value_head.FlaxAutoModelForCausalLMWithValueHead.push_to_hub","title":"<code>push_to_hub(*args, **kwargs)</code>","text":"<p>The push_to_hub function is used to push the model to a remote location.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>*args</code> <p>Send a non-keyworded variable length argument list to the function</p> <code>()</code> <code>**kwargs</code> <p>Pass keyworded, variable-length argument list to a function</p> <code>{}</code> <p>Returns:</p> Type Description <p>The pretrained model</p> Source code in <code>lib/python/EasyDel/rl_trainer/models/modelling_value_head.py</code> <pre><code>def push_to_hub(self, *args, **kwargs):\n    \"\"\"\n    The push_to_hub function is used to push the model to a remote location.\n\n    :param self: Represent the instance of the class\n    :param *args: Send a non-keyworded variable length argument list to the function\n    :param **kwargs: Pass keyworded, variable-length argument list to a function\n    :return: The pretrained model\n\n    \"\"\"\n    setattr(self.pretrained_model, \"v_head\", self.v_head)\n\n    return self.pretrained_model.push_to_hub(*args, **kwargs)\n</code></pre>"},{"location":"lib-python-EasyDel-_rl_trainer-models-modelling_value_head/#lib.python.EasyDel.rl_trainer.models.modelling_value_head.ValueHead","title":"<code>ValueHead</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>lib/python/EasyDel/rl_trainer/models/modelling_value_head.py</code> <pre><code>class ValueHead(nn.Module):\n    config: typing.Any\n    summary_dropout_prob: float = 0.0\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: typing.Optional[jax.lax.Precision] = jax.lax.Precision('fastest')\n\n    def setup(self):\n        \"\"\"\n        The setup function is called by the model's constructor.\n        It initializes all of the layers in your model, and assigns them to member variables.\n        The setup function should be used for any initialization that needs to happen before running forward().\n        This includes things like loading weights from a file, or setting up an optimizer.\n\n        :param self: Represent the instance of the class\n        :return: A tuple of the following:\n\n        \"\"\"\n        config = self.config\n\n        self.dropout = nn.Dropout(self.summary_dropout_prob)\n\n        if hasattr(config, \"hidden_size\"):\n            hidden_size = config.hidden_size\n        if hasattr(config, \"word_embed_proj_dim\"):\n            hidden_size = config.word_embed_proj_dim\n        elif hasattr(config, \"is_encoder_decoder\"):\n            if config.is_encoder_decoder and hasattr(config, \"decoder\"):\n                if hasattr(config.decoder, \"hidden_size\"):\n                    hidden_size = config.decoder.hidden_size\n\n        self.summary = nn.Dense(\n            1,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n\n    def __call__(self, hidden_states: chex.Array, deterministic: bool = True):\n        \"\"\"\n        The __call__ function is the main function of a class.\n        It is called when an instance of the class (an object) is invoked as a function, e.g., x(arg).\n        The __call__ method enables instances of a class to be called like standard Python functions.\n\n        :param self: Represent the instance of the class\n        :param hidden_states: chex.Array: Pass the hidden states of the previous layer\n        :param deterministic: bool: Determine whether to use dropout\n        :return: A tensor of shape (batch_size, num_classes)\n\n        \"\"\"\n        output = self.dropout(hidden_states, deterministic=deterministic)\n        if output.dtype != self.summary.weight.dtype:\n            output = output.to(self.summary.weight.dtype)\n        return self.summary(output)\n</code></pre>"},{"location":"lib-python-EasyDel-_rl_trainer-models-modelling_value_head/#lib.python.EasyDel.rl_trainer.models.modelling_value_head.ValueHead.__call__","title":"<code>__call__(hidden_states, deterministic=True)</code>","text":"<p>The call function is the main function of a class. It is called when an instance of the class (an object) is invoked as a function, e.g., x(arg). The call method enables instances of a class to be called like standard Python functions.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the hidden states of the previous layer</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout</p> <code>True</code> <p>Returns:</p> Type Description <p>A tensor of shape (batch_size, num_classes)</p> Source code in <code>lib/python/EasyDel/rl_trainer/models/modelling_value_head.py</code> <pre><code>def __call__(self, hidden_states: chex.Array, deterministic: bool = True):\n    \"\"\"\n    The __call__ function is the main function of a class.\n    It is called when an instance of the class (an object) is invoked as a function, e.g., x(arg).\n    The __call__ method enables instances of a class to be called like standard Python functions.\n\n    :param self: Represent the instance of the class\n    :param hidden_states: chex.Array: Pass the hidden states of the previous layer\n    :param deterministic: bool: Determine whether to use dropout\n    :return: A tensor of shape (batch_size, num_classes)\n\n    \"\"\"\n    output = self.dropout(hidden_states, deterministic=deterministic)\n    if output.dtype != self.summary.weight.dtype:\n        output = output.to(self.summary.weight.dtype)\n    return self.summary(output)\n</code></pre>"},{"location":"lib-python-EasyDel-_rl_trainer-models-modelling_value_head/#lib.python.EasyDel.rl_trainer.models.modelling_value_head.ValueHead.setup","title":"<code>setup()</code>","text":"<p>The setup function is called by the model's constructor. It initializes all of the layers in your model, and assigns them to member variables. The setup function should be used for any initialization that needs to happen before running forward(). This includes things like loading weights from a file, or setting up an optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A tuple of the following:</p> Source code in <code>lib/python/EasyDel/rl_trainer/models/modelling_value_head.py</code> <pre><code>def setup(self):\n    \"\"\"\n    The setup function is called by the model's constructor.\n    It initializes all of the layers in your model, and assigns them to member variables.\n    The setup function should be used for any initialization that needs to happen before running forward().\n    This includes things like loading weights from a file, or setting up an optimizer.\n\n    :param self: Represent the instance of the class\n    :return: A tuple of the following:\n\n    \"\"\"\n    config = self.config\n\n    self.dropout = nn.Dropout(self.summary_dropout_prob)\n\n    if hasattr(config, \"hidden_size\"):\n        hidden_size = config.hidden_size\n    if hasattr(config, \"word_embed_proj_dim\"):\n        hidden_size = config.word_embed_proj_dim\n    elif hasattr(config, \"is_encoder_decoder\"):\n        if config.is_encoder_decoder and hasattr(config, \"decoder\"):\n            if hasattr(config.decoder, \"hidden_size\"):\n                hidden_size = config.decoder.hidden_size\n\n    self.summary = nn.Dense(\n        1,\n        dtype=self.dtype,\n        param_dtype=self.param_dtype,\n        precision=self.precision\n    )\n</code></pre>"},{"location":"lib-python-EasyDel-configs-configs/","title":"configs.configs","text":""},{"location":"lib-python-EasyDel-configs-configs/#lib.python.EasyDel.configs.configs.get_config","title":"<code>get_config(model_type, struct)</code>","text":"<p>The get_config function takes in a model_type and struct, and returns the corresponding config.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>str: Determine which model to use</p> required <code>struct</code> <code>str</code> <p>str: Specify the structure of the model</p> required <p>Returns:</p> Type Description <p>A dictionary of hyperparameters</p> Source code in <code>lib/python/EasyDel/configs/configs.py</code> <pre><code>def get_config(model_type: str, struct: str):\n    \"\"\"\n    The get_config function takes in a model_type and struct, and returns the corresponding config.\n\n    :param model_type: str: Determine which model to use\n    :param struct: str: Specify the structure of the model\n    :return: A dictionary of hyperparameters\n\n    \"\"\"\n    if model_type == \"llama\":\n        return llama_configs[struct]\n    elif model_type == \"llama2\":\n        return llama_2_configs[struct]\n    elif model_type == \"opt\":\n        return opt_configs[struct]\n    elif model_type == \"gptj\":\n        return gptj_configs[struct]\n    elif model_type == \"falcon\":\n        return falcon_configs[struct]\n    elif model_type == \"mpt\":\n        return mpt_configs[struct]\n    else:\n        raise ValueError(f\"Unknown ModelType : {model_type}\")\n</code></pre>"},{"location":"lib-python-EasyDel-data_preprocessing-_processor/","title":"data_preprocessing._processor","text":""},{"location":"lib-python-EasyDel-eval-lm_eval/","title":"eval.lm_eval","text":""},{"location":"lib-python-EasyDel-eval-lm_eval/#lib.python.EasyDel.eval.lm_eval.evaluate","title":"<code>evaluate(model, task_list=None, write_out=True, limit=0, shots=5)</code>","text":"<p>The evaluate function takes a model and evaluates it on the tasks specified in task_list. The results are printed to stdout, and optionally written out to a file.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Specify the model to be evaluated</p> required <code>task_list</code> <code>Optional[List[str]]</code> <p>Optional[List[str]]: Specify which tasks to evaluate on</p> <code>None</code> <code>write_out</code> <code>bool</code> <p>bool: Write the output to a file</p> <code>True</code> <code>limit</code> <code>int</code> <p>int: Limit the number of examples that are evaluated</p> <code>0</code> <code>shots</code> <code>int</code> <p>int: Specify how many times to run the model on a given task</p> <code>5</code> <p>Returns:</p> Type Description <p>A dictionary with the following keys</p> Source code in <code>lib/python/EasyDel/eval/lm_eval.py</code> <pre><code>def evaluate(model, task_list: Optional[List[str]] = None, write_out: bool = True, limit: int = 0, shots: int = 5):\n    \"\"\"\n    The evaluate function takes a model and evaluates it on the tasks specified in task_list.\n    The results are printed to stdout, and optionally written out to a file.\n\n\n    :param model: Specify the model to be evaluated\n    :param task_list: Optional[List[str]]: Specify which tasks to evaluate on\n    :param write_out: bool: Write the output to a file\n    :param limit: int: Limit the number of examples that are evaluated\n    :param shots: int: Specify how many times to run the model on a given task\n    :return: A dictionary with the following keys\n\n    \"\"\"\n    if task_list is None:\n        task_list = ['wsc', \"piqa\"]\n\n    for task in task_list:\n        assert task in AVAILABLE_TASKS, f'UnKnown Task {tasks} available tasks are {AVAILABLE_TASKS}'\n    results = evaluator.evaluate(\n        model, tasks.get_task_dict(task_list), False, shots,\n        limit=None if limit &lt;= 0 else limit,\n        write_out=write_out,\n    )\n    pprint.pprint(results)\n    return results\n</code></pre>"},{"location":"lib-python-EasyDel-linen-bits/","title":"linen.bits","text":""},{"location":"lib-python-EasyDel-linen-bits/#lib.python.EasyDel.linen.bits.Dense8Bit","title":"<code>Dense8Bit</code>","text":"<p>             Bases: <code>Dense</code></p> Source code in <code>lib/python/EasyDel/linen/bits.py</code> <pre><code>class Dense8Bit(Dense):\n    @compact\n    def __call__(self, inputs: jax.Array) -&gt; jax.Array:\n        \"\"\"Applies a linear transformation to the inputs along the last dimension.\n\n        Args:\n          inputs: The nd-array to be transformed.\n\n        Returns:\n          The transformed input.\n        \"\"\"\n        if inputs.dtype == jnp.int8:\n            inputs = array_from_8bit(inputs, self.dtype)\n        kernel = array_from_8bit(self.param(\n            'kernel',\n            self.kernel_init,\n            (jnp.shape(inputs)[-1], self.features),\n            self.param_dtype,\n        ), self.param_dtype)\n        if self.use_bias:\n            bias = array_from_8bit(self.param(\n                'bias', self.bias_init, (self.features,), self.param_dtype\n            ), self.param_dtype)\n        else:\n            bias = None\n        inputs, kernel, bias = promote_dtype(inputs, kernel, bias, dtype=self.dtype)\n\n        if self.dot_general_cls is not None:\n            dot_general = self.dot_general_cls()\n        elif self.dot_general is not None:\n            dot_general = self.dot_general\n        else:\n            dot_general = lax.dot_general\n        y = dot_general(\n            inputs,\n            kernel,\n            (((inputs.ndim - 1,), (0,)), ((), ())),\n            precision=self.precision,\n        )\n        if bias is not None:\n            y += jnp.reshape(bias, (1,) * (y.ndim - 1) + (-1,))\n        return array_to_bit8(y)\n</code></pre>"},{"location":"lib-python-EasyDel-linen-bits/#lib.python.EasyDel.linen.bits.Dense8Bit.__call__","title":"<code>__call__(inputs)</code>","text":"<p>Applies a linear transformation to the inputs along the last dimension.</p> <p>Args:   inputs: The nd-array to be transformed.</p> <p>Returns:   The transformed input.</p> Source code in <code>lib/python/EasyDel/linen/bits.py</code> <pre><code>@compact\ndef __call__(self, inputs: jax.Array) -&gt; jax.Array:\n    \"\"\"Applies a linear transformation to the inputs along the last dimension.\n\n    Args:\n      inputs: The nd-array to be transformed.\n\n    Returns:\n      The transformed input.\n    \"\"\"\n    if inputs.dtype == jnp.int8:\n        inputs = array_from_8bit(inputs, self.dtype)\n    kernel = array_from_8bit(self.param(\n        'kernel',\n        self.kernel_init,\n        (jnp.shape(inputs)[-1], self.features),\n        self.param_dtype,\n    ), self.param_dtype)\n    if self.use_bias:\n        bias = array_from_8bit(self.param(\n            'bias', self.bias_init, (self.features,), self.param_dtype\n        ), self.param_dtype)\n    else:\n        bias = None\n    inputs, kernel, bias = promote_dtype(inputs, kernel, bias, dtype=self.dtype)\n\n    if self.dot_general_cls is not None:\n        dot_general = self.dot_general_cls()\n    elif self.dot_general is not None:\n        dot_general = self.dot_general\n    else:\n        dot_general = lax.dot_general\n    y = dot_general(\n        inputs,\n        kernel,\n        (((inputs.ndim - 1,), (0,)), ((), ())),\n        precision=self.precision,\n    )\n    if bias is not None:\n        y += jnp.reshape(bias, (1,) * (y.ndim - 1) + (-1,))\n    return array_to_bit8(y)\n</code></pre>"},{"location":"lib-python-EasyDel-linen-bits/#lib.python.EasyDel.linen.bits.MatmulLtState","title":"<code>MatmulLtState</code>  <code>dataclass</code>","text":"Source code in <code>lib/python/EasyDel/linen/bits.py</code> <pre><code>@dataclass\nclass MatmulLtState:\n    _tile_indices: Optional[jax.Array] = None\n    force_no_igemmlt: bool = False\n    CB = None\n    CxB = None\n    SB = None\n    SCB = None\n\n    CxBt = None\n    SBt = None\n    CBt = None\n\n    subB = None\n\n    outlier_pool = None\n    has_accumulated_gradients = False\n    threshold = 0.0\n    idx = None\n    is_training = True\n    has_fp16_weights = True\n    memory_efficient_backward = False\n    use_pool = False\n    formatB = \"col_turing\"\n\n    def reset_grads(self):\n        \"\"\"\n        The reset_grads function is used to reset the gradients of all the parameters in our model.\n        This function is called after each iteration of training, and before we update our weights.\n        The reason for this is that if we don't reset these values, they will accumulate over time and\n        cause problems with our weight updates.\n\n        :param self: Represent the instance of the class\n        :return: Nothing\n\n        \"\"\"\n        self.CB = None\n        self.CxB = None\n        self.SB = None\n        self.SCB = None\n\n        self.CxBt = None\n        self.SBt = None\n        self.CBt = None\n\n    @property\n    def tile_indices(self):\n        if self._tile_indices is None:\n            self._tile_indices = get_tile_inds(self.formatB)\n        return self._tile_indices\n</code></pre>"},{"location":"lib-python-EasyDel-linen-bits/#lib.python.EasyDel.linen.bits.MatmulLtState.reset_grads","title":"<code>reset_grads()</code>","text":"<p>The reset_grads function is used to reset the gradients of all the parameters in our model. This function is called after each iteration of training, and before we update our weights. The reason for this is that if we don't reset these values, they will accumulate over time and cause problems with our weight updates.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>Nothing</p> Source code in <code>lib/python/EasyDel/linen/bits.py</code> <pre><code>def reset_grads(self):\n    \"\"\"\n    The reset_grads function is used to reset the gradients of all the parameters in our model.\n    This function is called after each iteration of training, and before we update our weights.\n    The reason for this is that if we don't reset these values, they will accumulate over time and\n    cause problems with our weight updates.\n\n    :param self: Represent the instance of the class\n    :return: Nothing\n\n    \"\"\"\n    self.CB = None\n    self.CxB = None\n    self.SB = None\n    self.SCB = None\n\n    self.CxBt = None\n    self.SBt = None\n    self.CBt = None\n</code></pre>"},{"location":"lib-python-EasyDel-linen-bits/#lib.python.EasyDel.linen.bits.aqt_matmul_int8","title":"<code>aqt_matmul_int8(a, w)</code>","text":"<p>The aqt_matmul_int8 function performs the following steps:</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <p>Scale the input tensor</p> required <code>w</code> <p>Store the weights of the model</p> required <p>Returns:</p> Type Description <p>A result that is close to the original matrix multiplication</p> Source code in <code>lib/python/EasyDel/linen/bits.py</code> <pre><code>def aqt_matmul_int8(a, w):\n    \"\"\"\n    The aqt_matmul_int8 function performs the following steps:\n\n    :param a: Scale the input tensor\n    :param w: Store the weights of the model\n    :return: A result that is close to the original matrix multiplication\n\n    \"\"\"\n    max_int8 = 127\n\n    # This function is customizable and injectable, i.e:\n    # users can inject custom quant code into an AQT config.\n    def quant_int8(x):\n        return jnp.clip(jnp.round(x), -max_int8, max_int8).astype(jnp.int8)\n\n    # Calibration. Calibration function is also customizable and injectable.\n    a_s = max_int8 / jnp.max(jnp.abs(a), axis=1, keepdims=True)\n    w_s = max_int8 / jnp.max(jnp.abs(w), axis=0, keepdims=True)\n\n    # int8 matmul with int32 accumulator\n    result = matmul_true_int8(quant_int8(a * a_s), quant_int8(w * w_s)) / (a_s * w_s)\n\n    return result\n</code></pre>"},{"location":"lib-python-EasyDel-linen-bits/#lib.python.EasyDel.linen.bits.get_tile_inds","title":"<code>get_tile_inds(format_)</code>","text":"<p>The get_tile_inds function is used to get the indices of a tile in a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>format_</code> <p>Determine the shape of the tile_inds array</p> required <p>Returns:</p> Type Description <p>The indices of the tiles in a given format</p> Source code in <code>lib/python/EasyDel/linen/bits.py</code> <pre><code>def get_tile_inds(format_):\n    \"\"\"\n    The get_tile_inds function is used to get the indices of a tile in a tensor.\n\n    :param format_: Determine the shape of the tile_inds array\n    :return: The indices of the tiles in a given format\n\n    \"\"\"\n    transform = lambda x: lax.cond(\n        jnp.bitwise_and(jnp.array(format_), 1),\n        lambda _: lax.transpose(x),\n        lambda _: x,\n        operand=None,\n    )\n    inverse_transform = lambda x: lax.cond(\n        jnp.bitwise_and(jnp.array(format_), 1),\n        lambda _: lax.transpose(x),\n        lambda _: x,\n        operand=None,\n    )\n    tile_size = _get_tile_size(format_)\n    tile_inds = lax.broadcasted_iota(jnp.int32, [tile_size] * len(tile_size))\n    tile_inds = inverse_transform(tile_inds)\n    return tile_inds\n</code></pre>"},{"location":"lib-python-EasyDel-linen-utils/","title":"linen.utils","text":""},{"location":"lib-python-EasyDel-linen-utils/#lib.python.EasyDel.linen.utils.from_8bit","title":"<code>from_8bit(tree, dtype=jnp.float32)</code>","text":"<p>takes a pytree(unflatten) and convert that into original pytree</p> Source code in <code>lib/python/EasyDel/linen/utils.py</code> <pre><code>def from_8bit(tree: dict, dtype: jnp.dtype = jnp.float32):\n    \"\"\"\n    takes a pytree(unflatten) and convert that into original pytree\n    \"\"\"\n    tree = flatten_dict(tree)\n\n    for key, array in tree.items():\n        scale_factor = jnp.max(array)\n        array = (array / scale_factor).astype(dtype)\n        tree[key] = array\n    return unflatten_dict(tree)\n</code></pre>"},{"location":"lib-python-EasyDel-linen-utils/#lib.python.EasyDel.linen.utils.to_8bit","title":"<code>to_8bit(tree)</code>","text":"<p>takes a pytree(unflatten) and convert that into 8bit array pytree</p> Source code in <code>lib/python/EasyDel/linen/utils.py</code> <pre><code>def to_8bit(tree: dict):\n    \"\"\"\n    takes a pytree(unflatten) and convert that into 8bit array pytree\n    \"\"\"\n    tree = flatten_dict(tree)\n\n    for key, array in tree.items():\n        scale_factor = 127 / jnp.max(array)\n        array = (array * scale_factor).astype(jnp.int8)\n        tree[key] = array\n    return unflatten_dict(tree)\n</code></pre>"},{"location":"lib-python-EasyDel-modules-auto_models/","title":"modules.auto_models","text":""},{"location":"lib-python-EasyDel-modules-auto_models/#lib.python.EasyDel.modules.auto_models.AutoEasyDelModelForCausalLM","title":"<code>AutoEasyDelModelForCausalLM</code>","text":"Source code in <code>lib/python/EasyDel/modules/auto_models.py</code> <pre><code>class AutoEasyDelModelForCausalLM:\n    @classmethod\n    def from_pretrained(\n            cls,\n            repo_id: str,\n            device=jax.devices('cpu')[0],\n            dtype: jax.numpy.dtype = jax.numpy.float32,\n            param_dtype: jax.numpy.dtype = jax.numpy.float32,\n            precision: jax.lax.Precision = jax.lax.Precision('fastest'),\n            sharding_axis_dims: typing.Sequence[int] = (1, -1, 1, 1),\n            sharding_axis_names: typing.Sequence[str] = ('dp', 'fsdp', 'tp', 'mp'),\n            q_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n            k_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n            v_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n            b_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(\"dp\", None, (\"dp\", \"fsdp\"), None),\n            a_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n            input_shape: typing.Sequence[int] = (1, 1),\n            backend: typing.Optional[str] = None,\n            **kwargs\n    ) -&gt; typing.Union[FlaxPreTrainedModel, dict]:\n        \"\"\"\n        The from_pretrained function is a helper function that allows you to instantiate a model from the pretrained\n        model repository. It takes as input the name of the model (e.g., 'bert-base-uncased') and returns an instance of\n        the class corresponding to your model, with all weights loaded from disk.\n\n        :param cls: Create an instance of the class that called this function\n        :param repo_id: str: Identify the model in the huggingface model hub\n        :param device: Specify the device on which to run the model\n        :param dtype: jax.numpy.dtype: Specify the data type of the model\n        :param param_dtype: jax.numpy.dtype: Specify the dtype of the parameters\n        :param precision: jax.lax.Precision: Control the precision of the model\n        :param sharding_axis_dims: typing.Sequence[int]: Specify the dimension of each axis in the sharded model\n        :param sharding_axis_names: typing.Sequence[str]: Specify the order of sharding\n        :param q_ps: jax.sharding.PartitionSpec: Specify the partitioning of the query tensor\n        :param k_ps: jax.sharding.PartitionSpec: Partition the key matrix\n        :param v_ps: jax.sharding.PartitionSpec: Specify the partitioning of the value tensor\n        :param b_ps: jax.sharding.PartitionSpec: Specify the Attention Bias partition spec\n        :param a_ps: jax.sharding.PartitionSpec: Specify the partitioning of the attention weights\n        :param input_shape: typing.Sequence[int]: Specify the shape of the input to the model\n        :param backend: typing.Optional[str]: backend to use for model\n        :param **kwargs: Pass additional arguments to the model and config classes\n        :return: A model and parameters\n\n        \"\"\"\n\n        config = AutoConfig.from_pretrained(repo_id)\n        model_type = config.model_type\n\n        cfg, module, trf = get_modules_by_type(model_type)\n\n        model = AutoModelForCausalLM.from_pretrained(repo_id, **kwargs)\n        cfg = cfg.from_pretrained(repo_id)\n        if hasattr(cfg, 'add_jax_args'):\n            cfg.add_jax_args()\n        cfg.axis_dims = sharding_axis_dims\n        cfg.axis_names = sharding_axis_names\n        cfg.q_ps = q_ps\n        cfg.k_ps = k_ps\n        cfg.v_ps = v_ps\n        cfg.b_ps = b_ps\n        cfg.a_ps = a_ps\n        cfg.backend = backend\n        ed_model = module(\n            config=cfg,\n            _do_init=False,\n            dtype=dtype,\n            param_dtype=param_dtype,\n            precision=precision,\n            input_shape=input_shape\n        )\n\n        params = trf(model.state_dict(), config=config, device=device)\n        del model,\n        gc.collect()\n\n        if is_flatten(params):\n            params = unflatten_dict(params)\n\n        return ed_model, params\n</code></pre>"},{"location":"lib-python-EasyDel-modules-auto_models/#lib.python.EasyDel.modules.auto_models.AutoEasyDelModelForCausalLM.from_pretrained","title":"<code>from_pretrained(repo_id, device=jax.devices('cpu')[0], dtype=jax.numpy.float32, param_dtype=jax.numpy.float32, precision=jax.lax.Precision('fastest'), sharding_axis_dims=(1, -1, 1, 1), sharding_axis_names=('dp', 'fsdp', 'tp', 'mp'), q_ps=jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None), k_ps=jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None), v_ps=jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None), b_ps=jax.sharding.PartitionSpec('dp', None, ('dp', 'fsdp'), None), a_ps=jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None), input_shape=(1, 1), backend=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>The from_pretrained function is a helper function that allows you to instantiate a model from the pretrained model repository. It takes as input the name of the model (e.g., 'bert-base-uncased') and returns an instance of the class corresponding to your model, with all weights loaded from disk.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>Create an instance of the class that called this function</p> required <code>repo_id</code> <code>str</code> <p>str: Identify the model in the huggingface model hub</p> required <code>device</code> <p>Specify the device on which to run the model</p> <code>devices('cpu')[0]</code> <code>dtype</code> <code>dtype</code> <p>jax.numpy.dtype: Specify the data type of the model</p> <code>float32</code> <code>param_dtype</code> <code>dtype</code> <p>jax.numpy.dtype: Specify the dtype of the parameters</p> <code>float32</code> <code>precision</code> <code>Precision</code> <p>jax.lax.Precision: Control the precision of the model</p> <code>Precision('fastest')</code> <code>sharding_axis_dims</code> <code>Sequence[int]</code> <p>typing.Sequence[int]: Specify the dimension of each axis in the sharded model</p> <code>(1, -1, 1, 1)</code> <code>sharding_axis_names</code> <code>Sequence[str]</code> <p>typing.Sequence[str]: Specify the order of sharding</p> <code>('dp', 'fsdp', 'tp', 'mp')</code> <code>q_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the partitioning of the query tensor</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>k_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Partition the key matrix</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>v_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the partitioning of the value tensor</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>b_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the Attention Bias partition spec</p> <code>PartitionSpec('dp', None, ('dp', 'fsdp'), None)</code> <code>a_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the partitioning of the attention weights</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>input_shape</code> <code>Sequence[int]</code> <p>typing.Sequence[int]: Specify the shape of the input to the model</p> <code>(1, 1)</code> <code>backend</code> <code>Optional[str]</code> <p>typing.Optional[str]: backend to use for model</p> <code>None</code> <code>**kwargs</code> <p>Pass additional arguments to the model and config classes</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[FlaxPreTrainedModel, dict]</code> <p>A model and parameters</p> Source code in <code>lib/python/EasyDel/modules/auto_models.py</code> <pre><code>@classmethod\ndef from_pretrained(\n        cls,\n        repo_id: str,\n        device=jax.devices('cpu')[0],\n        dtype: jax.numpy.dtype = jax.numpy.float32,\n        param_dtype: jax.numpy.dtype = jax.numpy.float32,\n        precision: jax.lax.Precision = jax.lax.Precision('fastest'),\n        sharding_axis_dims: typing.Sequence[int] = (1, -1, 1, 1),\n        sharding_axis_names: typing.Sequence[str] = ('dp', 'fsdp', 'tp', 'mp'),\n        q_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n        k_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n        v_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n        b_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(\"dp\", None, (\"dp\", \"fsdp\"), None),\n        a_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n        input_shape: typing.Sequence[int] = (1, 1),\n        backend: typing.Optional[str] = None,\n        **kwargs\n) -&gt; typing.Union[FlaxPreTrainedModel, dict]:\n    \"\"\"\n    The from_pretrained function is a helper function that allows you to instantiate a model from the pretrained\n    model repository. It takes as input the name of the model (e.g., 'bert-base-uncased') and returns an instance of\n    the class corresponding to your model, with all weights loaded from disk.\n\n    :param cls: Create an instance of the class that called this function\n    :param repo_id: str: Identify the model in the huggingface model hub\n    :param device: Specify the device on which to run the model\n    :param dtype: jax.numpy.dtype: Specify the data type of the model\n    :param param_dtype: jax.numpy.dtype: Specify the dtype of the parameters\n    :param precision: jax.lax.Precision: Control the precision of the model\n    :param sharding_axis_dims: typing.Sequence[int]: Specify the dimension of each axis in the sharded model\n    :param sharding_axis_names: typing.Sequence[str]: Specify the order of sharding\n    :param q_ps: jax.sharding.PartitionSpec: Specify the partitioning of the query tensor\n    :param k_ps: jax.sharding.PartitionSpec: Partition the key matrix\n    :param v_ps: jax.sharding.PartitionSpec: Specify the partitioning of the value tensor\n    :param b_ps: jax.sharding.PartitionSpec: Specify the Attention Bias partition spec\n    :param a_ps: jax.sharding.PartitionSpec: Specify the partitioning of the attention weights\n    :param input_shape: typing.Sequence[int]: Specify the shape of the input to the model\n    :param backend: typing.Optional[str]: backend to use for model\n    :param **kwargs: Pass additional arguments to the model and config classes\n    :return: A model and parameters\n\n    \"\"\"\n\n    config = AutoConfig.from_pretrained(repo_id)\n    model_type = config.model_type\n\n    cfg, module, trf = get_modules_by_type(model_type)\n\n    model = AutoModelForCausalLM.from_pretrained(repo_id, **kwargs)\n    cfg = cfg.from_pretrained(repo_id)\n    if hasattr(cfg, 'add_jax_args'):\n        cfg.add_jax_args()\n    cfg.axis_dims = sharding_axis_dims\n    cfg.axis_names = sharding_axis_names\n    cfg.q_ps = q_ps\n    cfg.k_ps = k_ps\n    cfg.v_ps = v_ps\n    cfg.b_ps = b_ps\n    cfg.a_ps = a_ps\n    cfg.backend = backend\n    ed_model = module(\n        config=cfg,\n        _do_init=False,\n        dtype=dtype,\n        param_dtype=param_dtype,\n        precision=precision,\n        input_shape=input_shape\n    )\n\n    params = trf(model.state_dict(), config=config, device=device)\n    del model,\n    gc.collect()\n\n    if is_flatten(params):\n        params = unflatten_dict(params)\n\n    return ed_model, params\n</code></pre>"},{"location":"lib-python-EasyDel-modules-auto_models/#lib.python.EasyDel.modules.auto_models.get_modules_by_type","title":"<code>get_modules_by_type(model_type)</code>","text":"<p>The get_modules_by_type function is a helper function that returns the following:     1. The config class for the model type specified (e.g., LlamaConfig, FalconConfig)     2. The Flax Model class for the model type specified (e.g., FlaxLlamaForCausalLM, FlaxFalconForCausalLM)     3. A function to convert a HuggingFace pretrained checkpoint into an EasyDel checkpoint</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>str: Determine which model to use</p> required <p>Returns:</p> Type Description <p>A tuple of three elements (BaseConfig,BaseModel,Func To Transform Model from Torch to EasyDeL)</p> Source code in <code>lib/python/EasyDel/modules/auto_models.py</code> <pre><code>def get_modules_by_type(model_type: str):\n    \"\"\"\n    The get_modules_by_type function is a helper function that returns the following:\n        1. The config class for the model type specified (e.g., LlamaConfig, FalconConfig)\n        2. The Flax Model class for the model type specified (e.g., FlaxLlamaForCausalLM, FlaxFalconForCausalLM)\n        3. A function to convert a HuggingFace pretrained checkpoint into an EasyDel checkpoint\n\n    :param model_type: str: Determine which model to use\n    :return: A tuple of three elements (BaseConfig,BaseModel,Func To Transform Model from Torch to EasyDeL)\n\n    \"\"\"\n    if model_type == \"llama\":\n        from .llama import LlamaConfig as _LlamaConfig\n        from .llama import FlaxLlamaForCausalLM as _FlaxLlamaForCausalLM\n        from ..transform import llama_convert_hf_to_flax as _llama_convert_hf_to_flax\n        return (\n            _LlamaConfig,\n            _FlaxLlamaForCausalLM,\n            _llama_convert_hf_to_flax\n        )\n    elif model_type == \"falcon\":\n        from .falcon import FlaxFalconForCausalLM as _FlaxFalconForCausalLM\n        from .falcon import FalconConfig as _FalconConfig\n        from ..transform import falcon_convert_hf_to_flax as _falcon_convert_pt_to_flax\n\n        return (\n            _FalconConfig,\n            _FlaxFalconForCausalLM,\n            _falcon_convert_pt_to_flax\n        )\n    elif model_type == \"mpt\":\n        from .mosaic_mpt import FlaxMptForCausalLM as _FlaxMptForCausalLM\n        from .mosaic_mpt import MptConfig as _MptConfig\n        return (\n            _MptConfig,\n            _FlaxMptForCausalLM,\n            functools.partial(huggingface_to_easydel, embedding_layer_name=\"wte\")\n        )\n\n    elif model_type == \"mistral\":\n        from .mistral import FlaxMistralForCausalLM as _FlaxMistralForCausalLM\n        from .mistral import MistralConfig as _MistralConfig\n        from ..transform import mistral_convert_hf_to_flax as _mistral_convert_hf_to_flax\n        return (\n            _MistralConfig,\n            _FlaxMistralForCausalLM,\n            _mistral_convert_hf_to_flax\n        )\n    elif model_type == \"gptj\":\n        from .gpt_j import FlaxGPTJForCausalLM as _FlaxGPTJForCausalLM\n        from .gpt_j import GPTJConfig as _GPTJConfig\n        return (\n            _GPTJConfig,\n            _FlaxGPTJForCausalLM,\n            functools.partial(huggingface_to_easydel, embedding_layer_name=\"wte\")\n        )\n\n    elif model_type == \"gpt_neox\":\n        from .gpt_neo_x import FlaxGPTNeoXForCausalLM as _FlaxGPTNeoXForCausalLM\n        from .gpt_neo_x import GPTNeoXConfig as _GPTNeoXConfig\n\n        return (\n            _GPTNeoXConfig,\n            _FlaxGPTNeoXForCausalLM,\n            functools.partial(huggingface_to_easydel, embedding_layer_name=\"wte\")\n        )\n    elif model_type == \"palm\":\n        from .palm import FlaxPalmForCausalLM as _FlaxPalmForCausalLM\n        from .palm import PalmConfig as _PalmConfig\n        return (\n            _PalmConfig,\n            _FlaxPalmForCausalLM,\n            functools.partial(huggingface_to_easydel, embedding_layer_name=\"wte\")\n        )\n    elif model_type == \"lt\":\n        from .lucid_transformer import FlaxLTForCausalLM as _FlaxLTForCausalLM\n        from .lucid_transformer import FlaxLTConfig as _FlaxLTConfig\n\n        return (\n            _FlaxLTConfig,\n            _FlaxLTForCausalLM,\n            functools.partial(huggingface_to_easydel, embedding_layer_name=\"wte\")\n        )\n\n    else:\n        raise EasyDelRunTimeError(f'Model Type ({model_type}) is not supported or is not found')\n</code></pre>"},{"location":"lib-python-EasyDel-modules-auto_models/#lib.python.EasyDel.modules.auto_models.is_flatten","title":"<code>is_flatten(pytree)</code>","text":"<p>The is_flatten function checks if the pytree is flattened.     If it is, then the first key in the dictionary will be a tuple of (mpl, mpl_id).     Otherwise, it will be an integer representing mpl_id.</p> <p>Parameters:</p> Name Type Description Default <code>pytree</code> <code>dict</code> <p>dict: Pass the pytree to the function</p> required <p>Returns:</p> Type Description <p>True if the pytree is a flattened tree, and false otherwise</p> Source code in <code>lib/python/EasyDel/modules/auto_models.py</code> <pre><code>def is_flatten(pytree: dict):\n    \"\"\"\n    The is_flatten function checks if the pytree is flattened.\n        If it is, then the first key in the dictionary will be a tuple of (mpl, mpl_id).\n        Otherwise, it will be an integer representing mpl_id.\n\n    :param pytree: dict: Pass the pytree to the function\n    :return: True if the pytree is a flattened tree, and false otherwise\n\n    \"\"\"\n    mpl = [k for k in pytree.keys()][0]\n    return True if isinstance(mpl, tuple) else False\n</code></pre>"},{"location":"lib-python-EasyDel-modules-falcon-modelling_falcon_flax/","title":"modules.falcon.modelling_falcon_flax","text":""},{"location":"lib-python-EasyDel-modules-falcon-modelling_falcon_flax/#lib.python.EasyDel.modules.falcon.modelling_falcon_flax.apply_rotary_pos_embedding","title":"<code>apply_rotary_pos_embedding(tensor, sin_, cos_)</code>","text":"<p>The apply_rotary_pos_embedding function applies a rotary positional embedding to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <p>Pass in the tensor that we want to apply the positional embedding to</p> required <code>sin_</code> <p>Rotate the tensor by half of its length</p> required <code>cos_</code> <p>Multiply the tensor and cosine of the angle</p> required <p>Returns:</p> Type Description <p>A tensor with the same shape as its input,</p> Source code in <code>lib/python/EasyDel/modules/falcon/modelling_falcon_flax.py</code> <pre><code>def apply_rotary_pos_embedding(tensor, sin_, cos_):\n    \"\"\"\n    The apply_rotary_pos_embedding function applies a rotary positional embedding to the input tensor.\n\n    :param tensor: Pass in the tensor that we want to apply the positional embedding to\n    :param sin_: Rotate the tensor by half of its length\n    :param cos_: Multiply the tensor and cosine of the angle\n    :return: A tensor with the same shape as its input,\n\n    \"\"\"\n    return (tensor * cos_) + (_rotate_half(tensor) * sin_)\n</code></pre>"},{"location":"lib-python-EasyDel-modules-falcon-modelling_falcon_flax/#lib.python.EasyDel.modules.falcon.modelling_falcon_flax.built_bloom_alibi","title":"<code>built_bloom_alibi(attention_mask, num_attention_heads)</code>","text":"<p>The built_bloom_alibi function is used to create a bloom alibi for the attention mask. The bloom alibi is used in the Bloom Attention layer to ensure that each token has a unique attention vector, even if it's masked out. This ensures that all tokens have an equal chance of being selected as the most important token in the sequence, which helps with training stability and performance.</p> <p>Parameters:</p> Name Type Description Default <code>attention_mask</code> <p>Mask out the padding tokens in the input sequence</p> required <code>num_attention_heads</code> <p>Determine the number of attention heads in the model</p> required <p>Returns:</p> Type Description <p>A tensor of shape (batch_size, num_attention_heads, 1, sequence_length)</p> Source code in <code>lib/python/EasyDel/modules/falcon/modelling_falcon_flax.py</code> <pre><code>def built_bloom_alibi(attention_mask, num_attention_heads):\n    \"\"\"\n    The built_bloom_alibi function is used to create a bloom alibi for the attention mask.\n    The bloom alibi is used in the Bloom Attention layer to ensure that each token has a unique\n    attention vector, even if it's masked out. This ensures that all tokens have an equal chance of being selected as\n    the most important token in the sequence, which helps with training stability and performance.\n\n    :param attention_mask: Mask out the padding tokens in the input sequence\n    :param num_attention_heads: Determine the number of attention heads in the model\n    :return: A tensor of shape (batch_size, num_attention_heads, 1, sequence_length)\n\n    \"\"\"\n    batch_size, sequence_length = attention_mask.shape\n    cp2 = 2 ** math.floor(math.log2(num_attention_heads))\n    base = jnp.asarray(\n        2 ** (- (2 ** -(math.log2(cp2) - 3))), dtype=jnp.float32\n    )\n    powers = jnp.arange(1, 1 + cp2, dtype=jnp.float32)\n    slops = jnp.power(base, powers)\n    if cp2 != num_attention_heads:\n        extra_base = jnp.asarray(\n            2 ** (-(2 ** -(math.log2(2 * cp2) - 3))), dtype=jnp.float32\n        )\n        num_rem_heads = min(cp2, num_attention_heads - cp2)\n        extra_power = jnp.arange(1, 1 + 2 * num_rem_heads, 2, dtype=jnp.dtype)\n        slops = jnp.concatenate([slops, jnp.power(extra_base, extra_power)], axis=0)\n    arange_tensor = (((jnp.cumsum(attention_mask, axis=-1)) - 1) * attention_mask)[:, jnp.newaxis, :]\n    alibi = slops[..., jnp.newaxis].astype(jnp.bfloat16) * arange_tensor\n    return alibi.reshape(batch_size, num_attention_heads, 1, sequence_length)\n</code></pre>"},{"location":"lib-python-EasyDel-modules-falcon-modelling_falcon_flax/#lib.python.EasyDel.modules.falcon.modelling_falcon_flax.dropout_add","title":"<code>dropout_add(linen_drop, x, residual, deterministic)</code>","text":"<p>The dropout_add function is a helper function that adds the residual to the output of the dropout layer. This is necessary because we want to use deterministic=True when we are evaluating our model, but we still need to add in the residual. The reason for this is that during training, we have two paths through our network: one with dropout and one without. The path without dropout (residual) allows us to backpropagate gradients through both paths at once.</p> <p>Parameters:</p> Name Type Description Default <code>linen_drop</code> <code>Dropout</code> <p>nn.Dropout: Specify the dropout layer</p> required <code>x</code> <code>Array</code> <p>chex.Array: Pass in the input to the dropout layer</p> required <code>residual</code> <code>Array</code> <p>chex.Array: Add the residual to the output of dropout_add</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether the dropout layer is active or not</p> required <p>Returns:</p> Type Description <code>Array</code> <p>A tensor that is the sum of the residual and a dropout layer</p> Source code in <code>lib/python/EasyDel/modules/falcon/modelling_falcon_flax.py</code> <pre><code>def dropout_add(linen_drop: nn.Dropout, x: chex.Array, residual: chex.Array, deterministic: bool) -&gt; chex.Array:\n    \"\"\"\n    The dropout_add function is a helper function that adds the residual to the output of\n    the dropout layer. This is necessary because we want to use deterministic=True when\n    we are evaluating our model, but we still need to add in the residual. The reason for this\n    is that during training, we have two paths through our network: one with dropout and one without.\n    The path without dropout (residual) allows us to backpropagate gradients through both paths at once.\n\n    :param linen_drop: nn.Dropout: Specify the dropout layer\n    :param x: chex.Array: Pass in the input to the dropout layer\n    :param residual: chex.Array: Add the residual to the output of dropout_add\n    :param deterministic: bool: Determine whether the dropout layer is active or not\n    :return: A tensor that is the sum of the residual and a dropout layer\n\n    \"\"\"\n    out = linen_drop(inputs=x, deterministic=deterministic)\n    out = residual + out\n    return out\n</code></pre>"},{"location":"lib-python-EasyDel-modules-falcon-modelling_falcon_flax/#lib.python.EasyDel.modules.falcon.modelling_falcon_flax.precompute_falcon_freq_cis","title":"<code>precompute_falcon_freq_cis(max_position_embedding, head_dim, theta=10000)</code>","text":"<p>The precompute_falcon_freq_cis function is used to precompute the sinusoidal frequencies for the FALCON model. The function takes in three arguments: max_position_embedding, head_dim, and theta. The first two are self-explanatory; the third is a hyperparameter that controls how quickly the frequency increases with position (i.e., how many times higher it will be at position i than at position 0). The default value of 10000 was chosen because it worked well on the tasks we tested.</p> <p>Parameters:</p> Name Type Description Default <code>max_position_embedding</code> <code>int</code> <p>int: Set the maximum length of the sequence</p> required <code>head_dim</code> <code>int</code> <p>int: Determine the size of the positional embedding</p> required <code>theta</code> <code>float</code> <p>float: Adjust the frequency of the sinusoid</p> <code>10000</code> <p>Returns:</p> Type Description <p>A tuple of two arrays</p> Source code in <code>lib/python/EasyDel/modules/falcon/modelling_falcon_flax.py</code> <pre><code>def precompute_falcon_freq_cis(max_position_embedding: int, head_dim: int, theta: float = 10000):\n    \"\"\"\n    The precompute_falcon_freq_cis function is used to precompute the sinusoidal frequencies for the FALCON model.\n    The function takes in three arguments: max_position_embedding, head_dim, and theta. The first two are self-explanatory;\n    the third is a hyperparameter that controls how quickly the frequency increases with position (i.e., how many times\n    higher it will be at position i than at position 0). The default value of 10000 was chosen because it worked well on\n    the tasks we tested.\n\n    :param max_position_embedding: int: Set the maximum length of the sequence\n    :param head_dim: int: Determine the size of the positional embedding\n    :param theta: float: Adjust the frequency of the sinusoid\n    :return: A tuple of two arrays\n\n    \"\"\"\n    inv_freq_cis = 1.0 / (theta ** (jnp.arange(0, head_dim, 2, dtype=jnp.float32) / head_dim))\n    freq = jnp.einsum(\"i , j -&gt; i j\", jnp.arange(max_position_embedding), inv_freq_cis).astype(\"float32\")\n\n    embed = jnp.concatenate((freq, freq), axis=-1)\n    return jnp.sin(embed)[:, :], jnp.cos(embed)[:, :]\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/","title":"modules.flax_modelling_utils","text":""},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.JaxBaseClassModel","title":"<code>JaxBaseClassModel</code>","text":"<p>It initializes all the attributes of an object, and it's called when you create a new instance of that class.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the instance of the class</p> required <code>axis_dims</code> <code>Sequence[int]</code> <p>Sequence[int]: Specify the number of dimensions for each axis</p> <code>(1, -1, 1, 1)</code> <code>axis_names</code> <code>Sequence[str]</code> <p>Sequence[str]: Set the names of the axes</p> <code>('dp', 'fsdp', 'tp', 'mp')</code> <code>q_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the partitioning of the query tensor</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>k_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Partition the key matrix</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>v_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the partitioning of the value tensor</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>b_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the Attention Bias partition spec</p> <code>PartitionSpec('dp', None, ('dp', 'fsdp'), None)</code> <code>a_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the partitioning of the attention weights</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>backend</code> <code>Optional[None]</code> <p>Optional[None]: Specify the backend to use</p> <code>None</code> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>class JaxBaseClassModel:\n    \"\"\"\n    It initializes all the attributes of an object, and it's called when you create a new instance of that class.\n    :param self: Refer to the instance of the class\n    :param axis_dims: Sequence[int]: Specify the number of dimensions for each axis\n    :param axis_names: Sequence[str]: Set the names of the axes\n    :param q_ps: jax.sharding.PartitionSpec: Specify the partitioning of the query tensor\n    :param k_ps: jax.sharding.PartitionSpec: Partition the key matrix\n    :param v_ps: jax.sharding.PartitionSpec: Specify the partitioning of the value tensor\n    :param b_ps: jax.sharding.PartitionSpec: Specify the Attention Bias partition spec\n    :param a_ps: jax.sharding.PartitionSpec: Specify the partitioning of the attention weights\n    :param backend: Optional[None]: Specify the backend to use\n    \"\"\"\n\n    def __init__(\n            self,\n            axis_dims: Sequence[int] = (1, -1, 1, 1),\n            axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"mp\"),\n            q_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n            k_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n            v_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n            b_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(\"dp\", None, (\"dp\", \"fsdp\"), None),\n            a_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n            backend: Optional[None] = None\n    ):\n        \"\"\"\n        The __init__ function is the constructor for a class.\n        It initializes all the attributes of an object, and it's called when you create a new instance of that class.\n\n\n        :param self: Refer to the instance of the class\n        :param axis_dims: Sequence[int]: Specify the number of dimensions for each axis\n        :param axis_names: Sequence[str]: Set the names of the axes\n        :param q_ps: jax.sharding.PartitionSpec: Specify the partitioning of the query tensor\n        :param k_ps: jax.sharding.PartitionSpec: Partition the key matrix\n        :param v_ps: jax.sharding.PartitionSpec: Specify the partitioning of the value tensor\n        :param b_ps: jax.sharding.PartitionSpec: Specify the Attention Bias partition spec\n        :param a_ps: jax.sharding.PartitionSpec: Specify the partitioning of the attention weights\n        :param backend: Optional[None]: Specify the backend to use\n        :return: A new instance of the class\n\n        \"\"\"\n        self.q_ps = q_ps\n        self.k_ps = k_ps\n        self.v_ps = v_ps\n        self.b_ps = b_ps\n        self.a_ps = a_ps\n        self.axis_dims = axis_dims\n        self.axis_names = axis_names\n        self.backend = backend if backend is not None else \"\"\n\n    def jax_mesh(self) -&gt; jax.sharding.Mesh:\n        \"\"\"\n        The jax_mesh function is a helper function that creates a jax.sharding.Mesh object from the\n        axis_dims and axis_names attributes of an object, which are assumed to be lists of integers and strings, respectively.\n        The backend attribute is also used if it exists.\n\n        :param self: Refer to the object itself\n        :return: A jaxMesh\n\n        \"\"\"\n        return create_mesh(\n            axis_dims=self.axis_dims,\n            axis_names=self.axis_names,\n            backend=(self.backend if self.backend is not None else \"\") if hasattr(self, 'backend') else \"\"\n        )\n\n    def get_axis_dims(self) -&gt; Sequence[int]:\n        \"\"\"\n        The get_axis_dims function returns a sequence of integers representing the dimensions of each axis.\n\n        :param self: Represent the instance of the class\n        :return: The dimensions of the axes\n\n        \"\"\"\n        return self.axis_dims\n\n    def get_axis_names(self) -&gt; Sequence[str]:\n        \"\"\"\n        The get_axis_names function returns a list of the names of the axes.\n\n        :param self: Represent the instance of the class\n        :return: A list of the names of all axes\n\n        \"\"\"\n        return self.axis_names\n\n    def get_backend(self) -&gt; str:\n        \"\"\"\n        The get_backend function returns the backend that is currently being used.\n        If no backend has been set, it will return the default JAX backend.\n\n        :param self: Bind the method to an object\n        :return: The backend platform\n\n        \"\"\"\n        return self.backend if not self.backend == \"\" else jax.lib.xla_bridge.get_backend().platform\n\n    @staticmethod\n    def get_flash_attention():\n        \"\"\"\n        The get_flash_attention function is used to get the flash attention value from the database.\n            :returns: The flash attention value from the database.\n\n        :return: A function\n\n        \"\"\"\n        return get_flash_attention()\n\n    def add_pss(\n            self,\n            axis_dims: Sequence[int] = (1, -1, 1, 1),\n            axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"mp\"),\n            q_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n            k_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n            v_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n            b_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\", None),\n            a_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n            backend: Optional[str] = None,\n    ):\n        self.axis_dims = axis_dims\n        self.axis_names = axis_names\n        self.q_ps = q_ps\n        self.k_ps = k_ps\n        self.v_ps = v_ps\n        self.b_ps = b_ps\n        self.a_ps = a_ps\n        self.backend = backend\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.JaxBaseClassModel.__init__","title":"<code>__init__(axis_dims=(1, -1, 1, 1), axis_names=('dp', 'fsdp', 'tp', 'mp'), q_ps=jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None), k_ps=jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None), v_ps=jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None), b_ps=jax.sharding.PartitionSpec('dp', None, ('dp', 'fsdp'), None), a_ps=jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None), backend=None)</code>","text":"<p>The init function is the constructor for a class. It initializes all the attributes of an object, and it's called when you create a new instance of that class.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the instance of the class</p> required <code>axis_dims</code> <code>Sequence[int]</code> <p>Sequence[int]: Specify the number of dimensions for each axis</p> <code>(1, -1, 1, 1)</code> <code>axis_names</code> <code>Sequence[str]</code> <p>Sequence[str]: Set the names of the axes</p> <code>('dp', 'fsdp', 'tp', 'mp')</code> <code>q_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the partitioning of the query tensor</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>k_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Partition the key matrix</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>v_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the partitioning of the value tensor</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>b_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the Attention Bias partition spec</p> <code>PartitionSpec('dp', None, ('dp', 'fsdp'), None)</code> <code>a_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the partitioning of the attention weights</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>backend</code> <code>Optional[None]</code> <p>Optional[None]: Specify the backend to use</p> <code>None</code> <p>Returns:</p> Type Description <p>A new instance of the class</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def __init__(\n        self,\n        axis_dims: Sequence[int] = (1, -1, 1, 1),\n        axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"mp\"),\n        q_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n        k_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n        v_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n        b_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(\"dp\", None, (\"dp\", \"fsdp\"), None),\n        a_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n        backend: Optional[None] = None\n):\n    \"\"\"\n    The __init__ function is the constructor for a class.\n    It initializes all the attributes of an object, and it's called when you create a new instance of that class.\n\n\n    :param self: Refer to the instance of the class\n    :param axis_dims: Sequence[int]: Specify the number of dimensions for each axis\n    :param axis_names: Sequence[str]: Set the names of the axes\n    :param q_ps: jax.sharding.PartitionSpec: Specify the partitioning of the query tensor\n    :param k_ps: jax.sharding.PartitionSpec: Partition the key matrix\n    :param v_ps: jax.sharding.PartitionSpec: Specify the partitioning of the value tensor\n    :param b_ps: jax.sharding.PartitionSpec: Specify the Attention Bias partition spec\n    :param a_ps: jax.sharding.PartitionSpec: Specify the partitioning of the attention weights\n    :param backend: Optional[None]: Specify the backend to use\n    :return: A new instance of the class\n\n    \"\"\"\n    self.q_ps = q_ps\n    self.k_ps = k_ps\n    self.v_ps = v_ps\n    self.b_ps = b_ps\n    self.a_ps = a_ps\n    self.axis_dims = axis_dims\n    self.axis_names = axis_names\n    self.backend = backend if backend is not None else \"\"\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.JaxBaseClassModel.get_axis_dims","title":"<code>get_axis_dims()</code>","text":"<p>The get_axis_dims function returns a sequence of integers representing the dimensions of each axis.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <code>Sequence[int]</code> <p>The dimensions of the axes</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def get_axis_dims(self) -&gt; Sequence[int]:\n    \"\"\"\n    The get_axis_dims function returns a sequence of integers representing the dimensions of each axis.\n\n    :param self: Represent the instance of the class\n    :return: The dimensions of the axes\n\n    \"\"\"\n    return self.axis_dims\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.JaxBaseClassModel.get_axis_names","title":"<code>get_axis_names()</code>","text":"<p>The get_axis_names function returns a list of the names of the axes.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <code>Sequence[str]</code> <p>A list of the names of all axes</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def get_axis_names(self) -&gt; Sequence[str]:\n    \"\"\"\n    The get_axis_names function returns a list of the names of the axes.\n\n    :param self: Represent the instance of the class\n    :return: A list of the names of all axes\n\n    \"\"\"\n    return self.axis_names\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.JaxBaseClassModel.get_backend","title":"<code>get_backend()</code>","text":"<p>The get_backend function returns the backend that is currently being used. If no backend has been set, it will return the default JAX backend.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Bind the method to an object</p> required <p>Returns:</p> Type Description <code>str</code> <p>The backend platform</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def get_backend(self) -&gt; str:\n    \"\"\"\n    The get_backend function returns the backend that is currently being used.\n    If no backend has been set, it will return the default JAX backend.\n\n    :param self: Bind the method to an object\n    :return: The backend platform\n\n    \"\"\"\n    return self.backend if not self.backend == \"\" else jax.lib.xla_bridge.get_backend().platform\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.JaxBaseClassModel.get_flash_attention","title":"<code>get_flash_attention()</code>  <code>staticmethod</code>","text":"<p>The get_flash_attention function is used to get the flash attention value from the database.     :returns: The flash attention value from the database.</p> <p>Returns:</p> Type Description <p>A function</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>@staticmethod\ndef get_flash_attention():\n    \"\"\"\n    The get_flash_attention function is used to get the flash attention value from the database.\n        :returns: The flash attention value from the database.\n\n    :return: A function\n\n    \"\"\"\n    return get_flash_attention()\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.JaxBaseClassModel.jax_mesh","title":"<code>jax_mesh()</code>","text":"<p>The jax_mesh function is a helper function that creates a jax.sharding.Mesh object from the axis_dims and axis_names attributes of an object, which are assumed to be lists of integers and strings, respectively. The backend attribute is also used if it exists.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <p>Returns:</p> Type Description <code>Mesh</code> <p>A jaxMesh</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def jax_mesh(self) -&gt; jax.sharding.Mesh:\n    \"\"\"\n    The jax_mesh function is a helper function that creates a jax.sharding.Mesh object from the\n    axis_dims and axis_names attributes of an object, which are assumed to be lists of integers and strings, respectively.\n    The backend attribute is also used if it exists.\n\n    :param self: Refer to the object itself\n    :return: A jaxMesh\n\n    \"\"\"\n    return create_mesh(\n        axis_dims=self.axis_dims,\n        axis_names=self.axis_names,\n        backend=(self.backend if self.backend is not None else \"\") if hasattr(self, 'backend') else \"\"\n    )\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.add_start_docstrings","title":"<code>add_start_docstrings(*docstr)</code>","text":"<p>The add_start_docstrings function is a decorator that adds the docstrings to the beginning of a function. The add_start_docstrings function takes in an arbitrary number of strings and returns a decorator. The returned decorator takes in one argument, fn, which is assumed to be a function. The docstring for fn is set equal to the concatenation of all the strings passed into add_start_docstrings plus (if it exists) the original docstring for fn.</p> <p>Parameters:</p> Name Type Description Default <code>*docstr</code> <p>Pass in a variable number of arguments to the function</p> <code>()</code> <p>Returns:</p> Type Description <p>A decorator that adds the docstrings to the function</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def add_start_docstrings(*docstr):\n    \"\"\"\n    The add_start_docstrings function is a decorator that adds the docstrings to the beginning of a function.\n    The add_start_docstrings function takes in an arbitrary number of strings and returns a decorator.\n    The returned decorator takes in one argument, fn, which is assumed to be a function. The docstring for fn is set equal to\n    the concatenation of all the strings passed into add_start_docstrings plus (if it exists) the original docstring for fn.\n\n    :param *docstr: Pass in a variable number of arguments to the function\n    :return: A decorator that adds the docstrings to the function\n\n    \"\"\"\n\n    def docstring_decorator(fn):\n        fn.__doc__ = \"\".join(docstr) + (fn.__doc__ if fn.__doc__ is not None else \"\")\n        return fn\n\n    return docstring_decorator\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.apply_rotary_pos_emb","title":"<code>apply_rotary_pos_emb(tensor, sin_, cos_)</code>","text":"<p>The apply_rotary_pos_emb function applies a rotary positional embedding to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <p>Store the tensor that is passed into the function</p> required <code>sin_</code> <p>Rotate the tensor by pi/2</p> required <code>cos_</code> <p>Apply the cosine function to the tensor</p> required <p>Returns:</p> Type Description <p>A tensor with the same shape as the input tensor</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def apply_rotary_pos_emb(tensor, sin_, cos_):\n    \"\"\"\n    The apply_rotary_pos_emb function applies a rotary positional embedding to the input tensor.\n\n    :param tensor: Store the tensor that is passed into the function\n    :param sin_: Rotate the tensor by pi/2\n    :param cos_: Apply the cosine function to the tensor\n    :return: A tensor with the same shape as the input tensor\n\n    \"\"\"\n    return (tensor * cos_) + (rotate_half(tensor) * sin_)\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.canonicalize_dtype","title":"<code>canonicalize_dtype(*args, dtype=None, inexact=True)</code>","text":"<p>Canonicalize an optional dtype to the definitive dtype.</p> <p>If the <code>dtype</code> is None this function will infer the dtype. If it is not None it will be returned unmodified or an exceptions is raised if the dtype is invalid. from the input arguments using <code>jnp.result_type</code>.</p> <p>Args:   args: JAX array compatible values. None values     are ignored.   dtype: Optional dtype override. If specified the arguments are cast to     the specified dtype instead and dtype inference is disabled.   inexact: When True, the output dtype must be a subdtype   of <code>jnp.inexact</code>. Inexact dtypes are real or complex floating points. This   is useful when you want to apply operations that don't work directly on   integers like taking a mean for example. Returns:   The dtype that args should be cast to.</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def canonicalize_dtype(\n        *args, dtype: Optional[chex.ArrayDType] = None, inexact: bool = True\n) -&gt; chex.ArrayDType:\n    \"\"\"Canonicalize an optional dtype to the definitive dtype.\n\n    If the ``dtype`` is None this function will infer the dtype. If it is not\n    None it will be returned unmodified or an exceptions is raised if the dtype\n    is invalid.\n    from the input arguments using ``jnp.result_type``.\n\n    Args:\n      *args: JAX array compatible values. None values\n        are ignored.\n      dtype: Optional dtype override. If specified the arguments are cast to\n        the specified dtype instead and dtype inference is disabled.\n      inexact: When True, the output dtype must be a subdtype\n      of `jnp.inexact`. Inexact dtypes are real or complex floating points. This\n      is useful when you want to apply operations that don't work directly on\n      integers like taking a mean for example.\n    Returns:\n      The dtype that *args should be cast to.\n    \"\"\"\n    if dtype is None:\n        args_filtered = [jax.numpy.asarray(x) for x in args if x is not None]\n        dtype = jax.numpy.result_type(*args_filtered)\n        if inexact and not jax.numpy.issubdtype(dtype, jax.numpy.inexact):\n            dtype = jax.numpy.promote_types(jax.numpy.float32, dtype)\n    if inexact and not jax.numpy.issubdtype(dtype, jax.numpy.inexact):\n        raise ValueError(f'Dtype must be inexact: {dtype}')\n    return dtype\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.create_mesh","title":"<code>create_mesh(axis_dims=(1, -1, 1, 1), axis_names=('dp', 'fsdp', 'tp', 'mp'), backend='')</code>","text":"<p>The create_mesh function creates a mesh object that can be used to shard arrays.</p> <p>Parameters:</p> Name Type Description Default <code>axis_dims</code> <code>Sequence[int]</code> <p>Sequence[int]: Specify the dimensions of the mesh</p> <code>(1, -1, 1, 1)</code> <code>axis_names</code> <code>Sequence[str]</code> <p>Sequence[str]: Name the axes of the mesh</p> <code>('dp', 'fsdp', 'tp', 'mp')</code> <code>backend</code> <p>Specify the backend to use</p> <code>''</code> <p>Returns:</p> Type Description <p>A mesh object</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def create_mesh(\n        axis_dims: Sequence[int] = (1, -1, 1, 1), axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"mp\"), backend=\"\"\n):\n    \"\"\"\n    The create_mesh function creates a mesh object that can be used to shard arrays.\n\n    :param axis_dims: Sequence[int]: Specify the dimensions of the mesh\n    :param axis_names: Sequence[str]: Name the axes of the mesh\n    :param backend: Specify the backend to use\n    :return: A mesh object\n\n    \"\"\"\n    array_devices = jax.numpy.ones((len(jax.devices() if backend == \"\" else jax.devices(backend)), 1))\n    resh = array_devices.reshape(axis_dims).shape\n\n    return jax.sharding.Mesh(\n        create_device_mesh(resh), axis_names\n    )\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.get_flash_attention","title":"<code>get_flash_attention()</code>","text":"<p>return: FlashAttention FN, Upcast Needed to float32,do_shard_map</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def get_flash_attention():\n    \"\"\"\n    return: FlashAttention FN, Upcast Needed to float32,do_shard_map\n    \"\"\"\n    platform = jax.lib.xla_bridge.get_backend().platform\n    if platform == \"gpu\":\n        float32_logits = False\n        ring_attention_fn = fjformer.attention.ring_flash_attention_gpu\n        do_shard_map = True\n    elif platform == \"tpu\":\n        float32_logits = True\n        ring_attention_fn = fjformer.attention.tpu_flash_attention\n        do_shard_map = False\n    else:\n        raise ValueError(f\"Unsupported platform {platform}\")\n\n    return ring_attention_fn, float32_logits, do_shard_map\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.get_gradient_checkpoint_policy","title":"<code>get_gradient_checkpoint_policy(name)</code>","text":"<p>The get_gradient_checkpoint_policy function is a helper function that returns the gradient checkpoint policy     specified by the name parameter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>Select the checkpoint policy from the dictionary</p> required <p>Returns:</p> Type Description <p>A function that is used in the jax</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def get_gradient_checkpoint_policy(name):\n    \"\"\"\n    The get_gradient_checkpoint_policy function is a helper function that returns the gradient checkpoint policy\n        specified by the name parameter.\n\n    :param name: Select the checkpoint policy from the dictionary\n    :return: A function that is used in the jax\n\n    \"\"\"\n    gradients = dict(\n        everything_saveable=jax.checkpoint_policies.everything_saveable,\n        nothing_saveable=jax.checkpoint_policies.nothing_saveable,\n        dots_saveable=jax.checkpoint_policies.dots_saveable,\n        checkpoint_dots=jax.checkpoint_policies.checkpoint_dots,\n        dots_with_no_batch_dims_saveable=jax.checkpoint_policies.dots_with_no_batch_dims_saveable,\n        checkpoint_dots_with_no_batch_dims=jax.checkpoint_policies.checkpoint_dots_with_no_batch_dims,\n        save_anything_except_these_names=jax.checkpoint_policies.save_anything_except_these_names,\n        save_any_names_but_these=jax.checkpoint_policies.save_any_names_but_these,\n        save_only_these_names=jax.checkpoint_policies.save_only_these_names,\n        save_from_both_policies=jax.checkpoint_policies.save_from_both_policies\n    )\n    return gradients[name]\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.get_names_from_partition_spec","title":"<code>get_names_from_partition_spec(partition_specs)</code>","text":"<p>The get_names_from_partition_spec function takes a partition_specs argument, which is either a dictionary or list. If it's a dictionary, the function converts it to a list of values. Then for each item in the partition_specs list:     If the item is None, continue (do nothing) and move on to next iteration of loop.     If the item is an instance of str (i.e., if it's just one string), add that string to names set and move     on to next iteration of loop.     Otherwise, (if not None or str), call get_names_from_partition_spec recurs</p> <p>Parameters:</p> Name Type Description Default <code>partition_specs</code> <p>Define the partitioning of a table</p> required <p>Returns:</p> Type Description <p>A list of the names of all partitions</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def get_names_from_partition_spec(partition_specs):\n    \"\"\"\n    The get_names_from_partition_spec function takes a partition_specs argument, which is either a dictionary or list.\n    If it's a dictionary, the function converts it to a list of values. Then for each item in the partition_specs list:\n        If the item is None, continue (do nothing) and move on to next iteration of loop.\n        If the item is an instance of str (i.e., if it's just one string), add that string to names set and move\n        on to next iteration of loop.\n        Otherwise, (if not None or str), call get_names_from_partition_spec recurs\n\n    :param partition_specs: Define the partitioning of a table\n    :return: A list of the names of all partitions\n\n    \"\"\"\n    names = set()\n    if isinstance(partition_specs, dict):\n        partition_specs = partition_specs.values()\n    for item in partition_specs:\n        if item is None:\n            continue\n        elif isinstance(item, str):\n            names.add(item)\n        else:\n            names.update(get_names_from_partition_spec(item))\n\n    return list(names)\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.get_ranks_and_size","title":"<code>get_ranks_and_size(mesh)</code>","text":"<p>The get_ranks_and_size function is used to determine the number of MPI processes (<code>mp_node_size</code>) and the number of devices per process (<code>dp_node_size</code>). The <code>mesh.shape[&amp;quot;tp&amp;quot;] * mesh.shape[&amp;quot;mp&amp;quot;]</code> determines how many MPI processes are needed, and then we divide that by the local device count to get <code>`mp_node_size = max( 1, mp / jax.local )</code>. This means that if there are more than enough devices for all MPI ranks on a node, each rank will only use one device; otherwise it will use</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <p>Get the shape of the mesh</p> required <p>Returns:</p> Type Description <p>A dictionary with the following keys:</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def get_ranks_and_size(mesh):\n    \"\"\"\n    The get_ranks_and_size function is used to determine the number of MPI processes\n    (``mp_node_size``) and the number of devices per process (``dp_node_size``).\n    The ``mesh.shape[&amp;quot;tp&amp;quot;] * mesh.shape[&amp;quot;mp&amp;quot;]`` determines how many MPI processes are needed,\n    and then we divide that by the local device count to get ``mp_node_size = max( 1, mp / jax.local )`.\n    This means that if there are more than enough devices for all MPI ranks on a node, each rank will only use one device; otherwise it will use\n\n    :param mesh: Get the shape of the mesh\n    :return: A dictionary with the following keys:\n\n    \"\"\"\n    out = dict(mesh=mesh)\n    mp_size = mesh.shape[\"tp\"] * mesh.shape[\"mp\"]\n    mp_node_size = max(1, mp_size // jax.local_device_count())\n    dp_node_size = jax.process_count() // mp_node_size\n    out.update(mp_node_size=mp_node_size,\n               dp_node_size=dp_node_size)\n\n    dp_node_rank = jax.process_index() // mp_node_size\n    mp_node_rank = jax.process_index() % mp_node_size\n    out.update(dp_node_rank=dp_node_rank,\n               mp_node_rank=mp_node_rank)\n    return out\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.names_in_mesh","title":"<code>names_in_mesh(*names)</code>","text":"<p>The names_in_mesh function is a decorator that can be used to check whether the names of the axes passed into a function are valid.  It will raise an exception if any of the axis names are not in the physical mesh.  For example, if you have a function that takes two axes as arguments, and you want to make sure they're both in your mesh:</p> <p>Parameters:</p> Name Type Description Default <code>*names</code> <p>Collect all the names passed to the function into a tuple</p> <code>()</code> <p>Returns:</p> Type Description <p>A boolean indicating whether all the given</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def names_in_mesh(*names):\n    \"\"\"\n    The names_in_mesh function is a decorator that can be used to check whether\n    the names of the axes passed into a function are valid.  It will raise an\n    exception if any of the axis names are not in the physical mesh.  For example,\n    if you have a function that takes two axes as arguments, and you want to make sure they're both in your mesh:\n\n    :param *names: Collect all the names passed to the function into a tuple\n    :return: A boolean indicating whether all the given\n\n    \"\"\"\n    return set(names) &lt;= set(pxla.thread_resources.env.physical_mesh.axis_names)\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.precompute_freq_cis","title":"<code>precompute_freq_cis(max_position_embedding, head_dim)</code>","text":"<p>The precompute_freq_cis function is used to precompute the sinusoidal embeddings for positional encoding.</p> <p>Parameters:</p> Name Type Description Default <code>max_position_embedding</code> <p>Define the maximum length of the sequence</p> required <code>head_dim</code> <p>Determine the number of heads in the attention layer</p> required <p>Returns:</p> Type Description <p>Two arrays:</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def precompute_freq_cis(max_position_embedding, head_dim):\n    \"\"\"\n    The precompute_freq_cis function is used to precompute the sinusoidal embeddings for positional encoding.\n\n    :param max_position_embedding: Define the maximum length of the sequence\n    :param head_dim: Determine the number of heads in the attention layer\n    :return: Two arrays:\n\n    \"\"\"\n    inv_freq = 1.0 / (10000 ** (jax.numpy.arange(0, head_dim, 2, dtype=jax.numpy.float32) / head_dim))\n    freq = jax.numpy.einsum(\"i , j -&gt; i j\", jax.numpy.arange(max_position_embedding), inv_freq).astype(\"float32\")\n\n    embed = jax.numpy.concatenate((freq, freq), axis=-1)\n    return jax.numpy.sin(embed)[:, :], jax.numpy.cos(embed)[:, :]\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.repeat_kv_bnsh","title":"<code>repeat_kv_bnsh(x, n_rep)</code>","text":"<p>The repeat_kv_bnsh function is used to repeat the key and value vectors for each head in a multi-head attention module. This function takes as input an array of shape (batch_size, n_heads, sequence_length, head_dim) and returns an array of shape (batch_size, n_heads * nrep, sequence length, head dim). The reason this is necessary is because the attention module expects keys/values/queries to be repeated across heads but not across batches. However we want our keys/values/queries to be repeated both across heads AND batches so that we can use them</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>chex.Array: Pass in the input to the function</p> required <code>n_rep</code> <code>int</code> <p>int: Repeat the key and value heads</p> required <p>Returns:</p> Type Description <code>Array</code> <p>A new array with the same shape as x, except for the second dimension which is n_kv_heads * n_rep</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def repeat_kv_bnsh(x: chex.Array, n_rep: int) -&gt; chex.Array:\n    \"\"\"\n    The repeat_kv_bnsh function is used to repeat the key and value vectors for each head in a multi-head attention\n    module. This function takes as input an array of shape (batch_size, n_heads, sequence_length, head_dim) and returns\n    an array of shape (batch_size, n_heads * nrep, sequence length, head dim). The reason this is necessary is because the\n    attention module expects keys/values/queries to be repeated across heads but not across batches. However we want our\n    keys/values/queries to be repeated both across heads AND batches so that we can use them\n\n    :param x: chex.Array: Pass in the input to the function\n    :param n_rep: int: Repeat the key and value heads\n    :return: A new array with the same shape as x, except for the second dimension which is n_kv_heads * n_rep\n\n    \"\"\"\n    bs, n_kv_heads, s, head_dim = x.shape\n    if n_rep == 1:\n        return x\n    x = x[:, :, jax.numpy.newaxis, :, :]\n    x = jax.numpy.repeat(x, n_rep, axis=2)\n\n    return x.reshape(bs, n_kv_heads * n_rep, s, head_dim)\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.repeat_kv_bsnh","title":"<code>repeat_kv_bsnh(x, n_rep)</code>","text":"<p>The repeat_kv_bsnh function is used to repeat the key and value vectors for each head.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Array</code> <p>chex.Array: Specify the input array</p> required <code>n_rep</code> <code>int</code> <p>int: Repeat the key-value attention heads n_rep times</p> required <p>Returns:</p> Type Description <code>Array</code> <p>A new array with the same batch size, sequence length, and head dimension as the input array</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def repeat_kv_bsnh(x: chex.Array, n_rep: int) -&gt; chex.Array:\n    \"\"\"\n    The repeat_kv_bsnh function is used to repeat the key and value vectors for each head.\n\n    :param x: chex.Array: Specify the input array\n    :param n_rep: int: Repeat the key-value attention heads n_rep times\n    :return: A new array with the same batch size, sequence length, and head dimension as the input array\n\n    \"\"\"\n    bs, s, n_kv_heads, head_dim = x.shape\n    x = x.transpose(0, 2, 1, 3)\n    if n_rep == 1:\n        return x\n    x = x[:, :, jax.numpy.newaxis, :, :]\n    x = jax.numpy.repeat(x, n_rep, axis=2)\n\n    x = x.transpose(0, 2, 1, 3)\n\n    return x.reshape(bs, s, n_kv_heads * n_rep, head_dim)\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.rotate_half","title":"<code>rotate_half(x)</code>","text":"<p>The rotate_half function takes a complex-valued array and rotates the phase of its second half by 180 degrees. This is equivalent to multiplying the second half by -i, or equivalently rotating it 90 degrees counterclockwise.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Specify the input array</p> required <p>Returns:</p> Type Description <p>A new array that is the same as the input</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def rotate_half(x):\n    \"\"\"\n    The rotate_half function takes a complex-valued array and rotates the\n    phase of its second half by 180 degrees. This is equivalent to multiplying\n    the second half by -i, or equivalently rotating it 90 degrees counterclockwise.\n\n\n    :param x: Specify the input array\n    :return: A new array that is the same as the input\n\n    \"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2:]\n    return jax.numpy.concatenate((-x2, x1), axis=-1)\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.smart_flash_attention","title":"<code>smart_flash_attention(q, k, v, bias, q_ps, k_ps, v_ps, b_ps, a_ps, block_k, block_q, block_b, q_seq_len, kv_seq_len, num_attention_heads, head_dims, causal, attn_pdrop, mesh=None, dtype=jax.numpy.float32, precision=jax.lax.Precision('fastest'), dropout_rng=None, force_float32_tpu=True, deterministic=False)</code>","text":"<p>Smart Flash Attention mechanism for efficient attention computation.</p> <p>Parameters:</p> Name Type Description Default <code>q</code> <code>Array</code> <p>Query tensor with shape [batch_size, num_attention_heads, q_seq_len, head_dims].</p> required <code>k</code> <code>Array</code> <p>Key tensor with shape [batch_size, num_attention_heads, kv_seq_len, head_dims].</p> required <code>v</code> <code>Array</code> <p>Value tensor with shape [batch_size, num_attention_heads, kv_seq_len, head_dims].</p> required <code>bias</code> <code>Array</code> <p>Bias tensor with shape [batch_size, num_attention_heads, q_seq_len, kv_seq_len].</p> required <code>q_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the partitioning of the query tensor</p> required <code>k_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Partition the key matrix</p> required <code>v_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the partitioning of the value tensor</p> required <code>b_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the Attention Bias partition spec</p> required <code>a_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the partitioning of the attention weights</p> required <code>block_k</code> <code>int</code> <p>Block size for key tensor reshaping.</p> required <code>block_q</code> <code>int</code> <p>Block size for query tensor reshaping.</p> required <code>block_b</code> <code>int</code> <p>Block size for bias tensor reshaping.</p> required <code>q_seq_len</code> <code>int</code> <p>Length of the query sequence.</p> required <code>kv_seq_len</code> <code>int</code> <p>Length of the key-value sequence.</p> required <code>num_attention_heads</code> <code>int</code> <p>Number of attention heads.</p> required <code>head_dims</code> <code>int</code> <p>Dimensionality of each attention head.</p> required <code>causal</code> <code>bool</code> <p>If True, applies causal masking to the attention scores.</p> required <code>attn_pdrop</code> <code>float</code> <p>Dropout probability for attention weights.</p> required <code>mesh</code> <code>Mesh</code> <p>Mesh specifying the data distribution for parallel computation.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type of the tensors.</p> <code>float32</code> <code>precision</code> <code>Precision</code> <p>Precision mode for computation (default is 'fastest').</p> <code>Precision('fastest')</code> <code>dropout_rng</code> <code>PRNGKey</code> <p>Random number generator key for dropout.</p> <code>None</code> <code>force_float32_tpu</code> <code>bool</code> <p>If True, forces computation to use float32 on TPU.</p> <code>True</code> <code>deterministic</code> <code>bool</code> <p>If True, ensures deterministic computation.</p> <code>False</code> <p>Returns:</p> Type Description <code>tensor</code> <p>chex.Array: Output tensor with the same shape as the input value tensor v.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the shapes of input tensors are not compatible for attention computation.</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def smart_flash_attention(\n        q: chex.Array,\n        k: chex.Array,\n        v: chex.Array,\n        bias: chex.Array,\n        q_ps: jax.sharding.PartitionSpec,\n        k_ps: jax.sharding.PartitionSpec,\n        v_ps: jax.sharding.PartitionSpec,\n        b_ps: jax.sharding.PartitionSpec,\n        a_ps: jax.sharding.PartitionSpec,\n        block_k: int,\n        block_q: int,\n        block_b: int,\n        q_seq_len: int,\n        kv_seq_len: int,\n        num_attention_heads: int,\n        head_dims: int,\n        causal: bool,\n        attn_pdrop: float,\n        mesh: jax.sharding.Mesh = None,\n        dtype: jax.numpy.dtype = jax.numpy.float32,\n        precision: jax.lax.Precision = jax.lax.Precision('fastest'),\n        dropout_rng: jax.random.PRNGKey = None,\n        force_float32_tpu: bool = True,\n        deterministic: bool = False\n) -&gt; chex.Array:\n    \"\"\"\n    Smart Flash Attention mechanism for efficient attention computation.\n\n    :param q: Query tensor with shape [batch_size, num_attention_heads, q_seq_len, head_dims].\n    :type q: tensor\n\n    :param k: Key tensor with shape [batch_size, num_attention_heads, kv_seq_len, head_dims].\n    :type k: tensor\n\n    :param v: Value tensor with shape [batch_size, num_attention_heads, kv_seq_len, head_dims].\n    :type v: tensor\n\n    :param bias: Bias tensor with shape [batch_size, num_attention_heads, q_seq_len, kv_seq_len].\n    :type bias: tensor\n\n    :param q_ps: jax.sharding.PartitionSpec: Specify the partitioning of the query tensor\n\n    :param k_ps: jax.sharding.PartitionSpec: Partition the key matrix\n\n    :param v_ps: jax.sharding.PartitionSpec: Specify the partitioning of the value tensor\n\n    :param b_ps: jax.sharding.PartitionSpec: Specify the Attention Bias partition spec\n\n    :param a_ps: jax.sharding.PartitionSpec: Specify the partitioning of the attention weights\n\n    :param block_k: Block size for key tensor reshaping.\n    :type block_k: int\n\n    :param block_q: Block size for query tensor reshaping.\n    :type block_q: int\n\n    :param block_b: Block size for bias tensor reshaping.\n    :type block_b: int\n\n    :param q_seq_len: Length of the query sequence.\n    :type q_seq_len: int\n\n    :param kv_seq_len: Length of the key-value sequence.\n    :type kv_seq_len: int\n\n    :param num_attention_heads: Number of attention heads.\n    :type num_attention_heads: int\n\n    :param head_dims: Dimensionality of each attention head.\n    :type head_dims: int\n\n    :param causal: If True, applies causal masking to the attention scores.\n    :type causal: bool\n\n    :param attn_pdrop: Dropout probability for attention weights.\n    :type attn_pdrop: float\n\n    :param mesh: Mesh specifying the data distribution for parallel computation.\n    :type mesh: mesh_type\n\n    :param dtype: Data type of the tensors.\n    :type dtype: data_type\n\n    :param precision: Precision mode for computation (default is 'fastest').\n    :type precision: str\n\n    :param dropout_rng: Random number generator key for dropout.\n    :type dropout_rng: rng_key\n\n    :param force_float32_tpu: If True, forces computation to use float32 on TPU.\n    :type force_float32_tpu: bool\n\n    :param deterministic: If True, ensures deterministic computation.\n    :type deterministic: bool\n\n    :return: chex.Array: Output tensor with the same shape as the input value tensor v.\n    :rtype: tensor\n\n    :raises ValueError: If the shapes of input tensors are not compatible for attention computation.\n    \"\"\"\n    assertion_mkv_err = \"\"\"\n    Q,K,V and bias shapes must be like\n    Q Shape : [batch_size, num_attention_heads, q_seq_len, head_dims]\n    K Shape : [batch_size, num_attention_heads, kv_seq_len, head_dims]\n    V Shape : [batch_size, num_attention_heads, kv_seq_len, head_dims]\n    bias Shape : [batch_size, num_attention_heads, q_seq_len, kv_seq_len]\n    \"\"\"\n    batch_size = q.shape[0]\n    assert batch_size == k.shape[0] == v.shape[0], 'Batch Size for q,k,v wont match'\n\n    assert q.shape == (batch_size, num_attention_heads, q_seq_len, head_dims), assertion_mkv_err\n    assert k.shape == (batch_size, num_attention_heads, kv_seq_len, head_dims), assertion_mkv_err\n    assert v.shape == (batch_size, num_attention_heads, kv_seq_len, head_dims), assertion_mkv_err\n    assert bias.shape == (batch_size, num_attention_heads, q_seq_len, kv_seq_len), assertion_mkv_err\n\n    flash_attn_fn, f32_upcast, do_shard_map = get_flash_attention()\n\n    if do_shard_map:\n        q, k, v = map(lambda x: jax.numpy.transpose(x, (0, 2, 1, 3)), [q, k, v])\n        assert mesh is not None, 'For Using Shard Map on GPUs you have to pass Mesh'\n        ring_attention_sharded = shard_map(\n            partial(\n                flash_attn_fn,\n                axis_name=\"mp\",\n                float32_logits=f32_upcast,\n                blockwise_kwargs=dict(\n                    deterministic=deterministic,\n                    dropout_rng=dropout_rng,\n                    attn_pdrop=attn_pdrop,\n                    causal=causal,\n                    query_chunk_size=block_q,\n                    key_chunk_size=block_k,\n                    dtype=dtype,\n                    policy=jax.checkpoint_policies.nothing_saveable,\n                    precision=precision,\n                    prevent_cse=False,\n                )\n            ),\n            mesh=mesh,\n            in_specs=(\n                q_ps,\n                k_ps,\n                v_ps,\n                b_ps\n            ),\n            out_specs=a_ps,\n            check_rep=False\n        )\n        attn_output = ring_attention_sharded(q, k, v, bias)\n        attn_output = with_sharding_constraint(attn_output, a_ps)\n    else:\n        if force_float32_tpu or f32_upcast:\n            q, k, v = map(lambda x: x.astype(jax.numpy.float32), [q, k, v])\n        attn_output = fjformer.attention.jax_flash_attn_tpu.flash_attention(\n            q,\n            k,\n            v,\n            bias,\n            None,\n            causal=False,\n            sm_scale=1.0,\n            block_sizes=fjformer.attention.jax_flash_attn_tpu.BlockSizes(\n                block_b=block_b,\n                block_k=block_k,\n                block_q=block_q,\n                block_k_major=block_k\n            ),\n            debug=False,\n        )\n\n    attn_output = attn_output.astype(dtype)\n    return attn_output\n</code></pre>"},{"location":"lib-python-EasyDel-modules-flax_modelling_utils/#lib.python.EasyDel.modules.flax_modelling_utils.with_sharding_constraint","title":"<code>with_sharding_constraint(x, partition_specs)</code>","text":"<p>The with_sharding_constraint function is used to ensure that the sharding of a tensor is consistent with the sharding of its inputs.  This function should be called on any tensor which has been created by an operation which does not automatically handle this, such as tf.concat or tf.split.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Define the tensor that will be sharded</p> required <code>partition_specs</code> <p>Specify the partitioning of the data</p> required <p>Returns:</p> Type Description <p>The same tensor with the</p> Source code in <code>lib/python/EasyDel/modules/flax_modelling_utils.py</code> <pre><code>def with_sharding_constraint(x, partition_specs):\n    \"\"\"\n    The with_sharding_constraint function is used to ensure that the sharding of a tensor\n    is consistent with the sharding of its inputs.  This function should be called on any\n    tensor which has been created by an operation which does not automatically handle this,\n    such as tf.concat or tf.split.\n\n    :param x: Define the tensor that will be sharded\n    :param partition_specs: Specify the partitioning of the data\n    :return: The same tensor with the\n\n    \"\"\"\n    axis_names = get_names_from_partition_spec(partition_specs)\n    if names_in_mesh(*axis_names):\n        x = wsc(x, partition_specs)\n    return x\n</code></pre>"},{"location":"lib-python-EasyDel-modules-gpt_j-modelling_gpt_j_flax/","title":"modules.gpt_j.modelling_gpt_j_flax","text":"<p>GPT-J model configuration</p>"},{"location":"lib-python-EasyDel-modules-gpt_neo_x-modelling_gpt_neo_x_flax/","title":"modules.gpt_neo_x.modelling_gpt_neo_x_flax","text":""},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/","title":"modules.llama.modelling_llama_flax","text":""},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaAttention","title":"<code>FlaxLlamaAttention</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>class FlaxLlamaAttention(nn.Module):\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        config = self.config\n        self.hidden_size = config.hidden_size\n        self.head_dim = self.config.hidden_size // self.config.num_attention_heads\n        self.number_of_reps = self.config.num_attention_heads // self.config.num_key_value_heads\n\n        if self.config.bits is not None:\n            _dot_general_cls = q_config.fully_quantized(\n                fwd_bits=self.config.bits,\n                bwd_bits=self.config.bits\n            )\n        else:\n            _dot_general_cls = None\n\n        dot_general_cls = q_flax.QDotGeneral(_dot_general_cls)\n\n        if self.number_of_reps == 1:\n            assert self.config.num_attention_heads == self.config.num_key_value_heads\n        self.q_proj = nn.Dense(\n            config.num_attention_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.attention_bias,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n            precision=self.precision,\n            dot_general=dot_general_cls\n        )\n        self.k_proj = nn.Dense(\n            config.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.attention_bias,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n            precision=self.precision,\n            dot_general=dot_general_cls\n        )\n        self.v_proj = nn.Dense(\n            config.num_key_value_heads * self.head_dim,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=self.config.attention_bias,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n            precision=self.precision,\n            dot_general=dot_general_cls\n        )\n        self.o_proj = nn.Dense(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n            precision=self.precision,\n            dot_general=dot_general_cls\n        )\n\n        self.rotary = FlaxLlamaEmbedding(self.dtype)\n\n        self.resid_dropout = nn.Dropout(rate=config.resid_pdrop)\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.hidden_size,))\n\n    @nn.compact\n    def _concatenate_to_cache(self, key, value, query, attention_mask):\n        \"\"\"\n        The _concatenate_to_cache function is used to concatenate the key and value vectors\n        of a query with those of previous queries. This allows for the attention mechanism to\n        look at all previous queries when computing its output. The function takes in three\n        arguments: key, value, and query. It also uses two variables that are stored in the cache:\n        cached_key and cached_value.\n\n        :param self: Access the variables stored in the cache\n        :param key: Store the keys of the encoder-decoder attention\n        :param value: Initialize the cached_value variable\n        :param query: Determine the number of cache vectors to update\n        :param attention_mask: Mask out the padded vectors in the cache\n        :return: The key, value and attention_mask\n\n        \"\"\"\n        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jnp.array(0, dtype=jnp.int32))\n\n        if is_initialized:\n            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n            cur_index = cache_index.value\n            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n            key = lax.dynamic_update_slice(cached_key.value, key, indices)\n            value = lax.dynamic_update_slice(cached_value.value, value, indices)\n            cached_key.value = key\n            cached_value.value = value\n            num_updated_cache_vectors = query.shape[1]\n            cache_index.value = cache_index.value + num_updated_cache_vectors\n\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(max_length) &lt; cur_index + num_updated_cache_vectors,\n                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n            )\n            attention_mask = combine_masks(pad_mask, attention_mask)\n        return key, value, attention_mask\n\n    @staticmethod\n    def _t(query, key, value):\n        \"\"\"\n        The _t function transposes the query, key and value matrices.\n\n        :param query: Get the attention weights for each of the heads\n        :param key: Determine the number of heads\n        :param value: Store the values of the input\n        :return: The transpose of the query, key and value matrices\n\n        \"\"\"\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        \"\"\"\n        The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n        The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n        the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n        :param self: Access variables that belong to the class\n        :param batch_size: Reshape the query, key and value tensors\n        :param sequence_length: Reshape the query, key and value tensors\n        :param query: Calculate the attention weights\n        :param key: Calculate the attention\n        :param value: Compute the attention weights\n        :param freq_cis: Calculate the frequency of each word in the vocabulary\n        :param position_ids: Identify the position of each token in the sequence\n        :return: A tuple of 3 tensors: query, key and value\n\n        \"\"\"\n        query = query.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key = key.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n        value = value.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n        query, key, value = self._t(query, key, value)\n        query, key = self.rotary(position_ids=position_ids, query=query, key=key, freq_cis=freq_cis)\n        key = repeat_kv_bnsh(key, self.number_of_reps)\n        value = repeat_kv_bnsh(value, self.number_of_reps)\n        return self._t(query, key, value)\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            fcm_mask=None,\n    ):\n\n        \"\"\"\n\n        The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n        with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n        and it should return all outputs that are needed for training or inference.\n\n        :param self: Access variables that belong to the class\n        :param hidden_states: chex.Array: Pass the hidden states of the previous layer\n        :param freq_cis: chex.Array: Pass in the frequency coefficients for each position\n        :param attention_mask: chex.Array: Mask out certain tokens in the input sequence\n        :param position_ids: chex.Array: Determine the position of each token in a sequence\n        :param causal_mask: chex.Array: Mask out the future tokens in the decoder\n        :param deterministic: bool: Determine whether to use dropout or not\n        :param init_cache: bool: Initialize the cache\n        :param output_attentions: bool: Determine whether to return the attention weights or not\n        :param fcm_mask: Mask out the attention weights between the input and output tokens\n        :param : Determine if the attention is causal or not\n        :return: A tuple of two arrays\n\n        \"\"\"\n        batch_size, sequence_length = hidden_states.shape[:2]\n        query_state, key_state, value_state = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(\n            hidden_states)\n\n        if self.config.use_pjit_attention_force:\n            query_state = with_sharding_constraint(query_state, PS((\"dp\", \"fsdp\"), \"mp\", \"tp\"))\n            key_state = with_sharding_constraint(key_state, PS((\"dp\", \"fsdp\"), \"mp\", \"tp\"))\n            value_state = with_sharding_constraint(value_state, PS((\"dp\", \"fsdp\"), \"mp\", \"tp\"))\n\n        query_state = query_state.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key_state = key_state.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n        value_state = value_state.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n        query_state, key_state, value_state = self.apply_rotary(\n            query=query_state,\n            key=key_state,\n            value=value_state,\n            position_ids=position_ids,\n            freq_cis=freq_cis,\n            batch_size=batch_size,\n            sequence_length=sequence_length\n        )\n\n        assert_msg = (\n            \"num_attention_heads repeat wont work likely\\n\"\n            f\"INFO :\\n\\trepeat_kv_bnsh Used with number_of_reps = {self.number_of_reps}\\n\\t\"\n            f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n        )\n\n        assert query_state.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert key_state.shape[-2] == self.config.num_attention_heads, assert_msg\n        assert value_state.shape[-2] == self.config.num_attention_heads, assert_msg\n\n        query_length, key_length = query_state.shape[1], key_state.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1, query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n\n        dropout_rng = None\n        if not deterministic and self.config.attn_pdrop &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_state, value_state, attention_mask = self._concatenate_to_cache(key_state, value_state, query_state,\n                                                                                attention_mask)\n        if self.config.use_flash_attention and not (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n\n            if attention_mask.ndim == 2:\n                attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n            if attention_mask.shape[1] != self.config.num_attention_heads:\n                attention_mask = attention_mask.repeat(self.config.num_attention_heads, 1, )\n            attention_bias = lax.select(\n                attention_mask &gt; 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n            )\n            attn_weights = None\n            rtp_axis = (0, 2, 1, 3)\n            attn_output = smart_flash_attention(\n                q=jnp.transpose(query_state, rtp_axis),\n                k=jnp.transpose(key_state, rtp_axis),\n                v=jnp.transpose(value_state, rtp_axis),\n                q_ps=self.config.q_ps,\n                k_ps=self.config.k_ps,\n                v_ps=self.config.v_ps,\n                b_ps=self.config.b_ps,\n                a_ps=self.config.a_ps,\n                bias=attention_bias,\n                block_q=self.config.flash_attn_query_chunk_size,\n                block_k=self.config.flash_attn_key_chunk_size,\n                block_b=1,\n                num_attention_heads=self.config.num_attention_heads,\n                precision=self.precision,\n                dtype=self.dtype,\n                causal=False,\n                mesh=self.config.jax_mesh(),\n                dropout_rng=dropout_rng,\n                deterministic=deterministic,\n                q_seq_len=sequence_length,\n                kv_seq_len=key_length,\n                attn_pdrop=self.config.attn_pdrop,\n                head_dims=self.head_dim,\n                force_float32_tpu=True\n            )\n            attn_output = jnp.transpose(attn_output, rtp_axis)\n        else:\n            query_length, key_length = query_state.shape[1], key_state.shape[1]\n\n            if self.has_variable(\"cache\", \"cached_key\"):\n                mask_shift = self.variables[\"cache\"][\"cache_index\"]\n                max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n                causal_mask = lax.dynamic_slice(\n                    causal_mask, (0, 0, mask_shift, 0), (1, 1, query_length, max_decoder_length)\n                )\n            else:\n                causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n            batch_size = hidden_states.shape[0]\n            causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n            if attention_mask.ndim == 2:\n                attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n            attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n\n            if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n                key_state, value_state, attention_mask = self._concatenate_to_cache(key_state, value_state, query_state,\n                                                                                    attention_mask)\n\n            attn_weights = None\n            if self.config.use_shard_map:\n                ring_attention_sharded = shard_map(\n                    partial(fjformer.attention.ring_attention_standard, axis_name=\"mp\"),\n                    mesh=self.config.jax_mesh(),\n                    in_specs=(\n                        self.config.q_ps,\n                        self.config.k_ps,\n                        self.config.v_ps,\n                        self.config.b_ps\n                    ),\n                    out_specs=self.config.a_ps,\n                    check_rep=False\n                )\n\n                attn_output = ring_attention_sharded(\n                    query_state, key_state, value_state, attention_mask\n                )\n            else:\n                attn_output = partial(fjformer.attention.ring_attention_standard, axis_name=\"mp\")(\n                    query_state, key_state, value_state, attention_mask\n                )\n        attn_output = self._merge_heads(attn_output)\n        attn_output = self.o_proj(attn_output)\n\n        attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n        outputs = (attn_output, attn_weights) if output_attentions else (attn_output,)\n\n        return outputs\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaAttention.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, deterministic=True, init_cache=False, output_attentions=False, fcm_mask=None)</code>","text":"<p>The call function is the main function of a JAX module. It defines how the module behaves when called with inputs. The call function can be thought of as a \"forward pass\" through the model, and it should return all outputs that are needed for training or inference.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the hidden states of the previous layer</p> required <code>freq_cis</code> <code>Array</code> <p>chex.Array: Pass in the frequency coefficients for each position</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input sequence</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Determine the position of each token in a sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask out the future tokens in the decoder</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>False</code> <code>fcm_mask</code> <p>Mask out the attention weights between the input and output tokens</p> <code>None</code> <code></code> <p>Determine if the attention is causal or not</p> required <p>Returns:</p> Type Description <p>A tuple of two arrays</p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: chex.Array,\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        fcm_mask=None,\n):\n\n    \"\"\"\n\n    The __call__ function is the main function of a JAX module. It defines how the module behaves when called\n    with inputs. The __call__ function can be thought of as a &amp;quot;forward pass&amp;quot; through the model,\n    and it should return all outputs that are needed for training or inference.\n\n    :param self: Access variables that belong to the class\n    :param hidden_states: chex.Array: Pass the hidden states of the previous layer\n    :param freq_cis: chex.Array: Pass in the frequency coefficients for each position\n    :param attention_mask: chex.Array: Mask out certain tokens in the input sequence\n    :param position_ids: chex.Array: Determine the position of each token in a sequence\n    :param causal_mask: chex.Array: Mask out the future tokens in the decoder\n    :param deterministic: bool: Determine whether to use dropout or not\n    :param init_cache: bool: Initialize the cache\n    :param output_attentions: bool: Determine whether to return the attention weights or not\n    :param fcm_mask: Mask out the attention weights between the input and output tokens\n    :param : Determine if the attention is causal or not\n    :return: A tuple of two arrays\n\n    \"\"\"\n    batch_size, sequence_length = hidden_states.shape[:2]\n    query_state, key_state, value_state = self.q_proj(hidden_states), self.k_proj(hidden_states), self.v_proj(\n        hidden_states)\n\n    if self.config.use_pjit_attention_force:\n        query_state = with_sharding_constraint(query_state, PS((\"dp\", \"fsdp\"), \"mp\", \"tp\"))\n        key_state = with_sharding_constraint(key_state, PS((\"dp\", \"fsdp\"), \"mp\", \"tp\"))\n        value_state = with_sharding_constraint(value_state, PS((\"dp\", \"fsdp\"), \"mp\", \"tp\"))\n\n    query_state = query_state.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n    key_state = key_state.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n    value_state = value_state.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n    query_state, key_state, value_state = self.apply_rotary(\n        query=query_state,\n        key=key_state,\n        value=value_state,\n        position_ids=position_ids,\n        freq_cis=freq_cis,\n        batch_size=batch_size,\n        sequence_length=sequence_length\n    )\n\n    assert_msg = (\n        \"num_attention_heads repeat wont work likely\\n\"\n        f\"INFO :\\n\\trepeat_kv_bnsh Used with number_of_reps = {self.number_of_reps}\\n\\t\"\n        f\"NH : {self.config.num_attention_heads} KVH : {self.config.num_attention_heads}\"\n    )\n\n    assert query_state.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert key_state.shape[-2] == self.config.num_attention_heads, assert_msg\n    assert value_state.shape[-2] == self.config.num_attention_heads, assert_msg\n\n    query_length, key_length = query_state.shape[1], key_state.shape[1]\n\n    if self.has_variable(\"cache\", \"cached_key\"):\n        mask_shift = self.variables[\"cache\"][\"cache_index\"]\n        max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n        causal_mask = lax.dynamic_slice(\n            causal_mask, (0, 0, mask_shift, 0), (1, 1, query_length, max_decoder_length)\n        )\n    else:\n        causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n    batch_size = hidden_states.shape[0]\n    causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n    attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n    attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n\n    dropout_rng = None\n    if not deterministic and self.config.attn_pdrop &gt; 0.0:\n        dropout_rng = self.make_rng(\"dropout\")\n\n    if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n        key_state, value_state, attention_mask = self._concatenate_to_cache(key_state, value_state, query_state,\n                                                                            attention_mask)\n    if self.config.use_flash_attention and not (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        if attention_mask.shape[1] != self.config.num_attention_heads:\n            attention_mask = attention_mask.repeat(self.config.num_attention_heads, 1, )\n        attention_bias = lax.select(\n            attention_mask &gt; 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n        )\n        attn_weights = None\n        rtp_axis = (0, 2, 1, 3)\n        attn_output = smart_flash_attention(\n            q=jnp.transpose(query_state, rtp_axis),\n            k=jnp.transpose(key_state, rtp_axis),\n            v=jnp.transpose(value_state, rtp_axis),\n            q_ps=self.config.q_ps,\n            k_ps=self.config.k_ps,\n            v_ps=self.config.v_ps,\n            b_ps=self.config.b_ps,\n            a_ps=self.config.a_ps,\n            bias=attention_bias,\n            block_q=self.config.flash_attn_query_chunk_size,\n            block_k=self.config.flash_attn_key_chunk_size,\n            block_b=1,\n            num_attention_heads=self.config.num_attention_heads,\n            precision=self.precision,\n            dtype=self.dtype,\n            causal=False,\n            mesh=self.config.jax_mesh(),\n            dropout_rng=dropout_rng,\n            deterministic=deterministic,\n            q_seq_len=sequence_length,\n            kv_seq_len=key_length,\n            attn_pdrop=self.config.attn_pdrop,\n            head_dims=self.head_dim,\n            force_float32_tpu=True\n        )\n        attn_output = jnp.transpose(attn_output, rtp_axis)\n    else:\n        query_length, key_length = query_state.shape[1], key_state.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1, query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_states.shape[0]\n        causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask, fcm_mask)\n\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key_state, value_state, attention_mask = self._concatenate_to_cache(key_state, value_state, query_state,\n                                                                                attention_mask)\n\n        attn_weights = None\n        if self.config.use_shard_map:\n            ring_attention_sharded = shard_map(\n                partial(fjformer.attention.ring_attention_standard, axis_name=\"mp\"),\n                mesh=self.config.jax_mesh(),\n                in_specs=(\n                    self.config.q_ps,\n                    self.config.k_ps,\n                    self.config.v_ps,\n                    self.config.b_ps\n                ),\n                out_specs=self.config.a_ps,\n                check_rep=False\n            )\n\n            attn_output = ring_attention_sharded(\n                query_state, key_state, value_state, attention_mask\n            )\n        else:\n            attn_output = partial(fjformer.attention.ring_attention_standard, axis_name=\"mp\")(\n                query_state, key_state, value_state, attention_mask\n            )\n    attn_output = self._merge_heads(attn_output)\n    attn_output = self.o_proj(attn_output)\n\n    attn_output = self.resid_dropout(attn_output, deterministic=deterministic)\n    outputs = (attn_output, attn_weights) if output_attentions else (attn_output,)\n\n    return outputs\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaAttention.apply_rotary","title":"<code>apply_rotary(batch_size, sequence_length, query, key, value, freq_cis, position_ids)</code>","text":"<p>The apply_rotary function is a modified version of the apply_attention function in the BertModel class. The main difference is that it takes in an additional argument, freq_cis, which are used to calculate the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>batch_size</code> <p>Reshape the query, key and value tensors</p> required <code>sequence_length</code> <p>Reshape the query, key and value tensors</p> required <code>query</code> <p>Calculate the attention weights</p> required <code>key</code> <p>Calculate the attention</p> required <code>value</code> <p>Compute the attention weights</p> required <code>freq_cis</code> <p>Calculate the frequency of each word in the vocabulary</p> required <code>position_ids</code> <p>Identify the position of each token in the sequence</p> required <p>Returns:</p> Type Description <p>A tuple of 3 tensors: query, key and value</p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>def apply_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n    \"\"\"\n    The apply_rotary function is a modified version of the apply_attention function in the BertModel class.\n    The main difference is that it takes in an additional argument, freq_cis, which are used to calculate\n    the rotary attention weights. The other differences are minor and mostly related to reshaping tensors.\n\n    :param self: Access variables that belong to the class\n    :param batch_size: Reshape the query, key and value tensors\n    :param sequence_length: Reshape the query, key and value tensors\n    :param query: Calculate the attention weights\n    :param key: Calculate the attention\n    :param value: Compute the attention weights\n    :param freq_cis: Calculate the frequency of each word in the vocabulary\n    :param position_ids: Identify the position of each token in the sequence\n    :return: A tuple of 3 tensors: query, key and value\n\n    \"\"\"\n    query = query.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n    key = key.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n    value = value.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n    query, key, value = self._t(query, key, value)\n    query, key = self.rotary(position_ids=position_ids, query=query, key=key, freq_cis=freq_cis)\n    key = repeat_kv_bnsh(key, self.number_of_reps)\n    value = repeat_kv_bnsh(value, self.number_of_reps)\n    return self._t(query, key, value)\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaBlock","title":"<code>FlaxLlamaBlock</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>class FlaxLlamaBlock(nn.Module):\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -&gt; None:\n        attn_block = FlaxLlamaAttention\n        if self.config.gradient_checkpointing != '':\n            attn_block = nn_partitioning.remat(\n                FlaxLlamaAttention, static_argnums=(5, 6, 7),\n                policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing)\n            )\n\n        self.self_attn = attn_block(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        mlp_block = FlaxLlamaMLP\n\n        if self.config.gradient_checkpointing != '':\n            mlp_block = nn_partitioning.remat(\n                FlaxLlamaMLP, static_argnums=(1,),\n                policy=get_gradient_checkpoint_policy(self.config.gradient_checkpointing)\n            )\n\n        self.mlp = mlp_block(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n        self.input_layernorm = RMSNorm(\n            self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n        self.post_attention_layernorm = RMSNorm(\n            self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n\n        )\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            fcm_mask: Optional[jnp.ndarray] = None,\n    ):\n\n        \"\"\"\n        The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in hidden states, frequency-domain inputs, and masks as input. It then\n        applies self-attention to the hidden states using those inputs and returns an\n        output tensor with shape (batch_size, sequence_length, model_dim).\n\n        :param self: Refer to the class instance itself\n        :param hidden_states: chex.Array: Pass in the hidden state of the previous layer\n        :param freq_cis: chex.Array: Pass in the frequency information\n        :param attention_mask: chex.Array: Mask out the attention weights for padding tokens\n        :param position_ids: chex.Array: Determine the position of each token in the sequence\n        :param causal_mask: chex.Array: Mask the attention weights\n        :param deterministic: bool: Control whether the dropout is applied or not\n        :param init_cache: bool: Initialize the cache in the attention layer\n        :param output_attentions: bool: Return the attention weights\n        :param fcm_mask: Optional[jnp.ndarray]: Mask the self-attention\n        :param : Control the dropout in the self attention layer\n        :return: A tuple of two items\n\n        \"\"\"\n        attn_outputs = self.self_attn(\n            self.input_layernorm(hidden_states),\n            freq_cis,\n            attention_mask,\n            position_ids,\n            causal_mask,\n            deterministic,\n            init_cache,\n            output_attentions,\n            fcm_mask,\n        )\n        attn_output = attn_outputs[0]\n        hidden_states = hidden_states + attn_output\n\n        feed_forward_input = self.post_attention_layernorm(hidden_states)\n\n        if self.config.use_sacn_mlp:\n            feed_forward_input = einops.rearrange(\n                feed_forward_input,\n                '... (b s) d -&gt; ... b s d',\n                b=self.config.scan_mlp_chunk_size\n            )\n\n            def mlp_forward(mlp, carry, x):\n                return None, mlp(x, deterministic)\n\n            scan_axis = feed_forward_input.ndim - 3\n\n            _, feed_forward_hidden_states = nn.scan(\n                mlp_forward,\n                variable_broadcast=\"params\",\n                split_rngs={\"params\": False, \"dropout\": True},\n                in_axes=scan_axis,\n                out_axes=scan_axis,\n            )(self.mlp, None, feed_forward_input)\n            feed_forward_hidden_states = einops.rearrange(\n                feed_forward_hidden_states,\n                '... b s d -&gt; ... (b s) d'\n            )\n        else:\n            feed_forward_hidden_states = self.mlp(\n                feed_forward_input,\n                deterministic,\n            )\n\n        hidden_states = hidden_states + feed_forward_hidden_states\n\n        return (hidden_states,) + attn_outputs[1:]\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaBlock.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, deterministic=True, init_cache=False, output_attentions=False, fcm_mask=None)</code>","text":"<p>The call function is the main function of a TransformerEncoderLayer. It takes in hidden states, frequency-domain inputs, and masks as input. It then applies self-attention to the hidden states using those inputs and returns an output tensor with shape (batch_size, sequence_length, model_dim).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the class instance itself</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass in the hidden state of the previous layer</p> required <code>freq_cis</code> <code>Array</code> <p>chex.Array: Pass in the frequency information</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the attention weights for padding tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Determine the position of each token in the sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the attention weights</p> required <code>deterministic</code> <code>bool</code> <p>bool: Control whether the dropout is applied or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache in the attention layer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>fcm_mask</code> <code>Optional[ndarray]</code> <p>Optional[jnp.ndarray]: Mask the self-attention</p> <code>None</code> <code></code> <p>Control the dropout in the self attention layer</p> required <p>Returns:</p> Type Description <p>A tuple of two items</p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: chex.Array,\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        fcm_mask: Optional[jnp.ndarray] = None,\n):\n\n    \"\"\"\n    The __call__ function is the main function of a TransformerEncoderLayer.\n    It takes in hidden states, frequency-domain inputs, and masks as input. It then\n    applies self-attention to the hidden states using those inputs and returns an\n    output tensor with shape (batch_size, sequence_length, model_dim).\n\n    :param self: Refer to the class instance itself\n    :param hidden_states: chex.Array: Pass in the hidden state of the previous layer\n    :param freq_cis: chex.Array: Pass in the frequency information\n    :param attention_mask: chex.Array: Mask out the attention weights for padding tokens\n    :param position_ids: chex.Array: Determine the position of each token in the sequence\n    :param causal_mask: chex.Array: Mask the attention weights\n    :param deterministic: bool: Control whether the dropout is applied or not\n    :param init_cache: bool: Initialize the cache in the attention layer\n    :param output_attentions: bool: Return the attention weights\n    :param fcm_mask: Optional[jnp.ndarray]: Mask the self-attention\n    :param : Control the dropout in the self attention layer\n    :return: A tuple of two items\n\n    \"\"\"\n    attn_outputs = self.self_attn(\n        self.input_layernorm(hidden_states),\n        freq_cis,\n        attention_mask,\n        position_ids,\n        causal_mask,\n        deterministic,\n        init_cache,\n        output_attentions,\n        fcm_mask,\n    )\n    attn_output = attn_outputs[0]\n    hidden_states = hidden_states + attn_output\n\n    feed_forward_input = self.post_attention_layernorm(hidden_states)\n\n    if self.config.use_sacn_mlp:\n        feed_forward_input = einops.rearrange(\n            feed_forward_input,\n            '... (b s) d -&gt; ... b s d',\n            b=self.config.scan_mlp_chunk_size\n        )\n\n        def mlp_forward(mlp, carry, x):\n            return None, mlp(x, deterministic)\n\n        scan_axis = feed_forward_input.ndim - 3\n\n        _, feed_forward_hidden_states = nn.scan(\n            mlp_forward,\n            variable_broadcast=\"params\",\n            split_rngs={\"params\": False, \"dropout\": True},\n            in_axes=scan_axis,\n            out_axes=scan_axis,\n        )(self.mlp, None, feed_forward_input)\n        feed_forward_hidden_states = einops.rearrange(\n            feed_forward_hidden_states,\n            '... b s d -&gt; ... (b s) d'\n        )\n    else:\n        feed_forward_hidden_states = self.mlp(\n            feed_forward_input,\n            deterministic,\n        )\n\n    hidden_states = hidden_states + feed_forward_hidden_states\n\n    return (hidden_states,) + attn_outputs[1:]\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaBlockCollection","title":"<code>FlaxLlamaBlockCollection</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>class FlaxLlamaBlockCollection(nn.Module):\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.blocks = [\n            FlaxLlamaBlock(\n                self.config,\n                name=str(i),\n                dtype=self.dtype,\n                param_dtype=self.param_dtype,\n                precision=self.precision\n            )\n            for i in range(self.config.num_hidden_layers)\n        ]\n\n    def __call__(\n            self,\n            hidden_states: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            causal_mask: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        \"\"\"    \n        The __call__ function is the main function of a JAX nn.Module.\n        It defines how the module behaves when called as a function, and it's what you'll use to call your model in training loops or inference scripts.\n        The __call__ method should take all inputs that are necessary for computing outputs from the module, and return all outputs that are computed by this module.\n\n        :param self: Represent the instance of the class\n        :param hidden_states: chex.Array: Pass the input tensor to the encoder\n        :param freq_cis: chex.Array: Pass in the frequency of each token\n        :param attention_mask: chex.Array: Mask out certain tokens in the input sequence\n        :param position_ids: chex.Array: Specify the position of each token in a sequence\n        :param causal_mask: chex.Array: Mask the attention weights\n        :param deterministic: bool: Determine whether the model is in training or evaluation mode\n        :param init_cache: bool: Initialize the cache for each layer\n        :param output_attentions: bool: Determine whether to output the attention weights\n        :param output_hidden_states: bool: Determine whether to return the hidden states of each layer\n        :param return_dict: bool: Return a dictionary of the outputs\n        :param : Determine whether to use the forgetful causal mask\n        :return: A tuple of 3 values\n\n        \"\"\"\n        all_attentions = () if output_attentions else None\n        all_hidden_states = () if output_hidden_states else None\n\n        if not deterministic and self.config.fcm_max_ratio &gt; 0:\n            # Apply forgetful causal mask\n            batch_size, seq_length = hidden_states.shape[0], hidden_states.shape[1]\n            fcm_ratio = jax.random.uniform(\n                self.make_rng('fcm'), shape=(batch_size, 1, 1, 1),\n                minval=self.config.fcm_min_ratio,\n                maxval=self.config.fcm_max_ratio\n            )\n            fcm_mask = jax.random.uniform(\n                self.make_rng('fcm'),\n                shape=(batch_size, 1, seq_length, seq_length)\n            ) &gt; fcm_ratio\n            fcm_mask = fcm_mask.at[:, :, :, 0].set(True)\n            fcm_mask = fcm_mask.astype('bool')\n        else:\n            fcm_mask = None\n\n        for block in self.blocks:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            layer_outputs = block(\n                hidden_states=hidden_states,\n                freq_cis=freq_cis,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                causal_mask=causal_mask,\n                deterministic=deterministic,\n                init_cache=init_cache,\n                output_attentions=output_attentions,\n                fcm_mask=fcm_mask,\n            )\n            hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_attentions += (layer_outputs[1],)\n\n        outputs = (hidden_states, all_hidden_states, all_attentions)\n\n        return outputs\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaBlockCollection.__call__","title":"<code>__call__(hidden_states, freq_cis, attention_mask, position_ids, causal_mask, deterministic=True, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True)</code>","text":"<p>The call function is the main function of a JAX nn.Module. It defines how the module behaves when called as a function, and it's what you'll use to call your model in training loops or inference scripts. The call method should take all inputs that are necessary for computing outputs from the module, and return all outputs that are computed by this module.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the input tensor to the encoder</p> required <code>freq_cis</code> <code>Array</code> <p>chex.Array: Pass in the frequency of each token</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input sequence</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in a sequence</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the attention weights</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether the model is in training or evaluation mode</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for each layer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to output the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Determine whether to return the hidden states of each layer</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs</p> <code>True</code> <code></code> <p>Determine whether to use the forgetful causal mask</p> required <p>Returns:</p> Type Description <p>A tuple of 3 values</p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_states: chex.Array,\n        freq_cis: chex.Array,\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        causal_mask: chex.Array,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n):\n    \"\"\"    \n    The __call__ function is the main function of a JAX nn.Module.\n    It defines how the module behaves when called as a function, and it's what you'll use to call your model in training loops or inference scripts.\n    The __call__ method should take all inputs that are necessary for computing outputs from the module, and return all outputs that are computed by this module.\n\n    :param self: Represent the instance of the class\n    :param hidden_states: chex.Array: Pass the input tensor to the encoder\n    :param freq_cis: chex.Array: Pass in the frequency of each token\n    :param attention_mask: chex.Array: Mask out certain tokens in the input sequence\n    :param position_ids: chex.Array: Specify the position of each token in a sequence\n    :param causal_mask: chex.Array: Mask the attention weights\n    :param deterministic: bool: Determine whether the model is in training or evaluation mode\n    :param init_cache: bool: Initialize the cache for each layer\n    :param output_attentions: bool: Determine whether to output the attention weights\n    :param output_hidden_states: bool: Determine whether to return the hidden states of each layer\n    :param return_dict: bool: Return a dictionary of the outputs\n    :param : Determine whether to use the forgetful causal mask\n    :return: A tuple of 3 values\n\n    \"\"\"\n    all_attentions = () if output_attentions else None\n    all_hidden_states = () if output_hidden_states else None\n\n    if not deterministic and self.config.fcm_max_ratio &gt; 0:\n        # Apply forgetful causal mask\n        batch_size, seq_length = hidden_states.shape[0], hidden_states.shape[1]\n        fcm_ratio = jax.random.uniform(\n            self.make_rng('fcm'), shape=(batch_size, 1, 1, 1),\n            minval=self.config.fcm_min_ratio,\n            maxval=self.config.fcm_max_ratio\n        )\n        fcm_mask = jax.random.uniform(\n            self.make_rng('fcm'),\n            shape=(batch_size, 1, seq_length, seq_length)\n        ) &gt; fcm_ratio\n        fcm_mask = fcm_mask.at[:, :, :, 0].set(True)\n        fcm_mask = fcm_mask.astype('bool')\n    else:\n        fcm_mask = None\n\n    for block in self.blocks:\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        layer_outputs = block(\n            hidden_states=hidden_states,\n            freq_cis=freq_cis,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            causal_mask=causal_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            fcm_mask=fcm_mask,\n        )\n        hidden_states = layer_outputs[0]\n\n        if output_attentions:\n            all_attentions += (layer_outputs[1],)\n\n    outputs = (hidden_states, all_hidden_states, all_attentions)\n\n    return outputs\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaForCausalLM","title":"<code>FlaxLlamaForCausalLM</code>","text":"<p>             Bases: <code>FlaxLlamaPreTrainedModel</code></p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>class FlaxLlamaForCausalLM(FlaxLlamaPreTrainedModel):\n    module_class = FlaxLlamaForCausalLMModule\n\n    def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n        \"\"\"\n        The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n        :param self: Access variables that belong to the class\n        :param input_ids: Pass in the input tokens\n        :param max_length: Set the length of the sequence to be generated\n        :param attention_mask: Optional[chex.Array]: Mask the attention weights\n        :return: A dictionary of the past_key_values, attention_mask and position ids\n\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        past_key_values = self.init_cache(batch_size, max_length)\n        extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n        if attention_mask is not None:\n            position_ids = attention_mask.cumsum(axis=-1) - 1\n            extended_attention_mask = lax.dynamic_update_slice(extended_attention_mask, attention_mask, (0, 0))\n        else:\n            position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[None, :], (batch_size, seq_length))\n\n        return {\n            \"past_key_values\": past_key_values,\n            \"attention_mask\": extended_attention_mask,\n            \"position_ids\": position_ids,\n        }\n\n    def update_inputs_for_generation(self, model_outputs, model_kwargs):\n        model_kwargs[\"past_key_values\"] = model_outputs.past_key_values\n        model_kwargs[\"position_ids\"] = model_kwargs[\"position_ids\"][:, -1:] + 1\n        return model_kwargs\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaForCausalLM.prepare_inputs_for_generation","title":"<code>prepare_inputs_for_generation(input_ids, max_length, attention_mask=None)</code>","text":"<p>The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>input_ids</code> <p>Pass in the input tokens</p> required <code>max_length</code> <p>Set the length of the sequence to be generated</p> required <code>attention_mask</code> <code>Optional[Array]</code> <p>Optional[chex.Array]: Mask the attention weights</p> <code>None</code> <p>Returns:</p> Type Description <p>A dictionary of the past_key_values, attention_mask and position ids</p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>def prepare_inputs_for_generation(self, input_ids, max_length, attention_mask: Optional[chex.Array] = None):\n    \"\"\"\n    The prepare_inputs_for_generation function is used to prepare the inputs for a generation task.\n\n    :param self: Access variables that belong to the class\n    :param input_ids: Pass in the input tokens\n    :param max_length: Set the length of the sequence to be generated\n    :param attention_mask: Optional[chex.Array]: Mask the attention weights\n    :return: A dictionary of the past_key_values, attention_mask and position ids\n\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n\n    past_key_values = self.init_cache(batch_size, max_length)\n    extended_attention_mask = jnp.ones((batch_size, max_length), dtype=\"i4\")\n    if attention_mask is not None:\n        position_ids = attention_mask.cumsum(axis=-1) - 1\n        extended_attention_mask = lax.dynamic_update_slice(extended_attention_mask, attention_mask, (0, 0))\n    else:\n        position_ids = jnp.broadcast_to(jnp.arange(seq_length, dtype=\"i4\")[None, :], (batch_size, seq_length))\n\n    return {\n        \"past_key_values\": past_key_values,\n        \"attention_mask\": extended_attention_mask,\n        \"position_ids\": position_ids,\n    }\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaForCausalLMModule","title":"<code>FlaxLlamaForCausalLMModule</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>class FlaxLlamaForCausalLMModule(nn.Module):\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.model = FlaxLlamaModule(self.config,\n                                     dtype=self.dtype,\n                                     param_dtype=self.param_dtype,\n                                     precision=self.precision,\n                                     )\n\n        if self.config.bits is not None:\n            _dot_general_cls = q_config.fully_quantized(\n                fwd_bits=self.config.bits,\n                bwd_bits=self.config.bits\n            )\n        else:\n            _dot_general_cls = None\n\n        dot_general_cls = q_flax.QDotGeneral(_dot_general_cls)\n\n        self.lm_head = nn.Dense(\n            self.config.vocab_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            precision=self.precision,\n            dot_general=dot_general_cls\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"\n        The __call__ function is the main function of a Flax module. It takes in inputs and returns outputs.\n\n        :param self: Refer to the object itself\n        :param input_ids: chex.Array: Pass the input token ids to the model\n        :param attention_mask: chex.Array: Mask out the padding tokens\n        :param position_ids: chex.Array: Specify the position of each token in the input sequence\n        :param deterministic: bool: Control whether the model is trained or not\n        :param init_cache: bool: Initialize the cache for the decoder\n        :param output_attentions: bool: Return the attention weights\n        :param output_hidden_states: bool: Determine whether or not to return the hidden states\n        :param return_dict: bool: Return a dictionary of the outputs or not\n        :param extra_embedding: Optional[Union[jnp.ndarray: Pass in the embedding of the word that we want to predict\n        :param None]]: Pass in the extra embedding\n        :return: The logits and the hidden states\n\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.model(\n            input_ids,\n            attention_mask,\n            position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            extra_embedding=extra_embedding\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.tie_word_embeddings:\n            shared_kernel = self.model.variables[\"params\"][\"embed_tokens\"][\"embedding\"].T\n            lm_logits = self.lm_head.apply({\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        lm_logits = lm_logits.astype(jnp.float32)\n\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaForCausalLMModule.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, deterministic=True, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True, extra_embedding=None)</code>","text":"<p>The call function is the main function of a Flax module. It takes in inputs and returns outputs.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass the input token ids to the model</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the padding tokens</p> <code>None</code> <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in the input sequence</p> <code>None</code> <code>deterministic</code> <code>bool</code> <p>bool: Control whether the model is trained or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the decoder</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Determine whether or not to return the hidden states</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs or not</p> <code>True</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray: Pass in the embedding of the word that we want to predict</p> <code>None</code> <code>None]]</code> <p>Pass in the extra embedding</p> required <p>Returns:</p> Type Description <p>The logits and the hidden states</p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array = None,\n        position_ids: chex.Array = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n):\n    \"\"\"\n    The __call__ function is the main function of a Flax module. It takes in inputs and returns outputs.\n\n    :param self: Refer to the object itself\n    :param input_ids: chex.Array: Pass the input token ids to the model\n    :param attention_mask: chex.Array: Mask out the padding tokens\n    :param position_ids: chex.Array: Specify the position of each token in the input sequence\n    :param deterministic: bool: Control whether the model is trained or not\n    :param init_cache: bool: Initialize the cache for the decoder\n    :param output_attentions: bool: Return the attention weights\n    :param output_hidden_states: bool: Determine whether or not to return the hidden states\n    :param return_dict: bool: Return a dictionary of the outputs or not\n    :param extra_embedding: Optional[Union[jnp.ndarray: Pass in the embedding of the word that we want to predict\n    :param None]]: Pass in the extra embedding\n    :return: The logits and the hidden states\n\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n    if attention_mask is None:\n        attention_mask = jnp.ones_like(input_ids)\n    if position_ids is None:\n        position_ids = jnp.broadcast_to(\n            jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n            (batch_size, seq_length)\n        )\n    outputs = self.model(\n        input_ids,\n        attention_mask,\n        position_ids,\n        deterministic=deterministic,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n        extra_embedding=extra_embedding\n    )\n\n    hidden_states = outputs[0]\n\n    if self.config.tie_word_embeddings:\n        shared_kernel = self.model.variables[\"params\"][\"embed_tokens\"][\"embedding\"].T\n        lm_logits = self.lm_head.apply({\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n    else:\n        lm_logits = self.lm_head(hidden_states)\n\n    lm_logits = lm_logits.astype(jnp.float32)\n\n    if not return_dict:\n        return (lm_logits,) + outputs[1:]\n\n    return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaForSequenceClassificationModule","title":"<code>FlaxLlamaForSequenceClassificationModule</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>class FlaxLlamaForSequenceClassificationModule(nn.Module):\n    num_classes: int\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        \"\"\"\n        The setup function is called once at the beginning of training.\n        It initializes the model and optimizer, and sets up any other state that needs to be initialized.\n\n        :param self: Access variables that belong to the class\n        :return: A tuple of the model and the classifier\n        \"\"\"\n        self.model = FlaxLlamaModule(self.config, dtype=self.dtype)\n        self.classifier = nn.Dense(\n            self.num_classes,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            precision=self.precision,\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"\n        The __call__ function is the main function of a Flax module.\n        It takes in all the inputs to the model and returns all outputs from it.\n        The __call__ function can be called directly on an instance of a class, or by using parentheses after an instance:\n            &amp;gt;&amp;gt;&amp;gt; my_model = MyModel()  # instantiate your model class\n            &amp;gt;&amp;gt;&amp;gt; output = my_model(input)  # call your model with input data as arguments to __call__\n\n        :param self: Refer to the class instance\n        :param input_ids: chex.Array: Pass the input to the model\n        :param attention_mask: chex.Array: Specify which tokens are masked\n        :param position_ids: chex.Array: Specify the position of each token in the sequence\n        :param deterministic: bool: Control whether the model is run in deterministic or stochastic mode\n        :param init_cache: bool: Initialize the cache for the transformer\n        :param output_attentions: bool: Return the attention weights\n        :param output_hidden_states: bool: Return the hidden states of all layers\n        :param return_dict: bool: Return a dictionary of outputs\n        :param extra_embedding: Optional[Union[jnp.ndarray: Pass in the embedding of a new word\n        :param None]]: Pass the extra embedding to the model\n        :return: A tuple of logits and hidden_states\n\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n        if attention_mask is None:\n            attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.model(\n            input_ids,\n            attention_mask,\n            position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            extra_embedding=extra_embedding\n        )\n\n        hidden_states = outputs[0]\n        prediction = self.classifier(hidden_states)\n        if return_dict:\n            return FlaxSequenceClassifierOutput(\n                logits=prediction,\n                hidden_states=hidden_states\n            )\n        else:\n            return prediction,\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaForSequenceClassificationModule.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, deterministic=True, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True, extra_embedding=None)</code>","text":"<p>The call function is the main function of a Flax module. It takes in all the inputs to the model and returns all outputs from it. The call function can be called directly on an instance of a class, or by using parentheses after an instance:     &gt;&gt;&gt; my_model = MyModel()  # instantiate your model class     &gt;&gt;&gt; output = my_model(input)  # call your model with input data as arguments to call</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the class instance</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass the input to the model</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Specify which tokens are masked</p> <code>None</code> <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in the sequence</p> <code>None</code> <code>deterministic</code> <code>bool</code> <p>bool: Control whether the model is run in deterministic or stochastic mode</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the transformer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Return the hidden states of all layers</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of outputs</p> <code>True</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray: Pass in the embedding of a new word</p> <code>None</code> <code>None]]</code> <p>Pass the extra embedding to the model</p> required <p>Returns:</p> Type Description <p>A tuple of logits and hidden_states</p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array = None,\n        position_ids: chex.Array = None,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n):\n    \"\"\"\n    The __call__ function is the main function of a Flax module.\n    It takes in all the inputs to the model and returns all outputs from it.\n    The __call__ function can be called directly on an instance of a class, or by using parentheses after an instance:\n        &amp;gt;&amp;gt;&amp;gt; my_model = MyModel()  # instantiate your model class\n        &amp;gt;&amp;gt;&amp;gt; output = my_model(input)  # call your model with input data as arguments to __call__\n\n    :param self: Refer to the class instance\n    :param input_ids: chex.Array: Pass the input to the model\n    :param attention_mask: chex.Array: Specify which tokens are masked\n    :param position_ids: chex.Array: Specify the position of each token in the sequence\n    :param deterministic: bool: Control whether the model is run in deterministic or stochastic mode\n    :param init_cache: bool: Initialize the cache for the transformer\n    :param output_attentions: bool: Return the attention weights\n    :param output_hidden_states: bool: Return the hidden states of all layers\n    :param return_dict: bool: Return a dictionary of outputs\n    :param extra_embedding: Optional[Union[jnp.ndarray: Pass in the embedding of a new word\n    :param None]]: Pass the extra embedding to the model\n    :return: A tuple of logits and hidden_states\n\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n    if attention_mask is None:\n        attention_mask = jnp.ones_like(input_ids)\n    if position_ids is None:\n        position_ids = jnp.broadcast_to(\n            jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n            (batch_size, seq_length)\n        )\n    outputs = self.model(\n        input_ids,\n        attention_mask,\n        position_ids,\n        deterministic=deterministic,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n        extra_embedding=extra_embedding\n    )\n\n    hidden_states = outputs[0]\n    prediction = self.classifier(hidden_states)\n    if return_dict:\n        return FlaxSequenceClassifierOutput(\n            logits=prediction,\n            hidden_states=hidden_states\n        )\n    else:\n        return prediction,\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaForSequenceClassificationModule.setup","title":"<code>setup()</code>","text":"<p>The setup function is called once at the beginning of training. It initializes the model and optimizer, and sets up any other state that needs to be initialized.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <p>Returns:</p> Type Description <p>A tuple of the model and the classifier</p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>def setup(self):\n    \"\"\"\n    The setup function is called once at the beginning of training.\n    It initializes the model and optimizer, and sets up any other state that needs to be initialized.\n\n    :param self: Access variables that belong to the class\n    :return: A tuple of the model and the classifier\n    \"\"\"\n    self.model = FlaxLlamaModule(self.config, dtype=self.dtype)\n    self.classifier = nn.Dense(\n        self.num_classes,\n        dtype=self.dtype,\n        param_dtype=self.param_dtype,\n        use_bias=False,\n        kernel_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n        precision=self.precision,\n    )\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaMLP","title":"<code>FlaxLlamaMLP</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>class FlaxLlamaMLP(nn.Module):\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -&gt; None:\n        config = self.config\n\n        if self.config.bits is not None:\n            _dot_general_cls = q_config.fully_quantized(\n                fwd_bits=self.config.bits,\n                bwd_bits=self.config.bits\n            )\n        else:\n            _dot_general_cls = None\n\n        dot_general_cls = q_flax.QDotGeneral(_dot_general_cls)\n\n        self.gate_proj = nn.Dense(\n            config.intermediate_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n            precision=self.precision,\n            dot_general=dot_general_cls\n        )\n        self.down_proj = nn.Dense(\n            config.hidden_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n            precision=self.precision,\n            dot_general=dot_general_cls\n        )\n        self.up_proj = nn.Dense(\n            config.intermediate_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(self.config.initializer_range),\n            precision=self.precision,\n            dot_general=dot_general_cls\n        )\n        self.dropout = nn.Dropout(rate=self.config.resid_pdrop)\n\n    def __call__(self, x: jnp.ndarray, deterministic: bool = True) -&gt; jnp.ndarray:\n        \"\"\"\n        The __call__ function is the main function of a class.\n        It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments).\n        The __call__ method enables instances of a class to be called like standard Python functions.\n\n        :param self: Represent the instance of the class\n        :param x: jnp.ndarray: Pass in the input to the layer\n        :param deterministic: bool: Determine whether to use dropout\n        :return: A tensor that is the result of applying a dropout function to x\n\n        \"\"\"\n        x = self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))\n        x = self.dropout(x, deterministic=deterministic)\n        return x\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaMLP.__call__","title":"<code>__call__(x, deterministic=True)</code>","text":"<p>The call function is the main function of a class. It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments). The call method enables instances of a class to be called like standard Python functions.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>x</code> <code>ndarray</code> <p>jnp.ndarray: Pass in the input to the layer</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A tensor that is the result of applying a dropout function to x</p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __call__(self, x: jnp.ndarray, deterministic: bool = True) -&gt; jnp.ndarray:\n    \"\"\"\n    The __call__ function is the main function of a class.\n    It is called when an instance of the class (an object) is invoked as a function, i.e., obj(arguments).\n    The __call__ method enables instances of a class to be called like standard Python functions.\n\n    :param self: Represent the instance of the class\n    :param x: jnp.ndarray: Pass in the input to the layer\n    :param deterministic: bool: Determine whether to use dropout\n    :return: A tensor that is the result of applying a dropout function to x\n\n    \"\"\"\n    x = self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))\n    x = self.dropout(x, deterministic=deterministic)\n    return x\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaModule","title":"<code>FlaxLlamaModule</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>class FlaxLlamaModule(nn.Module):\n    config: LlamaConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n\n        self.embed_tokens = nn.Embed(\n            self.config.vocab_size,\n            self.config.hidden_size,\n            embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n        self.dropout = nn.Dropout(rate=self.config.embd_pdrop)\n        self.layers = FlaxLlamaBlockCollection(self.config, dtype=self.dtype, param_dtype=self.param_dtype,\n                                               precision=self.precision)\n        self.norm = RMSNorm(self.config.hidden_size, eps=self.config.rms_norm_eps, dtype=self.dtype,\n                            param_dtype=self.param_dtype)\n        config = self.config\n        self.causal_mask = make_causal_mask(jnp.ones((1, config.max_position_embeddings)))\n        self.freq_cis = precompute_freq_cis(\n            max_position_embedding=config.max_position_embeddings,\n            head_dim=config.hidden_size // config.num_attention_heads\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            input_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n    ):\n        \"\"\"    \n        The __call__ function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids\n        and returns the output of the model. The __call__ function also has optional arguments that can be used to control\n        the behavior of the model (e.g., deterministic=True). These optional arguments are passed as keyword arguments when\n        calling a Flax model.\n\n        :param self: Represent the instance of the class\n        :param input_ids: chex.Array: Pass in the input token ids\n        :param attention_mask: chex.Array: Mask out the padding tokens\n        :param position_ids: chex.Array: Indicate the position of each token in a sequence\n        :param deterministic: bool: Control whether dropout is applied or not\n        :param input_embeds: chex.Array: Pass in the embeddings of the input tokens\n        :param init_cache: bool: Initialize the cache\n        :param output_attentions: bool: Determine whether to return the attentions or not\n        :param output_hidden_states: bool: Determine whether to return hidden states\n        :param return_dict: bool: Return a dictionary of the output or not\n        :param extra_embedding: Optional[Union[jnp.ndarray: Pass in the embedding of the\n        :param None]]: Pass in the extra embedding\n        :return: A tuple of:\n\n        \"\"\"\n        if input_embeds is None:\n            input_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n\n        batch_size, sequence_length = input_ids.shape\n        assert sequence_length &lt;= self.config.max_position_embeddings, (f'Position out of range '\n                                                                        f'(Model Support '\n                                                                        f'{self.config.max_position_embeddings} got'\n                                                                        f' {sequence_length})')\n        input_embeds = input_embeds + extra_embedding if extra_embedding is not None else input_embeds\n        hidden_states = self.dropout(input_embeds, deterministic=deterministic)\n\n        outputs = self.layers(\n            hidden_states=hidden_states,\n            freq_cis=self.freq_cis,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            causal_mask=self.causal_mask,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n        hidden_states = self.norm(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states = outputs[1] + (hidden_states,)\n            outputs = (hidden_states, all_hidden_states) + outputs[2:]\n        else:\n            outputs = (hidden_states,) + outputs[1:]\n\n        if not return_dict:\n            return tuple(v for v in outputs if v is not None)\n\n        return FlaxBaseModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=outputs[1],\n            attentions=outputs[-1],\n        )\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaModule.__call__","title":"<code>__call__(input_ids, attention_mask, position_ids, deterministic=True, input_embeds=None, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True, extra_embedding=None)</code>","text":"<p>The call function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids and returns the output of the model. The call function also has optional arguments that can be used to control the behavior of the model (e.g., deterministic=True). These optional arguments are passed as keyword arguments when calling a Flax model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass in the input token ids</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the padding tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Indicate the position of each token in a sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Control whether dropout is applied or not</p> <code>True</code> <code>input_embeds</code> <code>Array</code> <p>chex.Array: Pass in the embeddings of the input tokens</p> <code>None</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attentions or not</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Determine whether to return hidden states</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the output or not</p> <code>True</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray: Pass in the embedding of the</p> <code>None</code> <code>None]]</code> <p>Pass in the extra embedding</p> required <p>Returns:</p> Type Description <p>A tuple of:</p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        deterministic: bool = True,\n        input_embeds: chex.Array = None,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None\n):\n    \"\"\"    \n    The __call__ function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids\n    and returns the output of the model. The __call__ function also has optional arguments that can be used to control\n    the behavior of the model (e.g., deterministic=True). These optional arguments are passed as keyword arguments when\n    calling a Flax model.\n\n    :param self: Represent the instance of the class\n    :param input_ids: chex.Array: Pass in the input token ids\n    :param attention_mask: chex.Array: Mask out the padding tokens\n    :param position_ids: chex.Array: Indicate the position of each token in a sequence\n    :param deterministic: bool: Control whether dropout is applied or not\n    :param input_embeds: chex.Array: Pass in the embeddings of the input tokens\n    :param init_cache: bool: Initialize the cache\n    :param output_attentions: bool: Determine whether to return the attentions or not\n    :param output_hidden_states: bool: Determine whether to return hidden states\n    :param return_dict: bool: Return a dictionary of the output or not\n    :param extra_embedding: Optional[Union[jnp.ndarray: Pass in the embedding of the\n    :param None]]: Pass in the extra embedding\n    :return: A tuple of:\n\n    \"\"\"\n    if input_embeds is None:\n        input_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n\n    batch_size, sequence_length = input_ids.shape\n    assert sequence_length &lt;= self.config.max_position_embeddings, (f'Position out of range '\n                                                                    f'(Model Support '\n                                                                    f'{self.config.max_position_embeddings} got'\n                                                                    f' {sequence_length})')\n    input_embeds = input_embeds + extra_embedding if extra_embedding is not None else input_embeds\n    hidden_states = self.dropout(input_embeds, deterministic=deterministic)\n\n    outputs = self.layers(\n        hidden_states=hidden_states,\n        freq_cis=self.freq_cis,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        causal_mask=self.causal_mask,\n        deterministic=deterministic,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n\n    hidden_states = outputs[0]\n    hidden_states = self.norm(hidden_states)\n\n    if output_hidden_states:\n        all_hidden_states = outputs[1] + (hidden_states,)\n        outputs = (hidden_states, all_hidden_states) + outputs[2:]\n    else:\n        outputs = (hidden_states,) + outputs[1:]\n\n    if not return_dict:\n        return tuple(v for v in outputs if v is not None)\n\n    return FlaxBaseModelOutput(\n        last_hidden_state=hidden_states,\n        hidden_states=outputs[1],\n        attentions=outputs[-1],\n    )\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaPreTrainedModel","title":"<code>FlaxLlamaPreTrainedModel</code>","text":"<p>             Bases: <code>FlaxPreTrainedModel</code></p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>class FlaxLlamaPreTrainedModel(FlaxPreTrainedModel):\n    config_class = LlamaConfig\n    base_model_prefix = \"model\"\n    module_class: nn.Module = None\n\n    def __init__(\n            self,\n            config: LlamaConfig,\n            input_shape: Tuple = (1, 1),\n            seed: int = 0,\n            dtype: jnp.dtype = jnp.float32,\n            _do_init: bool = True,\n            **kwargs,\n    ):\n        \"\"\"    \n        The __init__ function is called when the class is instantiated.\n        It sets up the instance of the class, and defines what happens when it's created.\n        The __init__ function can take arguments, but self is always required (it refers to the instance of the object).\n\n\n        :param self: Refer to the object itself\n        :param config: LlamaConfig: Pass the configuration to the module\n        :param input_shape: Tuple: Specify the shape of the input to the model\n        :param seed: int: Set the seed for random number generation\n        :param dtype: jnp.dtype: Specify the data type of the input\n        :param _do_init: bool: Control whether the module is initialized or not\n        :param **kwargs: Pass in any additional parameters that the module_class might need\n        :param : Specify the number of layers in the network\n        :return: The super() of the class\n\n        \"\"\"\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n\n        \"\"\"    \n        The init_weights function is used to initialize the weights of a model.\n\n        :param self: Access variables that belong to the class\n        :param rng: jax.random.PRNGKey: Initialize the weights of the model\n        :param input_shape: Tuple: Specify the shape of the input tensor\n        :param params: FrozenDict: Pass in the parameters of a pre-trained model\n        :return: A frozendict of parameters\n\n        \"\"\"\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rngs,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(rngs, input_ids, attention_mask, position_ids, return_dict=False)\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n\n        \"\"\"    \n        The init_cache function is used to initialize the cache for a given batch size and sequence length.\n        The cache is a dictionary that contains all the intermediate states from each layer in the model.\n        This allows us to run inference on multiple batches without having to re-run forward passes through every layer in \n        the model, which would be very slow.\n\n        :param self: Access the module\n        :param batch_size: Define the batch size of the input tensors\n        :param max_length: Set the length of the input sequence\n        :return: A dictionary with the following keys:\n\n        \"\"\"\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array = None,\n            position_ids: chex.Array = None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n            add_params_field: bool = False\n    ):\n\n        \"\"\"    \n        The __call__ function is the main function of a JAX module.\n        It takes in inputs and returns outputs, but it also has some other important features:\n        - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end.\n        - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.\n\n        :param self: Represent the instance of the class\n        :param input_ids: chex.Array: Pass in the input tokens\n        :param attention_mask: chex.Array: Mask out certain tokens in the input\n        :param position_ids: chex.Array: Create the positional embeddings\n        :param params: dict: Pass in the parameters of the model\n        :param past_key_values: dict: Pass in the past key values from a previous call to __call__\n        :param dropout_rng: jax.random.PRNGKey: Make sure that the dropout is applied in a random way\n        :param train: bool: Determine whether to use dropout or not\n        :param output_attentions: Optional[bool]: Determine whether to return the attention weights\n        :param output_hidden_states: Optional[bool]: Return the hidden states of all layers\n        :param return_dict: Optional[bool]: Determine whether to return a dictionary or not\n        :param extra_embedding: Optional[Union[jnp.ndarray,None]]: Pass in the embedding for the input_ids\n        :param add_params_field: bool: Add the params field to the inputs dictionary\n        :return: A tuple of the following:\n\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        assert sequence_length &lt;= self.config.max_position_embeddings, (f'Position out of range '\n                                                                        f'(Model Support '\n                                                                        f'{self.config.max_position_embeddings} got'\n                                                                        f' {sequence_length})')\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rngs = {}\n        if dropout_rng is not None:\n            rngs[\"dropout\"] = dropout_rng\n\n        rngs['params'] = jax.random.key(0)\n        inputs = {\"params\": params or self.params} if add_params_field else params or self.params\n\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,\n            False,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            extra_embedding,\n            rngs=rngs,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaPreTrainedModel.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, params=None, past_key_values=None, dropout_rng=None, train=False, output_attentions=None, output_hidden_states=None, return_dict=None, extra_embedding=None, add_params_field=False)</code>","text":"<p>The call function is the main function of a JAX module. It takes in inputs and returns outputs, but it also has some other important features: - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end. - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass in the input tokens</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain tokens in the input</p> <code>None</code> <code>position_ids</code> <code>Array</code> <p>chex.Array: Create the positional embeddings</p> <code>None</code> <code>params</code> <code>dict</code> <p>dict: Pass in the parameters of the model</p> <code>None</code> <code>past_key_values</code> <code>dict</code> <p>dict: Pass in the past key values from a previous call to call</p> <code>None</code> <code>dropout_rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Make sure that the dropout is applied in a random way</p> <code>None</code> <code>train</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the attention weights</p> <code>None</code> <code>output_hidden_states</code> <code>Optional[bool]</code> <p>Optional[bool]: Return the hidden states of all layers</p> <code>None</code> <code>return_dict</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return a dictionary or not</p> <code>None</code> <code>extra_embedding</code> <code>Optional[Union[ndarray, None]]</code> <p>Optional[Union[jnp.ndarray,None]]: Pass in the embedding for the input_ids</p> <code>None</code> <code>add_params_field</code> <code>bool</code> <p>bool: Add the params field to the inputs dictionary</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of the following:</p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array = None,\n        position_ids: chex.Array = None,\n        params: dict = None,\n        past_key_values: dict = None,\n        dropout_rng: jax.random.PRNGKey = None,\n        train: bool = False,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        extra_embedding: Optional[Union[jnp.ndarray, None]] = None,\n        add_params_field: bool = False\n):\n\n    \"\"\"    \n    The __call__ function is the main function of a JAX module.\n    It takes in inputs and returns outputs, but it also has some other important features:\n    - It can take in mutable state (e.g., past_key_values) that will be updated during the call and returned at the end.\n    - It can take in random number generators (rngs) that are used to generate random numbers for dropout or sampling operations.\n\n    :param self: Represent the instance of the class\n    :param input_ids: chex.Array: Pass in the input tokens\n    :param attention_mask: chex.Array: Mask out certain tokens in the input\n    :param position_ids: chex.Array: Create the positional embeddings\n    :param params: dict: Pass in the parameters of the model\n    :param past_key_values: dict: Pass in the past key values from a previous call to __call__\n    :param dropout_rng: jax.random.PRNGKey: Make sure that the dropout is applied in a random way\n    :param train: bool: Determine whether to use dropout or not\n    :param output_attentions: Optional[bool]: Determine whether to return the attention weights\n    :param output_hidden_states: Optional[bool]: Return the hidden states of all layers\n    :param return_dict: Optional[bool]: Determine whether to return a dictionary or not\n    :param extra_embedding: Optional[Union[jnp.ndarray,None]]: Pass in the embedding for the input_ids\n    :param add_params_field: bool: Add the params field to the inputs dictionary\n    :return: A tuple of the following:\n\n    \"\"\"\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n    batch_size, sequence_length = input_ids.shape\n\n    assert sequence_length &lt;= self.config.max_position_embeddings, (f'Position out of range '\n                                                                    f'(Model Support '\n                                                                    f'{self.config.max_position_embeddings} got'\n                                                                    f' {sequence_length})')\n\n    if position_ids is None:\n        if past_key_values is not None:\n            raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n    if attention_mask is None:\n        attention_mask = jnp.ones((batch_size, sequence_length))\n\n    rngs = {}\n    if dropout_rng is not None:\n        rngs[\"dropout\"] = dropout_rng\n\n    rngs['params'] = jax.random.key(0)\n    inputs = {\"params\": params or self.params} if add_params_field else params or self.params\n\n    if past_key_values:\n        inputs[\"cache\"] = past_key_values\n        mutable = [\"cache\"]\n    else:\n        mutable = False\n\n    outputs = self.module.apply(\n        inputs,\n        jnp.array(input_ids, dtype=\"i4\"),\n        jnp.array(attention_mask, dtype=\"i4\"),\n        jnp.array(position_ids, dtype=\"i4\"),\n        not train,\n        False,\n        output_attentions,\n        output_hidden_states,\n        return_dict,\n        extra_embedding,\n        rngs=rngs,\n        mutable=mutable,\n    )\n\n    if past_key_values is not None and return_dict:\n        outputs, past_key_values = outputs\n        outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n        return outputs\n    elif past_key_values is not None and not return_dict:\n        outputs, past_key_values = outputs\n        outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n    return outputs\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaPreTrainedModel.__init__","title":"<code>__init__(config, input_shape=(1, 1), seed=0, dtype=jnp.float32, _do_init=True, **kwargs)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the instance of the class, and defines what happens when it's created. The init function can take arguments, but self is always required (it refers to the instance of the object).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>config</code> <code>LlamaConfig</code> <p>LlamaConfig: Pass the configuration to the module</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Specify the shape of the input to the model</p> <code>(1, 1)</code> <code>seed</code> <code>int</code> <p>int: Set the seed for random number generation</p> <code>0</code> <code>dtype</code> <code>dtype</code> <p>jnp.dtype: Specify the data type of the input</p> <code>float32</code> <code>_do_init</code> <code>bool</code> <p>bool: Control whether the module is initialized or not</p> <code>True</code> <code>**kwargs</code> <p>Pass in any additional parameters that the module_class might need</p> <code>{}</code> <code></code> <p>Specify the number of layers in the network</p> required <p>Returns:</p> Type Description <p>The super() of the class</p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __init__(\n        self,\n        config: LlamaConfig,\n        input_shape: Tuple = (1, 1),\n        seed: int = 0,\n        dtype: jnp.dtype = jnp.float32,\n        _do_init: bool = True,\n        **kwargs,\n):\n    \"\"\"    \n    The __init__ function is called when the class is instantiated.\n    It sets up the instance of the class, and defines what happens when it's created.\n    The __init__ function can take arguments, but self is always required (it refers to the instance of the object).\n\n\n    :param self: Refer to the object itself\n    :param config: LlamaConfig: Pass the configuration to the module\n    :param input_shape: Tuple: Specify the shape of the input to the model\n    :param seed: int: Set the seed for random number generation\n    :param dtype: jnp.dtype: Specify the data type of the input\n    :param _do_init: bool: Control whether the module is initialized or not\n    :param **kwargs: Pass in any additional parameters that the module_class might need\n    :param : Specify the number of layers in the network\n    :return: The super() of the class\n\n    \"\"\"\n    module = self.module_class(config=config, dtype=dtype, **kwargs)\n    super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaPreTrainedModel.init_cache","title":"<code>init_cache(batch_size, max_length)</code>","text":"<p>The init_cache function is used to initialize the cache for a given batch size and sequence length. The cache is a dictionary that contains all the intermediate states from each layer in the model. This allows us to run inference on multiple batches without having to re-run forward passes through every layer in  the model, which would be very slow.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the module</p> required <code>batch_size</code> <p>Define the batch size of the input tensors</p> required <code>max_length</code> <p>Set the length of the input sequence</p> required <p>Returns:</p> Type Description <p>A dictionary with the following keys:</p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>def init_cache(self, batch_size, max_length):\n\n    \"\"\"    \n    The init_cache function is used to initialize the cache for a given batch size and sequence length.\n    The cache is a dictionary that contains all the intermediate states from each layer in the model.\n    This allows us to run inference on multiple batches without having to re-run forward passes through every layer in \n    the model, which would be very slow.\n\n    :param self: Access the module\n    :param batch_size: Define the batch size of the input tensors\n    :param max_length: Set the length of the input sequence\n    :return: A dictionary with the following keys:\n\n    \"\"\"\n    input_ids = jnp.ones((batch_size, max_length))\n    attention_mask = jnp.ones_like(input_ids)\n    position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n    init_variables = self.module.init(\n        jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n    )\n    return init_variables[\"cache\"]\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.FlaxLlamaPreTrainedModel.init_weights","title":"<code>init_weights(rng, input_shape, params=None)</code>","text":"<p>The init_weights function is used to initialize the weights of a model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Initialize the weights of the model</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Specify the shape of the input tensor</p> required <code>params</code> <code>FrozenDict</code> <p>FrozenDict: Pass in the parameters of a pre-trained model</p> <code>None</code> <p>Returns:</p> Type Description <code>FrozenDict</code> <p>A frozendict of parameters</p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: FrozenDict = None) -&gt; FrozenDict:\n\n    \"\"\"    \n    The init_weights function is used to initialize the weights of a model.\n\n    :param self: Access variables that belong to the class\n    :param rng: jax.random.PRNGKey: Initialize the weights of the model\n    :param input_shape: Tuple: Specify the shape of the input tensor\n    :param params: FrozenDict: Pass in the parameters of a pre-trained model\n    :return: A frozendict of parameters\n\n    \"\"\"\n    input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n    attention_mask = jnp.ones_like(input_ids)\n    position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n    params_rng, dropout_rng = jax.random.split(rng)\n    rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n    if self.config.add_cross_attention:\n        encoder_hidden_states = jnp.zeros(input_shape + (self.config.hidden_size,))\n        encoder_attention_mask = attention_mask\n        module_init_outputs = self.module.init(\n            rngs,\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            return_dict=False,\n        )\n    else:\n        module_init_outputs = self.module.init(rngs, input_ids, attention_mask, position_ids, return_dict=False)\n\n    random_params = module_init_outputs[\"params\"]\n\n    if params is not None:\n        random_params = flatten_dict(unfreeze(random_params))\n        params = flatten_dict(unfreeze(params))\n        for missing_key in self._missing_keys:\n            params[missing_key] = random_params[missing_key]\n        self._missing_keys = set()\n        return freeze(unflatten_dict(params))\n    else:\n        return random_params\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.LlamaConfig","title":"<code>LlamaConfig</code>","text":"<p>             Bases: <code>PretrainedConfig</code>, <code>JaxBaseClassModel</code></p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>class LlamaConfig(PretrainedConfig, JaxBaseClassModel):\n    model_type = \"llama\"\n\n    def __init__(\n            self,\n            vocab_size: int = 32000,\n            hidden_size: int = 4096,\n            intermediate_size: int = 11008,\n            num_hidden_layers: int = 32,\n            num_attention_heads: int = 32,\n            number_rep_kv: int = 1,\n            num_key_value_heads: Optional[int] = None,\n            max_position_embeddings: int = 2048,\n            rms_norm_eps: float = 1e-6,\n            initializer_range: float = 0.02,\n            use_cache: bool = True,\n            bos_token_id: int = 0,\n            eos_token_id: int = 1,\n            resid_pdrop: float = 0.0,\n            embd_pdrop: float = 0.0,\n            attn_pdrop: float = 0.0,\n            rope_theta: float = 10000.,\n            attention_bias: bool = False,\n            tie_word_embeddings: bool = False,\n            gradient_checkpointing: str = \"nothing_saveable\",\n            fcm_min_ratio: float = -1,\n            fcm_max_ratio: float = -1,\n            use_pjit_attention_force: bool = False,\n            rope_scaling: Dict[str, Union[str, float]] = None,\n            use_flash_attention: bool = False,\n            use_sacn_mlp: bool = False,\n            flash_attn_query_chunk_size: int = 1024,\n            flash_attn_key_chunk_size: int = 1024,\n            scan_mlp_chunk_size: int = 1024,\n            bits: Optional[int] = None,\n            hidden_act: str = 'silu',\n            pretraining_tp: int = 1,\n            axis_dims: Sequence[int] = (1, -1, 1, 1),\n            axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"mp\"),\n            scan_layers: bool = True,\n            use_shard_map: bool = True,\n            **kwargs,\n    ):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It sets up the attributes of an object, which are sometimes called fields or properties.\n        The __init__ function can accept arguments, but self must be the first one.\n\n        :param self: Refer to the object itself\n        :param vocab_size: int: Set the size of the vocabulary\n        :param hidden_size: int: Set the size of the hidden layers in each transformer block\n        :param intermediate_size: int: Set the size of the intermediate layer\n        :param num_hidden_layers: int: Determine the number of layers in the transformer\n        :param num_attention_heads: int: Determine the number of attention heads\n        :param number_rep_kv: int: Set the number of times to repeat the key and value vectors\n        :param num_key_value_heads: Optional[int]: Define the number of key-value heads\n        :param max_position_embeddings: int: Set the maximum length of a sequence\n        :param rms_norm_eps: float: Prevent division by zero in the rms normalization\n        :param initializer_range: float: Initialize the weights of the model\n        :param use_cache: bool: Determine whether the attention layer should use a cache for faster computation\n        :param bos_token_id: int: Set the beginning of sequence token\n        :param eos_token_id: int: Specify the end of sentence token\n        :param resid_pdrop: float: Set the dropout rate for residual connections\n        :param embd_pdrop: float: Dropout the embedding layer\n        :param attn_pdrop: float: Dropout the attention weights\n        :param tie_word_embeddings: bool: Tie the word embeddings and output layer weights\n        :param gradient_checkpointing: str: Specify how to checkpoint the gradients\n        :param fcm_min_ratio: float: Set the minimum ratio of the number of elements in a tensor to be processed by flash\n        :param fcm_max_ratio: float: Determine the maximum ratio of\n        :param use_shard_map: bool: whenever to use shard map for attention\n        :param use_pjit_attention_force: bool: Determine whether to use the pytorch jit compiler\n        :param rope_scaling: Dict[str: Define the scaling of the rope\n        :param Union[str: Specify the type of the parameter\n        :param float]]: Specify the type of the parameter\n        :param use_flash_attention: bool: Determine whether to use the flash attention or not\n        :param use_sacn_mlp: bool: Determine whether to use scan_mlp or not\n        :param flash_attn_query_chunk_size: int: Specify the chunk size of the query tensor\n        :param flash_attn_key_chunk_size: int: Determine the chunk size of the key tensor\n        :param scan_mlp_chunk_size: int: Specify the chunk size of the scan_mlp\n        :param bits: Optional[int]: Specify the number of bits used to quantize the weights\n        :param rope_theta: float : rope_theta for compute rope\n        :param attention_bias: bool : whenever to use attention bias or no\n        :param hidden_act: str : hidden_act for mlp\n        :param axis_dims: Sequence[int]: Specify the dimensions of each axis\n        :param axis_names: Sequence[str]: Specify the names of the axes in a tensor\n        :param scan_layers: bool: Determine whether to use the scan_layers or not\n        :param **kwargs: Pass a variable number of keyword arguments to a function\n        :param : Define the number of layers in the model\n        :return: Nothing\n\n        \"\"\"\n        num_key_value_heads = num_key_value_heads or number_rep_kv * num_attention_heads\n        self.num_key_value_heads = num_key_value_heads\n        self.vocab_size = vocab_size\n\n        self.number_rep_kv = number_rep_kv\n        self.hidden_size = hidden_size\n        self.initializer_range = initializer_range\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.rope_theta = rope_theta\n        self.attention_bias = attention_bias\n        self.num_attention_heads = num_attention_heads\n        self.max_position_embeddings = max_position_embeddings\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.pretraining_tp = pretraining_tp\n        self.resid_pdrop = resid_pdrop\n        self.embd_pdrop = embd_pdrop\n        self.attn_pdrop = attn_pdrop\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.fcm_min_ratio = fcm_min_ratio\n        self.hidden_act = hidden_act\n        self.use_shard_map = use_shard_map\n        self.fcm_max_ratio = fcm_max_ratio\n        self.rope_scaling = rope_scaling\n        self.use_flash_attention = use_flash_attention\n        self.use_sacn_mlp = use_sacn_mlp\n        self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n        self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.bits = bits\n        self.scan_layers = scan_layers\n        super().__init__(\n            axis_dims=axis_dims,\n            axis_names=axis_names,\n            backend=None,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n    @staticmethod\n    def get_partition_rules(fully_fsdp: bool = True):\n        \"\"\"\n        The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n            1) A regex string that matches the name of one or more parameters in the model.\n            2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n        :param fully_fsdp: bool: Determine whether to partition the model fully or not\n        :return: A list of tuples\n\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PS(\"tp\", (\"fsdp\", \"mp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PS((\"fsdp\", \"mp\"), \"tp\")),\n            (\"self_attn/o_proj/kernel\", PS(\"tp\", (\"fsdp\", \"mp\"))),\n\n            (\"mlp/gate_proj/kernel\", PS((\"fsdp\", \"mp\"), \"tp\")),\n            (\"mlp/down_proj/kernel\", PS(\"tp\", (\"fsdp\", \"mp\"))),\n            (\"mlp/up_proj/kernel\", PS((\"fsdp\", \"mp\"), \"tp\")),\n\n            (\"input_layernorm/kernel\", PS(None)),\n            (\"post_attention_layernorm/kernel\", PS(None)),\n\n            (\"model/norm/kernel\", PS(None)),\n            (\"lm_head/kernel\", PS((\"fsdp\", \"mp\"), \"tp\")),\n            ('.*', PS(None)),\n        ) if not fully_fsdp else (\n\n            (\"model/embed_tokens/embedding\", PS((\"fsdp\", \"mp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PS((\"fsdp\", \"mp\"))),\n            (\"self_attn/o_proj/kernel\", PS((\"fsdp\", \"mp\"))),\n\n            (\"mlp/gate_proj/kernel\", PS((\"fsdp\", \"mp\"))),\n            (\"mlp/down_proj/kernel\", PS((\"fsdp\", \"mp\"))),\n            (\"mlp/up_proj/kernel\", PS((\"fsdp\", \"mp\"))),\n\n            (\"input_layernorm/kernel\", PS(None)),\n            (\"post_attention_layernorm/kernel\", PS(None)),\n\n            (\"model/norm/kernel\", PS(None)),\n            (\"lm_head/kernel\", PS((\"fsdp\", \"mp\"))),\n            ('.*', PS('fsdp')),\n        )\n\n    def add_jax_args(self,\n                     resid_pdrop: float = 0.0,\n                     embd_pdrop: float = 0.0,\n                     attn_pdrop: float = 0.0,\n                     tie_word_embeddings: bool = False,\n                     gradient_checkpointing: str = 'nothing_saveable',\n                     fcm_min_ratio: float = 0.0,\n                     fcm_max_ratio: float = 0.0,\n                     use_pjit_attention_force: bool = False,\n                     use_flash_attention: bool = False,\n                     use_sacn_mlp: bool = False,\n                     use_shard_map: bool = True,\n                     flash_attn_query_chunk_size: int = 1024,\n                     flash_attn_key_chunk_size: int = 1024,\n                     scan_mlp_chunk_size: int = 1024,\n                     number_rep_kv: int = 1,\n                     bits: Optional[int] = None,\n                     rope_theta: float = 10000.,\n                     attention_bias: bool = False,\n                     hidden_act: str = 'silu',\n                     axis_dims: Sequence[int] = (1, -1, 1, 1),\n                     axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"mp\"),\n                     q_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n                     k_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n                     v_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n                     b_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), None, \"mp\", None),\n                     a_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n                     backend: Optional[str] = None,\n                     scan_layers: bool = True,\n                     **kwargs,\n                     ):\n        \"\"\"\n        The add_jax_args function adds the following arguments to the Transformer class:\n\n        :param self: Refer to the current object\n        :param resid_pdrop: float: Set the dropout rate for residual connections\n        :param embd_pdrop: float: Set the probability of dropping an embedding\n        :param attn_pdrop: float: Set the probability of dropping out the attention layer\n        :param tie_word_embeddings: bool: Tie the word embeddings to the decoder\n        :param gradient_checkpointing: str: Control the amount of memory used by jax\n        :param use_shard_map: bool: whenever to use shard map for attention\n        :param fcm_min_ratio: float: Control the minimum ratio of the number of chunks to be used in flash-based computation\n        :param fcm_max_ratio: float: Set the maximum ratio of the number of input tokens to output tokens\n        :param use_pjit_attention_force: bool: Determine if the attention force is used\n        :param use_flash_attention: bool: Determine whether to use the flash attention or not\n        :param use_sacn_mlp: bool: Determine whether to use the scan_mlp function or not\n        :param flash_attn_query_chunk_size: int: Determine the size of the chunks that will be used to compute\n        :param flash_attn_key_chunk_size: int: Set the size of the key chunk\n        :param scan_mlp_chunk_size: int: Set the chunk size for scan_mlp\n        :param number_rep_kv: int: Determine how many times the key and value vectors are repeated\n        :param bits: Optional[int]: Determine the number of bits used in the quantization\n        :param rope_theta: float : rope_theta for compute rope\n        :param attention_bias: bool : whenever to use attention bias or no\n        :param hidden_act: str : hidden_act for mlp\n        :param axis_dims: Sequence[int]: Specify the dimension of each axis\n        :param axis_names: Sequence[str]: Name the axes of the tensor\n        :param q_ps: jax.sharding.PartitionSpec: Specify the partitioning of the query tensor\n        :param k_ps: jax.sharding.PartitionSpec: Partition the key matrix\n        :param v_ps: jax.sharding.PartitionSpec: Specify the partitioning of the value tensor\n        :param b_ps: jax.sharding.PartitionSpec: Specify the Attention Bias partition spec\n        :param a_ps: jax.sharding.PartitionSpec: Specify the partitioning of the attention weights\n        :param backend: typing.Optional[str]: backend to use for model\n        :param scan_layers: bool: Determine whether to use scan layers or not\n        :return: The following:\n\n        \"\"\"\n        self.axis_names = axis_names\n        self.axis_dims = axis_dims\n        self.q_ps = q_ps\n        self.k_ps = k_ps\n        self.v_ps = v_ps\n        self.b_ps = b_ps\n        self.a_ps = a_ps\n        self.backend = backend\n        self.scan_layers = scan_layers\n        self.axis_names = axis_names\n        self.use_shard_map = use_shard_map\n        self.axis_dims = axis_dims\n        self.use_flash_attention = use_flash_attention\n        self.embd_pdrop = embd_pdrop\n        self.number_rep_kv = number_rep_kv\n        self.resid_pdrop = resid_pdrop\n        self.rope_theta = rope_theta\n        self.attention_bias = attention_bias\n        self.attn_pdrop = attn_pdrop\n        self.hidden_act = hidden_act\n        self.tie_word_embeddings = tie_word_embeddings\n        self.gradient_checkpointing = gradient_checkpointing\n        self.fcm_min_ratio = fcm_min_ratio\n        self.fcm_max_ratio = fcm_max_ratio\n        self.use_pjit_attention_force = use_pjit_attention_force\n\n        self.use_sacn_mlp = use_sacn_mlp\n        self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n        self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.bits = bits\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return 'params', 'dropout', 'fcm'\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.LlamaConfig.__init__","title":"<code>__init__(vocab_size=32000, hidden_size=4096, intermediate_size=11008, num_hidden_layers=32, num_attention_heads=32, number_rep_kv=1, num_key_value_heads=None, max_position_embeddings=2048, rms_norm_eps=1e-06, initializer_range=0.02, use_cache=True, bos_token_id=0, eos_token_id=1, resid_pdrop=0.0, embd_pdrop=0.0, attn_pdrop=0.0, rope_theta=10000.0, attention_bias=False, tie_word_embeddings=False, gradient_checkpointing='nothing_saveable', fcm_min_ratio=-1, fcm_max_ratio=-1, use_pjit_attention_force=False, rope_scaling=None, use_flash_attention=False, use_sacn_mlp=False, flash_attn_query_chunk_size=1024, flash_attn_key_chunk_size=1024, scan_mlp_chunk_size=1024, bits=None, hidden_act='silu', pretraining_tp=1, axis_dims=(1, -1, 1, 1), axis_names=('dp', 'fsdp', 'tp', 'mp'), scan_layers=True, use_shard_map=True, **kwargs)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the attributes of an object, which are sometimes called fields or properties. The init function can accept arguments, but self must be the first one.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>vocab_size</code> <code>int</code> <p>int: Set the size of the vocabulary</p> <code>32000</code> <code>hidden_size</code> <code>int</code> <p>int: Set the size of the hidden layers in each transformer block</p> <code>4096</code> <code>intermediate_size</code> <code>int</code> <p>int: Set the size of the intermediate layer</p> <code>11008</code> <code>num_hidden_layers</code> <code>int</code> <p>int: Determine the number of layers in the transformer</p> <code>32</code> <code>num_attention_heads</code> <code>int</code> <p>int: Determine the number of attention heads</p> <code>32</code> <code>number_rep_kv</code> <code>int</code> <p>int: Set the number of times to repeat the key and value vectors</p> <code>1</code> <code>num_key_value_heads</code> <code>Optional[int]</code> <p>Optional[int]: Define the number of key-value heads</p> <code>None</code> <code>max_position_embeddings</code> <code>int</code> <p>int: Set the maximum length of a sequence</p> <code>2048</code> <code>rms_norm_eps</code> <code>float</code> <p>float: Prevent division by zero in the rms normalization</p> <code>1e-06</code> <code>initializer_range</code> <code>float</code> <p>float: Initialize the weights of the model</p> <code>0.02</code> <code>use_cache</code> <code>bool</code> <p>bool: Determine whether the attention layer should use a cache for faster computation</p> <code>True</code> <code>bos_token_id</code> <code>int</code> <p>int: Set the beginning of sequence token</p> <code>0</code> <code>eos_token_id</code> <code>int</code> <p>int: Specify the end of sentence token</p> <code>1</code> <code>resid_pdrop</code> <code>float</code> <p>float: Set the dropout rate for residual connections</p> <code>0.0</code> <code>embd_pdrop</code> <code>float</code> <p>float: Dropout the embedding layer</p> <code>0.0</code> <code>attn_pdrop</code> <code>float</code> <p>float: Dropout the attention weights</p> <code>0.0</code> <code>tie_word_embeddings</code> <code>bool</code> <p>bool: Tie the word embeddings and output layer weights</p> <code>False</code> <code>gradient_checkpointing</code> <code>str</code> <p>str: Specify how to checkpoint the gradients</p> <code>'nothing_saveable'</code> <code>fcm_min_ratio</code> <code>float</code> <p>float: Set the minimum ratio of the number of elements in a tensor to be processed by flash</p> <code>-1</code> <code>fcm_max_ratio</code> <code>float</code> <p>float: Determine the maximum ratio of</p> <code>-1</code> <code>use_shard_map</code> <code>bool</code> <p>bool: whenever to use shard map for attention</p> <code>True</code> <code>use_pjit_attention_force</code> <code>bool</code> <p>bool: Determine whether to use the pytorch jit compiler</p> <code>False</code> <code>rope_scaling</code> <code>Dict[str, Union[str, float]]</code> <p>Dict[str: Define the scaling of the rope</p> <code>None</code> <code>Union[str</code> <p>Specify the type of the parameter</p> required <code>float]]</code> <p>Specify the type of the parameter</p> required <code>use_flash_attention</code> <code>bool</code> <p>bool: Determine whether to use the flash attention or not</p> <code>False</code> <code>use_sacn_mlp</code> <code>bool</code> <p>bool: Determine whether to use scan_mlp or not</p> <code>False</code> <code>flash_attn_query_chunk_size</code> <code>int</code> <p>int: Specify the chunk size of the query tensor</p> <code>1024</code> <code>flash_attn_key_chunk_size</code> <code>int</code> <p>int: Determine the chunk size of the key tensor</p> <code>1024</code> <code>scan_mlp_chunk_size</code> <code>int</code> <p>int: Specify the chunk size of the scan_mlp</p> <code>1024</code> <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Specify the number of bits used to quantize the weights</p> <code>None</code> <code>rope_theta</code> <code>float</code> <p>float : rope_theta for compute rope</p> <code>10000.0</code> <code>attention_bias</code> <code>bool</code> <p>bool : whenever to use attention bias or no</p> <code>False</code> <code>hidden_act</code> <code>str</code> <p>str : hidden_act for mlp</p> <code>'silu'</code> <code>axis_dims</code> <code>Sequence[int]</code> <p>Sequence[int]: Specify the dimensions of each axis</p> <code>(1, -1, 1, 1)</code> <code>axis_names</code> <code>Sequence[str]</code> <p>Sequence[str]: Specify the names of the axes in a tensor</p> <code>('dp', 'fsdp', 'tp', 'mp')</code> <code>scan_layers</code> <code>bool</code> <p>bool: Determine whether to use the scan_layers or not</p> <code>True</code> <code>**kwargs</code> <p>Pass a variable number of keyword arguments to a function</p> <code>{}</code> <code></code> <p>Define the number of layers in the model</p> required <p>Returns:</p> Type Description <p>Nothing</p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>def __init__(\n        self,\n        vocab_size: int = 32000,\n        hidden_size: int = 4096,\n        intermediate_size: int = 11008,\n        num_hidden_layers: int = 32,\n        num_attention_heads: int = 32,\n        number_rep_kv: int = 1,\n        num_key_value_heads: Optional[int] = None,\n        max_position_embeddings: int = 2048,\n        rms_norm_eps: float = 1e-6,\n        initializer_range: float = 0.02,\n        use_cache: bool = True,\n        bos_token_id: int = 0,\n        eos_token_id: int = 1,\n        resid_pdrop: float = 0.0,\n        embd_pdrop: float = 0.0,\n        attn_pdrop: float = 0.0,\n        rope_theta: float = 10000.,\n        attention_bias: bool = False,\n        tie_word_embeddings: bool = False,\n        gradient_checkpointing: str = \"nothing_saveable\",\n        fcm_min_ratio: float = -1,\n        fcm_max_ratio: float = -1,\n        use_pjit_attention_force: bool = False,\n        rope_scaling: Dict[str, Union[str, float]] = None,\n        use_flash_attention: bool = False,\n        use_sacn_mlp: bool = False,\n        flash_attn_query_chunk_size: int = 1024,\n        flash_attn_key_chunk_size: int = 1024,\n        scan_mlp_chunk_size: int = 1024,\n        bits: Optional[int] = None,\n        hidden_act: str = 'silu',\n        pretraining_tp: int = 1,\n        axis_dims: Sequence[int] = (1, -1, 1, 1),\n        axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"mp\"),\n        scan_layers: bool = True,\n        use_shard_map: bool = True,\n        **kwargs,\n):\n    \"\"\"\n    The __init__ function is called when the class is instantiated.\n    It sets up the attributes of an object, which are sometimes called fields or properties.\n    The __init__ function can accept arguments, but self must be the first one.\n\n    :param self: Refer to the object itself\n    :param vocab_size: int: Set the size of the vocabulary\n    :param hidden_size: int: Set the size of the hidden layers in each transformer block\n    :param intermediate_size: int: Set the size of the intermediate layer\n    :param num_hidden_layers: int: Determine the number of layers in the transformer\n    :param num_attention_heads: int: Determine the number of attention heads\n    :param number_rep_kv: int: Set the number of times to repeat the key and value vectors\n    :param num_key_value_heads: Optional[int]: Define the number of key-value heads\n    :param max_position_embeddings: int: Set the maximum length of a sequence\n    :param rms_norm_eps: float: Prevent division by zero in the rms normalization\n    :param initializer_range: float: Initialize the weights of the model\n    :param use_cache: bool: Determine whether the attention layer should use a cache for faster computation\n    :param bos_token_id: int: Set the beginning of sequence token\n    :param eos_token_id: int: Specify the end of sentence token\n    :param resid_pdrop: float: Set the dropout rate for residual connections\n    :param embd_pdrop: float: Dropout the embedding layer\n    :param attn_pdrop: float: Dropout the attention weights\n    :param tie_word_embeddings: bool: Tie the word embeddings and output layer weights\n    :param gradient_checkpointing: str: Specify how to checkpoint the gradients\n    :param fcm_min_ratio: float: Set the minimum ratio of the number of elements in a tensor to be processed by flash\n    :param fcm_max_ratio: float: Determine the maximum ratio of\n    :param use_shard_map: bool: whenever to use shard map for attention\n    :param use_pjit_attention_force: bool: Determine whether to use the pytorch jit compiler\n    :param rope_scaling: Dict[str: Define the scaling of the rope\n    :param Union[str: Specify the type of the parameter\n    :param float]]: Specify the type of the parameter\n    :param use_flash_attention: bool: Determine whether to use the flash attention or not\n    :param use_sacn_mlp: bool: Determine whether to use scan_mlp or not\n    :param flash_attn_query_chunk_size: int: Specify the chunk size of the query tensor\n    :param flash_attn_key_chunk_size: int: Determine the chunk size of the key tensor\n    :param scan_mlp_chunk_size: int: Specify the chunk size of the scan_mlp\n    :param bits: Optional[int]: Specify the number of bits used to quantize the weights\n    :param rope_theta: float : rope_theta for compute rope\n    :param attention_bias: bool : whenever to use attention bias or no\n    :param hidden_act: str : hidden_act for mlp\n    :param axis_dims: Sequence[int]: Specify the dimensions of each axis\n    :param axis_names: Sequence[str]: Specify the names of the axes in a tensor\n    :param scan_layers: bool: Determine whether to use the scan_layers or not\n    :param **kwargs: Pass a variable number of keyword arguments to a function\n    :param : Define the number of layers in the model\n    :return: Nothing\n\n    \"\"\"\n    num_key_value_heads = num_key_value_heads or number_rep_kv * num_attention_heads\n    self.num_key_value_heads = num_key_value_heads\n    self.vocab_size = vocab_size\n\n    self.number_rep_kv = number_rep_kv\n    self.hidden_size = hidden_size\n    self.initializer_range = initializer_range\n    self.intermediate_size = intermediate_size\n    self.num_hidden_layers = num_hidden_layers\n    self.rope_theta = rope_theta\n    self.attention_bias = attention_bias\n    self.num_attention_heads = num_attention_heads\n    self.max_position_embeddings = max_position_embeddings\n    self.rms_norm_eps = rms_norm_eps\n    self.use_cache = use_cache\n    self.pretraining_tp = pretraining_tp\n    self.resid_pdrop = resid_pdrop\n    self.embd_pdrop = embd_pdrop\n    self.attn_pdrop = attn_pdrop\n    self.gradient_checkpointing = gradient_checkpointing\n    self.use_pjit_attention_force = use_pjit_attention_force\n    self.fcm_min_ratio = fcm_min_ratio\n    self.hidden_act = hidden_act\n    self.use_shard_map = use_shard_map\n    self.fcm_max_ratio = fcm_max_ratio\n    self.rope_scaling = rope_scaling\n    self.use_flash_attention = use_flash_attention\n    self.use_sacn_mlp = use_sacn_mlp\n    self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n    self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n    self.scan_mlp_chunk_size = scan_mlp_chunk_size\n    self.bits = bits\n    self.scan_layers = scan_layers\n    super().__init__(\n        axis_dims=axis_dims,\n        axis_names=axis_names,\n        backend=None,\n        bos_token_id=bos_token_id,\n        eos_token_id=eos_token_id,\n        tie_word_embeddings=tie_word_embeddings,\n        **kwargs,\n    )\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.LlamaConfig.add_jax_args","title":"<code>add_jax_args(resid_pdrop=0.0, embd_pdrop=0.0, attn_pdrop=0.0, tie_word_embeddings=False, gradient_checkpointing='nothing_saveable', fcm_min_ratio=0.0, fcm_max_ratio=0.0, use_pjit_attention_force=False, use_flash_attention=False, use_sacn_mlp=False, use_shard_map=True, flash_attn_query_chunk_size=1024, flash_attn_key_chunk_size=1024, scan_mlp_chunk_size=1024, number_rep_kv=1, bits=None, rope_theta=10000.0, attention_bias=False, hidden_act='silu', axis_dims=(1, -1, 1, 1), axis_names=('dp', 'fsdp', 'tp', 'mp'), q_ps=jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None), k_ps=jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None), v_ps=jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None), b_ps=jax.sharding.PartitionSpec(('dp', 'fsdp'), None, 'mp', None), a_ps=jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None), backend=None, scan_layers=True, **kwargs)</code>","text":"<p>The add_jax_args function adds the following arguments to the Transformer class:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the current object</p> required <code>resid_pdrop</code> <code>float</code> <p>float: Set the dropout rate for residual connections</p> <code>0.0</code> <code>embd_pdrop</code> <code>float</code> <p>float: Set the probability of dropping an embedding</p> <code>0.0</code> <code>attn_pdrop</code> <code>float</code> <p>float: Set the probability of dropping out the attention layer</p> <code>0.0</code> <code>tie_word_embeddings</code> <code>bool</code> <p>bool: Tie the word embeddings to the decoder</p> <code>False</code> <code>gradient_checkpointing</code> <code>str</code> <p>str: Control the amount of memory used by jax</p> <code>'nothing_saveable'</code> <code>use_shard_map</code> <code>bool</code> <p>bool: whenever to use shard map for attention</p> <code>True</code> <code>fcm_min_ratio</code> <code>float</code> <p>float: Control the minimum ratio of the number of chunks to be used in flash-based computation</p> <code>0.0</code> <code>fcm_max_ratio</code> <code>float</code> <p>float: Set the maximum ratio of the number of input tokens to output tokens</p> <code>0.0</code> <code>use_pjit_attention_force</code> <code>bool</code> <p>bool: Determine if the attention force is used</p> <code>False</code> <code>use_flash_attention</code> <code>bool</code> <p>bool: Determine whether to use the flash attention or not</p> <code>False</code> <code>use_sacn_mlp</code> <code>bool</code> <p>bool: Determine whether to use the scan_mlp function or not</p> <code>False</code> <code>flash_attn_query_chunk_size</code> <code>int</code> <p>int: Determine the size of the chunks that will be used to compute</p> <code>1024</code> <code>flash_attn_key_chunk_size</code> <code>int</code> <p>int: Set the size of the key chunk</p> <code>1024</code> <code>scan_mlp_chunk_size</code> <code>int</code> <p>int: Set the chunk size for scan_mlp</p> <code>1024</code> <code>number_rep_kv</code> <code>int</code> <p>int: Determine how many times the key and value vectors are repeated</p> <code>1</code> <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Determine the number of bits used in the quantization</p> <code>None</code> <code>rope_theta</code> <code>float</code> <p>float : rope_theta for compute rope</p> <code>10000.0</code> <code>attention_bias</code> <code>bool</code> <p>bool : whenever to use attention bias or no</p> <code>False</code> <code>hidden_act</code> <code>str</code> <p>str : hidden_act for mlp</p> <code>'silu'</code> <code>axis_dims</code> <code>Sequence[int]</code> <p>Sequence[int]: Specify the dimension of each axis</p> <code>(1, -1, 1, 1)</code> <code>axis_names</code> <code>Sequence[str]</code> <p>Sequence[str]: Name the axes of the tensor</p> <code>('dp', 'fsdp', 'tp', 'mp')</code> <code>q_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the partitioning of the query tensor</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>k_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Partition the key matrix</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>v_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the partitioning of the value tensor</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>b_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the Attention Bias partition spec</p> <code>PartitionSpec(('dp', 'fsdp'), None, 'mp', None)</code> <code>a_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the partitioning of the attention weights</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>backend</code> <code>Optional[str]</code> <p>typing.Optional[str]: backend to use for model</p> <code>None</code> <code>scan_layers</code> <code>bool</code> <p>bool: Determine whether to use scan layers or not</p> <code>True</code> <p>Returns:</p> Type Description <p>The following:</p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>def add_jax_args(self,\n                 resid_pdrop: float = 0.0,\n                 embd_pdrop: float = 0.0,\n                 attn_pdrop: float = 0.0,\n                 tie_word_embeddings: bool = False,\n                 gradient_checkpointing: str = 'nothing_saveable',\n                 fcm_min_ratio: float = 0.0,\n                 fcm_max_ratio: float = 0.0,\n                 use_pjit_attention_force: bool = False,\n                 use_flash_attention: bool = False,\n                 use_sacn_mlp: bool = False,\n                 use_shard_map: bool = True,\n                 flash_attn_query_chunk_size: int = 1024,\n                 flash_attn_key_chunk_size: int = 1024,\n                 scan_mlp_chunk_size: int = 1024,\n                 number_rep_kv: int = 1,\n                 bits: Optional[int] = None,\n                 rope_theta: float = 10000.,\n                 attention_bias: bool = False,\n                 hidden_act: str = 'silu',\n                 axis_dims: Sequence[int] = (1, -1, 1, 1),\n                 axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"mp\"),\n                 q_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n                 k_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n                 v_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n                 b_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), None, \"mp\", None),\n                 a_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n                 backend: Optional[str] = None,\n                 scan_layers: bool = True,\n                 **kwargs,\n                 ):\n    \"\"\"\n    The add_jax_args function adds the following arguments to the Transformer class:\n\n    :param self: Refer to the current object\n    :param resid_pdrop: float: Set the dropout rate for residual connections\n    :param embd_pdrop: float: Set the probability of dropping an embedding\n    :param attn_pdrop: float: Set the probability of dropping out the attention layer\n    :param tie_word_embeddings: bool: Tie the word embeddings to the decoder\n    :param gradient_checkpointing: str: Control the amount of memory used by jax\n    :param use_shard_map: bool: whenever to use shard map for attention\n    :param fcm_min_ratio: float: Control the minimum ratio of the number of chunks to be used in flash-based computation\n    :param fcm_max_ratio: float: Set the maximum ratio of the number of input tokens to output tokens\n    :param use_pjit_attention_force: bool: Determine if the attention force is used\n    :param use_flash_attention: bool: Determine whether to use the flash attention or not\n    :param use_sacn_mlp: bool: Determine whether to use the scan_mlp function or not\n    :param flash_attn_query_chunk_size: int: Determine the size of the chunks that will be used to compute\n    :param flash_attn_key_chunk_size: int: Set the size of the key chunk\n    :param scan_mlp_chunk_size: int: Set the chunk size for scan_mlp\n    :param number_rep_kv: int: Determine how many times the key and value vectors are repeated\n    :param bits: Optional[int]: Determine the number of bits used in the quantization\n    :param rope_theta: float : rope_theta for compute rope\n    :param attention_bias: bool : whenever to use attention bias or no\n    :param hidden_act: str : hidden_act for mlp\n    :param axis_dims: Sequence[int]: Specify the dimension of each axis\n    :param axis_names: Sequence[str]: Name the axes of the tensor\n    :param q_ps: jax.sharding.PartitionSpec: Specify the partitioning of the query tensor\n    :param k_ps: jax.sharding.PartitionSpec: Partition the key matrix\n    :param v_ps: jax.sharding.PartitionSpec: Specify the partitioning of the value tensor\n    :param b_ps: jax.sharding.PartitionSpec: Specify the Attention Bias partition spec\n    :param a_ps: jax.sharding.PartitionSpec: Specify the partitioning of the attention weights\n    :param backend: typing.Optional[str]: backend to use for model\n    :param scan_layers: bool: Determine whether to use scan layers or not\n    :return: The following:\n\n    \"\"\"\n    self.axis_names = axis_names\n    self.axis_dims = axis_dims\n    self.q_ps = q_ps\n    self.k_ps = k_ps\n    self.v_ps = v_ps\n    self.b_ps = b_ps\n    self.a_ps = a_ps\n    self.backend = backend\n    self.scan_layers = scan_layers\n    self.axis_names = axis_names\n    self.use_shard_map = use_shard_map\n    self.axis_dims = axis_dims\n    self.use_flash_attention = use_flash_attention\n    self.embd_pdrop = embd_pdrop\n    self.number_rep_kv = number_rep_kv\n    self.resid_pdrop = resid_pdrop\n    self.rope_theta = rope_theta\n    self.attention_bias = attention_bias\n    self.attn_pdrop = attn_pdrop\n    self.hidden_act = hidden_act\n    self.tie_word_embeddings = tie_word_embeddings\n    self.gradient_checkpointing = gradient_checkpointing\n    self.fcm_min_ratio = fcm_min_ratio\n    self.fcm_max_ratio = fcm_max_ratio\n    self.use_pjit_attention_force = use_pjit_attention_force\n\n    self.use_sacn_mlp = use_sacn_mlp\n    self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n    self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n    self.scan_mlp_chunk_size = scan_mlp_chunk_size\n    self.bits = bits\n</code></pre>"},{"location":"lib-python-EasyDel-modules-llama-modelling_llama_flax/#lib.python.EasyDel.modules.llama.modelling_llama_flax.LlamaConfig.get_partition_rules","title":"<code>get_partition_rules(fully_fsdp=True)</code>  <code>staticmethod</code>","text":"<p>The get_partition_rules function is used to define the partitioning scheme for a model. It returns a list of tuples, where each tuple contains two elements:     1) A regex string that matches the name of one or more parameters in the model.     2) A PartitionScheme object that defines how those parameters should be partitioned across devices.</p> <p>Parameters:</p> Name Type Description Default <code>fully_fsdp</code> <code>bool</code> <p>bool: Determine whether to partition the model fully or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A list of tuples</p> Source code in <code>lib/python/EasyDel/modules/llama/modelling_llama_flax.py</code> <pre><code>@staticmethod\ndef get_partition_rules(fully_fsdp: bool = True):\n    \"\"\"\n    The get_partition_rules function is used to define the partitioning scheme for a model.\n    It returns a list of tuples, where each tuple contains two elements:\n        1) A regex string that matches the name of one or more parameters in the model.\n        2) A PartitionScheme object that defines how those parameters should be partitioned across devices.\n\n    :param fully_fsdp: bool: Determine whether to partition the model fully or not\n    :return: A list of tuples\n\n    \"\"\"\n    return (\n\n        (\"model/embed_tokens/embedding\", PS(\"tp\", (\"fsdp\", \"mp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PS((\"fsdp\", \"mp\"), \"tp\")),\n        (\"self_attn/o_proj/kernel\", PS(\"tp\", (\"fsdp\", \"mp\"))),\n\n        (\"mlp/gate_proj/kernel\", PS((\"fsdp\", \"mp\"), \"tp\")),\n        (\"mlp/down_proj/kernel\", PS(\"tp\", (\"fsdp\", \"mp\"))),\n        (\"mlp/up_proj/kernel\", PS((\"fsdp\", \"mp\"), \"tp\")),\n\n        (\"input_layernorm/kernel\", PS(None)),\n        (\"post_attention_layernorm/kernel\", PS(None)),\n\n        (\"model/norm/kernel\", PS(None)),\n        (\"lm_head/kernel\", PS((\"fsdp\", \"mp\"), \"tp\")),\n        ('.*', PS(None)),\n    ) if not fully_fsdp else (\n\n        (\"model/embed_tokens/embedding\", PS((\"fsdp\", \"mp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PS((\"fsdp\", \"mp\"))),\n        (\"self_attn/o_proj/kernel\", PS((\"fsdp\", \"mp\"))),\n\n        (\"mlp/gate_proj/kernel\", PS((\"fsdp\", \"mp\"))),\n        (\"mlp/down_proj/kernel\", PS((\"fsdp\", \"mp\"))),\n        (\"mlp/up_proj/kernel\", PS((\"fsdp\", \"mp\"))),\n\n        (\"input_layernorm/kernel\", PS(None)),\n        (\"post_attention_layernorm/kernel\", PS(None)),\n\n        (\"model/norm/kernel\", PS(None)),\n        (\"lm_head/kernel\", PS((\"fsdp\", \"mp\"))),\n        ('.*', PS('fsdp')),\n    )\n</code></pre>"},{"location":"lib-python-EasyDel-modules-lucid_transformer-modelling_lt_flax/","title":"modules.lucid_transformer.modelling_lt_flax","text":""},{"location":"lib-python-EasyDel-modules-mistral-modelling_mistral_flax/","title":"modules.mistral.modelling_mistral_flax","text":""},{"location":"lib-python-EasyDel-modules-mistral-modelling_mistral_flax/#lib.python.EasyDel.modules.mistral.modelling_mistral_flax.FlaxMistralAttention","title":"<code>FlaxMistralAttention</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>class FlaxMistralAttention(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[None, jax.lax.Precision]] = jax.lax.Precision('fastest')\n\n    def setup(self) -&gt; None:\n        config = self.config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        if self.config.bits is not None:\n            _dot_general_cls = q_config.fully_quantized(\n                fwd_bits=self.config.bits,\n                bwd_bits=self.config.bits\n            )\n        else:\n            _dot_general_cls = None\n\n        dot_general_cls = q_flax.QDotGeneral(_dot_general_cls)\n        dense = functools.partial(\n            nn.Dense,\n            use_bias=False,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            kernel_init=nn.initializers.normal(),\n            dot_general=dot_general_cls\n        )\n\n        self.q_proj = dense(self.num_heads * self.head_dim)\n        self.k_proj = dense(self.num_key_value_heads * self.head_dim)\n        self.v_proj = dense(self.num_key_value_heads * self.head_dim)\n        self.o_proj = dense(self.hidden_size)\n        self.rotary = FlaxMistralRotaryEmbedding(self.dtype)\n\n    @nn.compact\n    def concatenate_to_cache_(self, query: chex.Array, key: chex.Array, value: chex.Array, attention_mask: chex.Array):\n        is_cache_available = self.has_variable('cache', 'key')\n        key_cache = self.variable('cache', 'key', jnp.zeros, key.shape, key.dtype)\n        value_cache = self.variable('cache', 'value', jnp.zeros, key.shape, value.dtype)\n        index_cache = self.variable('cache', 'index', lambda: jnp.array(0, dtype=jnp.int32))\n        if is_cache_available:\n            *bd, ml, nh, dph = key_cache.value.shape\n            indices = (0,) * len(bd) + (index_cache.value, 0, 0)\n            key = jax.lax.dynamic_update_slice(key_cache.value, key, indices)\n            value = jax.lax.dynamic_update_slice(value_cache.value, value, indices)\n            key_cache.value = key\n            value_cache.value = value\n            num_updated_cache_vector = query.shape[1]\n            index_cache.value = index_cache.value + num_updated_cache_vector\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(ml) &lt; index_cache.value,\n                tuple(bd) + (1, num_updated_cache_vector, ml)\n            )\n            attention_mask = nn.combine_masks(pad_mask, attention_mask)\n        return query, key, value, attention_mask\n\n    @staticmethod\n    def _t(query, key, value):\n        return jnp.transpose(query, (0, 2, 1, 3)), jnp.transpose(key, (0, 2, 1, 3)), jnp.transpose(value, (0, 2, 1, 3))\n\n    def t_rotary(self, batch_size, sequence_length, query, key, value, freq_cis, position_ids):\n        query = query.reshape(batch_size, sequence_length, self.config.num_attention_heads, self.head_dim)\n        key = key.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n        value = value.reshape(batch_size, sequence_length, self.config.num_key_value_heads, self.head_dim)\n\n        query, key, value = self._t(query, key, value)\n        query, key = self.rotary(position_ids=position_ids, query=query, key=key, freq_cis=freq_cis)\n        key = repeat_kv_bnsh(key, self.num_key_value_groups)\n        value = repeat_kv_bnsh(value, self.num_key_value_groups)\n        return self._t(query, key, value)\n\n    def __call__(\n            self,\n            hidden_state: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = True\n    ):\n        \"\"\"\n        The __call__ function is the main function of a JAX module.\n        It defines how the module behaves when called as a function, and it's what you'll use to call your model in practice.\n        The __call__ method takes an input tensor (x) and returns an output tensor (y).\n        In this case, we're defining our model to be a simple linear layer with no activation: y = x @ w + b.\n\n        :param self: Refer to the object itself\n        :param hidden_state: chex.Array: Pass in the hidden state of the model\n        :param freq_cis: chex.Array: Create the t_rotary variable\n        :param attention_mask: chex.Array: Mask the attention weights\n        :param causal_mask: chex.Array: Mask the attention weights\n        :param position_ids: chex.Array: Specify the position of each token in a sequence\n        :param deterministic: bool: Determine whether to use dropout or not\n        :param init_cache: bool: Initialize the cache\n        :param output_attentions: bool: Determine whether to return the attention weights\n        :return: A tuple of (out, attn_output)\n\n        \"\"\"\n        batch_size, sequence_length = hidden_state.shape[:2]\n        query, key, value = self.q_proj(hidden_state), self.k_proj(hidden_state), self.v_proj(hidden_state)\n\n        if self.config.use_pjit_attention_force:\n            query = with_sharding_constraint(query, PS('fsdp', 'mp', None))\n            key = with_sharding_constraint(key, PS('fsdp', 'mp', None))\n            value = with_sharding_constraint(value, PS('fsdp', 'mp', None))\n        query, key, value = self.t_rotary(\n            batch_size=batch_size,\n            sequence_length=sequence_length,\n            query=query,\n            key=key,\n            value=value,\n            freq_cis=freq_cis,\n            position_ids=position_ids\n        )\n        if self.has_variable('cache', 'key') or init_cache:\n            query, key, value, attention_mask = self.concatenate_to_cache_(query, key, value, attention_mask)\n\n        q_l, k_l = query.shape[1], key.shape[1]\n        if self.has_variable('cache', 'key'):\n            mask_shift: int = self.variables['cache']['index']\n            dl = self.variables['cache']['key'].shape[1]\n            causal_mask = jax.lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1, q_l, dl)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :q_l, :k_l]\n        dropout_rng = None\n        if not deterministic and self.config.attn_pdrop &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n        causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n\n        attention_mask = nn.combine_masks(attention_mask, causal_mask)\n\n        if self.config.use_flash_attention and not (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n\n            if attention_mask.ndim == 2:\n                attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n            if attention_mask.shape[1] != self.config.num_attention_heads:\n                attention_mask = attention_mask.repeat(self.config.num_attention_heads, 1, )\n            attention_bias = lax.select(\n                attention_mask &gt; 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n            )\n            attn_weights = None\n            rtp_axis = (0, 2, 1, 3)\n            attn_output = smart_flash_attention(\n                q=jnp.transpose(query, rtp_axis),\n                k=jnp.transpose(key, rtp_axis),\n                v=jnp.transpose(value, rtp_axis),\n                q_ps=self.config.q_ps,\n                k_ps=self.config.k_ps,\n                v_ps=self.config.v_ps,\n                b_ps=self.config.b_ps,\n                a_ps=self.config.a_ps,\n                bias=attention_bias,\n                block_q=self.config.flash_attn_query_chunk_size,\n                block_k=self.config.flash_attn_key_chunk_size,\n                block_b=1,\n                num_attention_heads=self.config.num_attention_heads,\n                precision=self.precision,\n                dtype=self.dtype,\n                causal=False,\n                mesh=self.config.jax_mesh(),\n                dropout_rng=dropout_rng,\n                deterministic=deterministic,\n                q_seq_len=q_l,\n                kv_seq_len=k_l,\n                attn_pdrop=self.config.attn_pdrop,\n                head_dims=self.head_dim,\n                force_float32_tpu=True\n            )\n            attn_output = jnp.transpose(attn_output, rtp_axis)\n        else:\n            query_length, key_length = query.shape[1], key.shape[1]\n\n            if self.has_variable(\"cache\", \"cached_key\"):\n                mask_shift = self.variables[\"cache\"][\"cache_index\"]\n                max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n                causal_mask = lax.dynamic_slice(\n                    causal_mask, (0, 0, mask_shift, 0), (1, 1, query_length, max_decoder_length)\n                )\n            else:\n                causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n            batch_size = hidden_state.shape[0]\n            causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n            if attention_mask.ndim == 2:\n                attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n            attention_mask = combine_masks(attention_mask, causal_mask)\n\n            if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n                key, value, attention_mask = self._concatenate_to_cache(key, value, query,\n                                                                        attention_mask)\n\n            attn_weights = None\n            ring_attention_sharded = shard_map(\n                functools.partial(fjformer.attention.ring_attention_standard, axis_name=\"mp\"),\n                mesh=self.config.jax_mesh(),\n                in_specs=(\n                    self.config.q_ps,\n                    self.config.k_ps,\n                    self.config.v_ps,\n                    self.config.b_ps\n                ),\n                out_specs=self.config.a_ps,\n                check_rep=False\n            )\n\n            attn_output = ring_attention_sharded(\n                query, key, value, attention_mask\n            )\n            attn_output = with_sharding_constraint(attn_output, self.config.a_ps)\n\n        out = self.o_proj(attn_output.reshape(batch_size, sequence_length, self.hidden_size))\n        outputs = (out, attn_output) if output_attentions else (out,)\n        return outputs\n</code></pre>"},{"location":"lib-python-EasyDel-modules-mistral-modelling_mistral_flax/#lib.python.EasyDel.modules.mistral.modelling_mistral_flax.FlaxMistralAttention.__call__","title":"<code>__call__(hidden_state, freq_cis, attention_mask, causal_mask, position_ids, deterministic=True, init_cache=False, output_attentions=True)</code>","text":"<p>The call function is the main function of a JAX module. It defines how the module behaves when called as a function, and it's what you'll use to call your model in practice. The call method takes an input tensor (x) and returns an output tensor (y). In this case, we're defining our model to be a simple linear layer with no activation: y = x @ w + b.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>hidden_state</code> <code>Array</code> <p>chex.Array: Pass in the hidden state of the model</p> required <code>freq_cis</code> <code>Array</code> <p>chex.Array: Create the t_rotary variable</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask the attention weights</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the attention weights</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in a sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights</p> <code>True</code> <p>Returns:</p> Type Description <p>A tuple of (out, attn_output)</p> Source code in <code>lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_state: chex.Array,\n        freq_cis: chex.Array,\n        attention_mask: chex.Array,\n        causal_mask: chex.Array,\n        position_ids: chex.Array,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = True\n):\n    \"\"\"\n    The __call__ function is the main function of a JAX module.\n    It defines how the module behaves when called as a function, and it's what you'll use to call your model in practice.\n    The __call__ method takes an input tensor (x) and returns an output tensor (y).\n    In this case, we're defining our model to be a simple linear layer with no activation: y = x @ w + b.\n\n    :param self: Refer to the object itself\n    :param hidden_state: chex.Array: Pass in the hidden state of the model\n    :param freq_cis: chex.Array: Create the t_rotary variable\n    :param attention_mask: chex.Array: Mask the attention weights\n    :param causal_mask: chex.Array: Mask the attention weights\n    :param position_ids: chex.Array: Specify the position of each token in a sequence\n    :param deterministic: bool: Determine whether to use dropout or not\n    :param init_cache: bool: Initialize the cache\n    :param output_attentions: bool: Determine whether to return the attention weights\n    :return: A tuple of (out, attn_output)\n\n    \"\"\"\n    batch_size, sequence_length = hidden_state.shape[:2]\n    query, key, value = self.q_proj(hidden_state), self.k_proj(hidden_state), self.v_proj(hidden_state)\n\n    if self.config.use_pjit_attention_force:\n        query = with_sharding_constraint(query, PS('fsdp', 'mp', None))\n        key = with_sharding_constraint(key, PS('fsdp', 'mp', None))\n        value = with_sharding_constraint(value, PS('fsdp', 'mp', None))\n    query, key, value = self.t_rotary(\n        batch_size=batch_size,\n        sequence_length=sequence_length,\n        query=query,\n        key=key,\n        value=value,\n        freq_cis=freq_cis,\n        position_ids=position_ids\n    )\n    if self.has_variable('cache', 'key') or init_cache:\n        query, key, value, attention_mask = self.concatenate_to_cache_(query, key, value, attention_mask)\n\n    q_l, k_l = query.shape[1], key.shape[1]\n    if self.has_variable('cache', 'key'):\n        mask_shift: int = self.variables['cache']['index']\n        dl = self.variables['cache']['key'].shape[1]\n        causal_mask = jax.lax.dynamic_slice(\n            causal_mask, (0, 0, mask_shift, 0), (1, 1, q_l, dl)\n        )\n    else:\n        causal_mask = causal_mask[:, :, :q_l, :k_l]\n    dropout_rng = None\n    if not deterministic and self.config.attn_pdrop &gt; 0.0:\n        dropout_rng = self.make_rng(\"dropout\")\n    causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n    if attention_mask.ndim == 2:\n        attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n\n    attention_mask = nn.combine_masks(attention_mask, causal_mask)\n\n    if self.config.use_flash_attention and not (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        if attention_mask.shape[1] != self.config.num_attention_heads:\n            attention_mask = attention_mask.repeat(self.config.num_attention_heads, 1, )\n        attention_bias = lax.select(\n            attention_mask &gt; 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n        )\n        attn_weights = None\n        rtp_axis = (0, 2, 1, 3)\n        attn_output = smart_flash_attention(\n            q=jnp.transpose(query, rtp_axis),\n            k=jnp.transpose(key, rtp_axis),\n            v=jnp.transpose(value, rtp_axis),\n            q_ps=self.config.q_ps,\n            k_ps=self.config.k_ps,\n            v_ps=self.config.v_ps,\n            b_ps=self.config.b_ps,\n            a_ps=self.config.a_ps,\n            bias=attention_bias,\n            block_q=self.config.flash_attn_query_chunk_size,\n            block_k=self.config.flash_attn_key_chunk_size,\n            block_b=1,\n            num_attention_heads=self.config.num_attention_heads,\n            precision=self.precision,\n            dtype=self.dtype,\n            causal=False,\n            mesh=self.config.jax_mesh(),\n            dropout_rng=dropout_rng,\n            deterministic=deterministic,\n            q_seq_len=q_l,\n            kv_seq_len=k_l,\n            attn_pdrop=self.config.attn_pdrop,\n            head_dims=self.head_dim,\n            force_float32_tpu=True\n        )\n        attn_output = jnp.transpose(attn_output, rtp_axis)\n    else:\n        query_length, key_length = query.shape[1], key.shape[1]\n\n        if self.has_variable(\"cache\", \"cached_key\"):\n            mask_shift = self.variables[\"cache\"][\"cache_index\"]\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            causal_mask = lax.dynamic_slice(\n                causal_mask, (0, 0, mask_shift, 0), (1, 1, query_length, max_decoder_length)\n            )\n        else:\n            causal_mask = causal_mask[:, :, :query_length, :key_length]\n\n        batch_size = hidden_state.shape[0]\n        causal_mask = jnp.broadcast_to(causal_mask, (batch_size,) + causal_mask.shape[1:])\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.broadcast_to(jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_mask.shape)\n        attention_mask = combine_masks(attention_mask, causal_mask)\n\n        if self.has_variable(\"cache\", \"cached_key\") or init_cache:\n            key, value, attention_mask = self._concatenate_to_cache(key, value, query,\n                                                                    attention_mask)\n\n        attn_weights = None\n        ring_attention_sharded = shard_map(\n            functools.partial(fjformer.attention.ring_attention_standard, axis_name=\"mp\"),\n            mesh=self.config.jax_mesh(),\n            in_specs=(\n                self.config.q_ps,\n                self.config.k_ps,\n                self.config.v_ps,\n                self.config.b_ps\n            ),\n            out_specs=self.config.a_ps,\n            check_rep=False\n        )\n\n        attn_output = ring_attention_sharded(\n            query, key, value, attention_mask\n        )\n        attn_output = with_sharding_constraint(attn_output, self.config.a_ps)\n\n    out = self.o_proj(attn_output.reshape(batch_size, sequence_length, self.hidden_size))\n    outputs = (out, attn_output) if output_attentions else (out,)\n    return outputs\n</code></pre>"},{"location":"lib-python-EasyDel-modules-mistral-modelling_mistral_flax/#lib.python.EasyDel.modules.mistral.modelling_mistral_flax.FlaxMistralDecoderLayer","title":"<code>FlaxMistralDecoderLayer</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>class FlaxMistralDecoderLayer(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[None, jax.lax.Precision]] = jax.lax.Precision('fastest')\n\n    def setup(self) -&gt; None:\n        self.self_attn = FlaxMistralAttention(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.mlp = FlaxMistralMLP(\n            config=self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.input_layernorm = MistralRMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n        self.post_attention_layernorm = MistralRMSNorm(\n            dim=self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n\n    def __call__(\n            self,\n            hidden_state: chex.Array,\n            freq_cis: chex.Array,\n            attention_mask: chex.Array,\n            causal_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            init_cache: bool = False,\n            output_attentions: bool = True\n    ):\n        \"\"\"\n        The __call__ function is the main function of a TransformerEncoderLayer.\n        It takes in the following arguments:\n            hidden_state (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.\n            freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num\n\n        :param self: Represent the instance of the class\n        :param hidden_state: chex.Array: Represent the input to the encoder layer\n        :param freq_cis: chex.Array: Pass the frequency information to the attention layer\n        :param attention_mask: chex.Array: Mask out the attention weights for certain positions\n        :param causal_mask: chex.Array: Mask the future tokens\n        :param position_ids: chex.Array: Indicate the position of each token in the sequence\n        :param deterministic: bool: Determine whether to use dropout or not\n        :param init_cache: bool: Initialize the cache for the self-attention layer\n        :param output_attentions: bool: Determine whether to return the attention weights or not\n        :return: A tuple of hidden_state and attention_output\n\n        \"\"\"\n        residual = hidden_state\n        attention_output = self.self_attn(\n            hidden_state=self.input_layernorm(hidden_state),\n            freq_cis=freq_cis,\n            attention_mask=attention_mask,\n            causal_mask=causal_mask,\n            position_ids=position_ids,\n            deterministic=deterministic,\n            init_cache=init_cache,\n            output_attentions=output_attentions\n        )\n\n        hidden_state = attention_output[0] + residual\n\n        hidden_state = self.mlp(self.post_attention_layernorm(hidden_state)) + hidden_state\n        outputs = (hidden_state,)\n        if output_attentions:\n            outputs += attention_output[1]\n        return outputs\n</code></pre>"},{"location":"lib-python-EasyDel-modules-mistral-modelling_mistral_flax/#lib.python.EasyDel.modules.mistral.modelling_mistral_flax.FlaxMistralDecoderLayer.__call__","title":"<code>__call__(hidden_state, freq_cis, attention_mask, causal_mask, position_ids, deterministic=True, init_cache=False, output_attentions=True)</code>","text":"<p>The call function is the main function of a TransformerEncoderLayer. It takes in the following arguments:     hidden_state (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.     freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>hidden_state</code> <code>Array</code> <p>chex.Array: Represent the input to the encoder layer</p> required <code>freq_cis</code> <code>Array</code> <p>chex.Array: Pass the frequency information to the attention layer</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the attention weights for certain positions</p> required <code>causal_mask</code> <code>Array</code> <p>chex.Array: Mask the future tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Indicate the position of each token in the sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the self-attention layer</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A tuple of hidden_state and attention_output</p> Source code in <code>lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>def __call__(\n        self,\n        hidden_state: chex.Array,\n        freq_cis: chex.Array,\n        attention_mask: chex.Array,\n        causal_mask: chex.Array,\n        position_ids: chex.Array,\n        deterministic: bool = True,\n        init_cache: bool = False,\n        output_attentions: bool = True\n):\n    \"\"\"\n    The __call__ function is the main function of a TransformerEncoderLayer.\n    It takes in the following arguments:\n        hidden_state (chex.Array): The input to the encoder layer, which is also its output after being processed by all sublayers.\n        freq_cis (chex.Array): A tensor containing frequency-domain representations of each token's context vector, used for computing self-attention weights and biases in a more efficient manner than using position embeddings or sinusoidal positional encoding vectors would allow for [2]. This tensor has shape `(batch_size, num\n\n    :param self: Represent the instance of the class\n    :param hidden_state: chex.Array: Represent the input to the encoder layer\n    :param freq_cis: chex.Array: Pass the frequency information to the attention layer\n    :param attention_mask: chex.Array: Mask out the attention weights for certain positions\n    :param causal_mask: chex.Array: Mask the future tokens\n    :param position_ids: chex.Array: Indicate the position of each token in the sequence\n    :param deterministic: bool: Determine whether to use dropout or not\n    :param init_cache: bool: Initialize the cache for the self-attention layer\n    :param output_attentions: bool: Determine whether to return the attention weights or not\n    :return: A tuple of hidden_state and attention_output\n\n    \"\"\"\n    residual = hidden_state\n    attention_output = self.self_attn(\n        hidden_state=self.input_layernorm(hidden_state),\n        freq_cis=freq_cis,\n        attention_mask=attention_mask,\n        causal_mask=causal_mask,\n        position_ids=position_ids,\n        deterministic=deterministic,\n        init_cache=init_cache,\n        output_attentions=output_attentions\n    )\n\n    hidden_state = attention_output[0] + residual\n\n    hidden_state = self.mlp(self.post_attention_layernorm(hidden_state)) + hidden_state\n    outputs = (hidden_state,)\n    if output_attentions:\n        outputs += attention_output[1]\n    return outputs\n</code></pre>"},{"location":"lib-python-EasyDel-modules-mistral-modelling_mistral_flax/#lib.python.EasyDel.modules.mistral.modelling_mistral_flax.FlaxMistralForCausalLMModule","title":"<code>FlaxMistralForCausalLMModule</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>class FlaxMistralForCausalLMModule(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n        self.model: FlaxMistralModule = FlaxMistralModule(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n        )\n\n        if self.config.bits is not None:\n            _dot_general_cls = q_config.fully_quantized(\n                fwd_bits=self.config.bits,\n                bwd_bits=self.config.bits\n            )\n        else:\n            _dot_general_cls = None\n\n        dot_general_cls = q_flax.QDotGeneral(_dot_general_cls)\n        self.lm_head = nn.Dense(\n            self.config.vocab_size,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            precision=self.precision,\n            dot_general=dot_general_cls\n        )\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            input_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n    ):\n        \"\"\"\n            The __call__ function is the main function of a Flax module. It defines how the model will be called,\n            and what it returns. In this case, we are calling our Transformer model with input_ids and attention_mask\n            as inputs (these are defined in __init__). We also have some optional arguments that can be passed to\n            the call function: deterministic (whether to use dropout), input_embeds (if you want to pass your own embeddings),\n            output_attentions and output_hidden states which return additional outputs from the transformer layers if set True. Finally,\n\n            :param self: Refer to the object itself\n            :param input_ids: chex.Array: Pass in the input tokens\n            :param attention_mask: chex.Array: Mask out the padding tokens\n            :param position_ids: chex.Array: Specify the position of each token in the sequence\n            :param deterministic: bool: Determine whether to use dropout in the model\n            :param input_embeds: chex.Array: Pass in the embeddings of the input tokens\n            :param init_cache: bool: Initialize the cache for the decoder\n            :param output_attentions: bool: Return the attention weights\n            :param output_hidden_states: bool: Return the hidden states of all layers\n            :param return_dict: bool: Return a dictionary of the outputs or just the logits\n            :param : Determine whether to return the logits or not\n            :return: A tuple of (lm_logits, hidden_states, attentions)\n\n        \"\"\"\n        batch_size, seq_length = input_ids.shape\n\n        if attention_mask is None: attention_mask = jnp.ones_like(input_ids)\n        if position_ids is None:\n            position_ids = jnp.broadcast_to(\n                jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n                (batch_size, seq_length)\n            )\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            deterministic=deterministic,\n            input_embeds=input_embeds,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict\n        )\n\n        hidden_states = outputs[0]\n\n        if self.config.tie_word_embeddings:\n            shared_kernel = self.transformer.variables[\"params\"][\"wte\"][\"embedding\"].T\n            lm_logits = self.lm_head.apply({\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n        else:\n            lm_logits = self.lm_head(hidden_states)\n\n        # lm_logits = lm_logits.astype(jnp.float32)\n\n        if not return_dict:\n            return (lm_logits,) + outputs[1:]\n\n        return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n</code></pre>"},{"location":"lib-python-EasyDel-modules-mistral-modelling_mistral_flax/#lib.python.EasyDel.modules.mistral.modelling_mistral_flax.FlaxMistralForCausalLMModule.__call__","title":"<code>__call__(input_ids, attention_mask, position_ids, deterministic=True, input_embeds=None, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True)</code>","text":"<p>The call function is the main function of a Flax module. It defines how the model will be called, and what it returns. In this case, we are calling our Transformer model with input_ids and attention_mask as inputs (these are defined in init). We also have some optional arguments that can be passed to the call function: deterministic (whether to use dropout), input_embeds (if you want to pass your own embeddings), output_attentions and output_hidden states which return additional outputs from the transformer layers if set True. Finally,</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass in the input tokens</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the padding tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in the sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout in the model</p> <code>True</code> <code>input_embeds</code> <code>Array</code> <p>chex.Array: Pass in the embeddings of the input tokens</p> <code>None</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the decoder</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Return the attention weights</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Return the hidden states of all layers</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs or just the logits</p> <code>True</code> <code></code> <p>Determine whether to return the logits or not</p> required <p>Returns:</p> Type Description <p>A tuple of (lm_logits, hidden_states, attentions)</p> Source code in <code>lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        deterministic: bool = True,\n        input_embeds: chex.Array = None,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n):\n    \"\"\"\n        The __call__ function is the main function of a Flax module. It defines how the model will be called,\n        and what it returns. In this case, we are calling our Transformer model with input_ids and attention_mask\n        as inputs (these are defined in __init__). We also have some optional arguments that can be passed to\n        the call function: deterministic (whether to use dropout), input_embeds (if you want to pass your own embeddings),\n        output_attentions and output_hidden states which return additional outputs from the transformer layers if set True. Finally,\n\n        :param self: Refer to the object itself\n        :param input_ids: chex.Array: Pass in the input tokens\n        :param attention_mask: chex.Array: Mask out the padding tokens\n        :param position_ids: chex.Array: Specify the position of each token in the sequence\n        :param deterministic: bool: Determine whether to use dropout in the model\n        :param input_embeds: chex.Array: Pass in the embeddings of the input tokens\n        :param init_cache: bool: Initialize the cache for the decoder\n        :param output_attentions: bool: Return the attention weights\n        :param output_hidden_states: bool: Return the hidden states of all layers\n        :param return_dict: bool: Return a dictionary of the outputs or just the logits\n        :param : Determine whether to return the logits or not\n        :return: A tuple of (lm_logits, hidden_states, attentions)\n\n    \"\"\"\n    batch_size, seq_length = input_ids.shape\n\n    if attention_mask is None: attention_mask = jnp.ones_like(input_ids)\n    if position_ids is None:\n        position_ids = jnp.broadcast_to(\n            jnp.clip(jnp.cumsum(attention_mask, axis=-1) - 1, a_min=0),\n            (batch_size, seq_length)\n        )\n    outputs = self.model(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        deterministic=deterministic,\n        input_embeds=input_embeds,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict\n    )\n\n    hidden_states = outputs[0]\n\n    if self.config.tie_word_embeddings:\n        shared_kernel = self.transformer.variables[\"params\"][\"wte\"][\"embedding\"].T\n        lm_logits = self.lm_head.apply({\"params\": {\"kernel\": shared_kernel}}, hidden_states)\n    else:\n        lm_logits = self.lm_head(hidden_states)\n\n    # lm_logits = lm_logits.astype(jnp.float32)\n\n    if not return_dict:\n        return (lm_logits,) + outputs[1:]\n\n    return FlaxCausalLMOutput(logits=lm_logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n</code></pre>"},{"location":"lib-python-EasyDel-modules-mistral-modelling_mistral_flax/#lib.python.EasyDel.modules.mistral.modelling_mistral_flax.FlaxMistralModule","title":"<code>FlaxMistralModule</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>class FlaxMistralModule(nn.Module):\n    config: MistralConfig\n    dtype: jnp.dtype = jnp.bfloat16\n    param_dtype: jnp.dtype = jnp.bfloat16\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self):\n\n        self.embed_tokens = nn.Embed(\n            self.config.vocab_size,\n            self.config.hidden_size,\n            embedding_init=jax.nn.initializers.normal(stddev=self.config.initializer_range),\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n        )\n\n        self.layers = FlaxMistralDecoratorCollection(\n            self.config,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision\n        )\n        self.norm = MistralRMSNorm(\n            self.config.hidden_size,\n            eps=self.config.rms_norm_eps,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype\n        )\n\n        self.freq_cis = precompute_freq_cis(\n            max_position_embedding=self.config.freq_max_position_embeddings if self.config.freq_max_position_embeddings is not None else self.config.max_position_embeddings,\n            head_dim=self.config.hidden_size // self.config.num_attention_heads\n        )\n        self.causal_mask = nn.make_causal_mask(jnp.ones((1, self.config.c_max_position_embeddings), dtype='i4'))\n\n    def __call__(\n            self,\n            input_ids: chex.Array,\n            attention_mask: chex.Array,\n            position_ids: chex.Array,\n            deterministic: bool = True,\n            input_embeds: chex.Array = None,\n            init_cache: bool = False,\n            output_attentions: bool = False,\n            output_hidden_states: bool = False,\n            return_dict: bool = True,\n\n    ) -&gt; typing.Union[Tuple[Array, ...], FlaxBaseModelOutput]:\n        \"\"\"\n        The __call__ function is the main function of a Flax model.\n        It takes in input_ids, attention_mask, and position_ids as inputs to the model.\n        The output is a tuple containing: last hidden state (hidden states), all hidden states (if output_hidden_states=True), attentions (if output attentions=True).\n\n\n        :param self: Represent the instance of the class\n        :param input_ids: chex.Array: Pass in the input ids\n        :param attention_mask: chex.Array: Mask out the attention weights for certain tokens\n        :param position_ids: chex.Array: Determine the position of each token in a sequence\n        :param deterministic: bool: Determine whether to use dropout or not\n        :param input_embeds: chex.Array: Pass in the embedding of the input_ids\n        :param init_cache: bool: Initialize the cache for the decoder\n        :param output_attentions: bool: Determine whether to return the attention weights or not\n        :param output_hidden_states: bool: Return all hidden states or just the last one\n        :param return_dict: bool: Return a dictionary of the outputs or not\n        :param : Determine whether the model is in training mode or not\n        :return: A tuple of the hidden states, all hidden states, and attentions\n\n        \"\"\"\n        if input_embeds is None:\n            input_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n        if attention_mask.ndim == 2:\n            b, s = attention_mask.shape\n            attention_mask = attention_mask.reshape(b, 1, 1, s)\n\n        outputs = self.layers(\n            hidden_state=input_embeds,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            freq_cis=self.freq_cis,\n            init_cache=init_cache,\n            output_attentions=output_attentions,\n            deterministic=deterministic,\n            causal_mask=self.causal_mask\n        )\n\n        hidden_states = outputs[0]\n        hidden_states = self.norm(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states = outputs[1] + (hidden_states,)\n            outputs = (hidden_states, all_hidden_states) + outputs[2:]\n        else:\n            outputs = (hidden_states,) + outputs[1:]\n\n        if not return_dict:\n            return tuple(value for value in outputs if value is not None)\n\n        return FlaxBaseModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=outputs[1],\n            attentions=outputs[-1],\n        )\n</code></pre>"},{"location":"lib-python-EasyDel-modules-mistral-modelling_mistral_flax/#lib.python.EasyDel.modules.mistral.modelling_mistral_flax.FlaxMistralModule.__call__","title":"<code>__call__(input_ids, attention_mask, position_ids, deterministic=True, input_embeds=None, init_cache=False, output_attentions=False, output_hidden_states=False, return_dict=True)</code>","text":"<p>The call function is the main function of a Flax model. It takes in input_ids, attention_mask, and position_ids as inputs to the model. The output is a tuple containing: last hidden state (hidden states), all hidden states (if output_hidden_states=True), attentions (if output attentions=True).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass in the input ids</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out the attention weights for certain tokens</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Determine the position of each token in a sequence</p> required <code>deterministic</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>True</code> <code>input_embeds</code> <code>Array</code> <p>chex.Array: Pass in the embedding of the input_ids</p> <code>None</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache for the decoder</p> <code>False</code> <code>output_attentions</code> <code>bool</code> <p>bool: Determine whether to return the attention weights or not</p> <code>False</code> <code>output_hidden_states</code> <code>bool</code> <p>bool: Return all hidden states or just the last one</p> <code>False</code> <code>return_dict</code> <code>bool</code> <p>bool: Return a dictionary of the outputs or not</p> <code>True</code> <code></code> <p>Determine whether the model is in training mode or not</p> required <p>Returns:</p> Type Description <code>Union[Tuple[Array, ...], FlaxBaseModelOutput]</code> <p>A tuple of the hidden states, all hidden states, and attentions</p> Source code in <code>lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids: chex.Array,\n        attention_mask: chex.Array,\n        position_ids: chex.Array,\n        deterministic: bool = True,\n        input_embeds: chex.Array = None,\n        init_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n\n) -&gt; typing.Union[Tuple[Array, ...], FlaxBaseModelOutput]:\n    \"\"\"\n    The __call__ function is the main function of a Flax model.\n    It takes in input_ids, attention_mask, and position_ids as inputs to the model.\n    The output is a tuple containing: last hidden state (hidden states), all hidden states (if output_hidden_states=True), attentions (if output attentions=True).\n\n\n    :param self: Represent the instance of the class\n    :param input_ids: chex.Array: Pass in the input ids\n    :param attention_mask: chex.Array: Mask out the attention weights for certain tokens\n    :param position_ids: chex.Array: Determine the position of each token in a sequence\n    :param deterministic: bool: Determine whether to use dropout or not\n    :param input_embeds: chex.Array: Pass in the embedding of the input_ids\n    :param init_cache: bool: Initialize the cache for the decoder\n    :param output_attentions: bool: Determine whether to return the attention weights or not\n    :param output_hidden_states: bool: Return all hidden states or just the last one\n    :param return_dict: bool: Return a dictionary of the outputs or not\n    :param : Determine whether the model is in training mode or not\n    :return: A tuple of the hidden states, all hidden states, and attentions\n\n    \"\"\"\n    if input_embeds is None:\n        input_embeds = self.embed_tokens(input_ids.astype(\"i4\"))\n    if attention_mask.ndim == 2:\n        b, s = attention_mask.shape\n        attention_mask = attention_mask.reshape(b, 1, 1, s)\n\n    outputs = self.layers(\n        hidden_state=input_embeds,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        freq_cis=self.freq_cis,\n        init_cache=init_cache,\n        output_attentions=output_attentions,\n        deterministic=deterministic,\n        causal_mask=self.causal_mask\n    )\n\n    hidden_states = outputs[0]\n    hidden_states = self.norm(hidden_states)\n\n    if output_hidden_states:\n        all_hidden_states = outputs[1] + (hidden_states,)\n        outputs = (hidden_states, all_hidden_states) + outputs[2:]\n    else:\n        outputs = (hidden_states,) + outputs[1:]\n\n    if not return_dict:\n        return tuple(value for value in outputs if value is not None)\n\n    return FlaxBaseModelOutput(\n        last_hidden_state=hidden_states,\n        hidden_states=outputs[1],\n        attentions=outputs[-1],\n    )\n</code></pre>"},{"location":"lib-python-EasyDel-modules-mistral-modelling_mistral_flax/#lib.python.EasyDel.modules.mistral.modelling_mistral_flax.FlaxMistralPretrainedModel","title":"<code>FlaxMistralPretrainedModel</code>","text":"<p>             Bases: <code>FlaxPreTrainedModel</code></p> Source code in <code>lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>class FlaxMistralPretrainedModel(FlaxPreTrainedModel):\n    config_class = MistralConfig\n    base_model_prefix = 'mistral'\n    module_class: nn.Module = None\n\n    def __init__(self,\n                 config: MistralConfig,\n                 input_shape: Tuple = (1, 1),\n                 seed: int = 0,\n                 dtype: jnp.dtype = jnp.bfloat16,\n                 _do_init: bool = True,\n                 **kwargs\n                 ):\n        module = self.module_class(config=config, dtype=dtype, **kwargs)\n        super().__init__(config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init)\n\n    def init_weights(\n            self,\n            rng: jax.random.PRNGKey,\n            input_shape: Tuple,\n            params: flax.core.FrozenDict = None\n    ) -&gt; flax.core.FrozenDict:\n\n        \"\"\"\n        The init_weights function is used to initialize the weights of a model.\n        It takes in an rng, which is a random number generator key that can be used to generate random numbers.\n        The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n        The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n        :param self: Access variables that belong to the class\n        :param rng: jax.random.PRNGKey: Initialize the weights of the model\n        :param input_shape: Tuple: Initialize the input_ids, attention_mask and position_ids\n        :param params: flax.core.FrozenDict: Pass in the parameters of a pre-trained model\n        :return: A frozendict of parameters\n\n        \"\"\"\n        input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n        params_rng, dropout_rng = jax.random.split(rng)\n        rng_s = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n        if self.config.add_cross_attention:\n            encoder_hidden_states = jnp.zeros(input_shape + (self.config.hidden_size,))\n            encoder_attention_mask = attention_mask\n            module_init_outputs = self.module.init(\n                rng_s,\n                input_ids,\n                attention_mask,\n                position_ids,\n                encoder_hidden_states,\n                encoder_attention_mask,\n                return_dict=False,\n            )\n        else:\n            module_init_outputs = self.module.init(rng_s, input_ids, attention_mask, position_ids, return_dict=False)\n\n        random_params = module_init_outputs[\"params\"]\n\n        if params is not None:\n            random_params = flatten_dict(unfreeze(random_params))\n            params = flatten_dict(unfreeze(params))\n            for missing_key in self._missing_keys:\n                params[missing_key] = random_params[missing_key]\n            self._missing_keys = set()\n            return freeze(unflatten_dict(params))\n        else:\n            return random_params\n\n    def init_cache(self, batch_size, max_length):\n\n        input_ids = jnp.ones((batch_size, max_length))\n        attention_mask = jnp.ones_like(input_ids)\n        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape)\n\n        init_variables = self.module.init(\n            jax.random.PRNGKey(0), input_ids, attention_mask, position_ids, return_dict=False, init_cache=True\n        )\n        return init_variables[\"cache\"]\n\n    def __call__(\n            self,\n            input_ids,\n            attention_mask=None,\n            position_ids=None,\n            params: dict = None,\n            past_key_values: dict = None,\n            dropout_rng: jax.random.PRNGKey = None,\n            train: bool = False,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n            add_params_field: bool = False\n    ):\n        \"\"\"\n        The __call__ function is the main function of a JAX module.\n        It takes as input:\n        - The parameters of the model (self.params)\n        - The inputs to the model (input_ids, attention_mask, position_ids)\n        - Whether we are training (train=True/False) and whether we want to return all hidden states and\n        attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n        :param self: Represent the instance of the class\n        :param input_ids: Pass the input sequence to the model\n        :param attention_mask: Mask out the padding tokens\n        :param position_ids: Specify the position of each token in the sequence\n        :param params: dict: Pass in the parameters of the model\n        :param past_key_values: dict: Pass the past key values to the model\n        :param dropout_rng: jax.random.PRNGKey: Pass in a random number generator key to the model\n        :param train: bool: Determine whether to use dropout or not\n        :param output_attentions: Optional[bool]: Determine whether to return the attention weights\n        :param output_hidden_states: Optional[bool]: Determine whether to return the hidden states of all layers\n        :param return_dict: Optional[bool]: Return a dictionary of the outputs\n        :param add_params_field: bool: Add a params field to the inputs dictionary\n        :return: A tuple of (last_hidden_state, past_key_values)\n\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n        batch_size, sequence_length = input_ids.shape\n\n        if position_ids is None:\n            if past_key_values is not None:\n                raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n            position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n        if attention_mask is None:\n            attention_mask = jnp.ones((batch_size, sequence_length))\n\n        rng_s = {}\n        if dropout_rng is not None:\n            rng_s[\"dropout\"] = dropout_rng\n\n        inputs = {\"params\": params or self.params} if add_params_field else params or self.params\n        rng_s['params'] = jax.random.key(0)\n        if past_key_values:\n            inputs[\"cache\"] = past_key_values\n            mutable = [\"cache\"]\n        else:\n            mutable = False\n\n        outputs = self.module.apply(\n            inputs,\n            jnp.array(input_ids, dtype=\"i4\"),\n            jnp.array(attention_mask, dtype=\"i4\"),\n            jnp.array(position_ids, dtype=\"i4\"),\n            not train,\n            None,\n            False,\n            output_attentions,\n            output_hidden_states,\n            return_dict,\n            rngs=rng_s,\n            mutable=mutable,\n        )\n\n        if past_key_values is not None and return_dict:\n            outputs, past_key_values = outputs\n            outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n            return outputs\n        elif past_key_values is not None and not return_dict:\n            outputs, past_key_values = outputs\n            outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n        return outputs\n</code></pre>"},{"location":"lib-python-EasyDel-modules-mistral-modelling_mistral_flax/#lib.python.EasyDel.modules.mistral.modelling_mistral_flax.FlaxMistralPretrainedModel.__call__","title":"<code>__call__(input_ids, attention_mask=None, position_ids=None, params=None, past_key_values=None, dropout_rng=None, train=False, output_attentions=None, output_hidden_states=None, return_dict=None, add_params_field=False)</code>","text":"<p>The call function is the main function of a JAX module. It takes as input: - The parameters of the model (self.params) - The inputs to the model (input_ids, attention_mask, position_ids) - Whether we are training (train=True/False) and whether we want to return all hidden states and attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>input_ids</code> <p>Pass the input sequence to the model</p> required <code>attention_mask</code> <p>Mask out the padding tokens</p> <code>None</code> <code>position_ids</code> <p>Specify the position of each token in the sequence</p> <code>None</code> <code>params</code> <code>dict</code> <p>dict: Pass in the parameters of the model</p> <code>None</code> <code>past_key_values</code> <code>dict</code> <p>dict: Pass the past key values to the model</p> <code>None</code> <code>dropout_rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Pass in a random number generator key to the model</p> <code>None</code> <code>train</code> <code>bool</code> <p>bool: Determine whether to use dropout or not</p> <code>False</code> <code>output_attentions</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the attention weights</p> <code>None</code> <code>output_hidden_states</code> <code>Optional[bool]</code> <p>Optional[bool]: Determine whether to return the hidden states of all layers</p> <code>None</code> <code>return_dict</code> <code>Optional[bool]</code> <p>Optional[bool]: Return a dictionary of the outputs</p> <code>None</code> <code>add_params_field</code> <code>bool</code> <p>bool: Add a params field to the inputs dictionary</p> <code>False</code> <p>Returns:</p> Type Description <p>A tuple of (last_hidden_state, past_key_values)</p> Source code in <code>lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>def __call__(\n        self,\n        input_ids,\n        attention_mask=None,\n        position_ids=None,\n        params: dict = None,\n        past_key_values: dict = None,\n        dropout_rng: jax.random.PRNGKey = None,\n        train: bool = False,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        add_params_field: bool = False\n):\n    \"\"\"\n    The __call__ function is the main function of a JAX module.\n    It takes as input:\n    - The parameters of the model (self.params)\n    - The inputs to the model (input_ids, attention_mask, position_ids)\n    - Whether we are training (train=True/False) and whether we want to return all hidden states and\n    attentions weights at each layer in addition to just the last layer output (output_hidden_states=True/False).\n\n    :param self: Represent the instance of the class\n    :param input_ids: Pass the input sequence to the model\n    :param attention_mask: Mask out the padding tokens\n    :param position_ids: Specify the position of each token in the sequence\n    :param params: dict: Pass in the parameters of the model\n    :param past_key_values: dict: Pass the past key values to the model\n    :param dropout_rng: jax.random.PRNGKey: Pass in a random number generator key to the model\n    :param train: bool: Determine whether to use dropout or not\n    :param output_attentions: Optional[bool]: Determine whether to return the attention weights\n    :param output_hidden_states: Optional[bool]: Determine whether to return the hidden states of all layers\n    :param return_dict: Optional[bool]: Return a dictionary of the outputs\n    :param add_params_field: bool: Add a params field to the inputs dictionary\n    :return: A tuple of (last_hidden_state, past_key_values)\n\n    \"\"\"\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.return_dict\n\n    batch_size, sequence_length = input_ids.shape\n\n    if position_ids is None:\n        if past_key_values is not None:\n            raise ValueError(\"Make sure to provide `position_ids` when passing `past_key_values`.\")\n\n        position_ids = jnp.broadcast_to(jnp.arange(sequence_length)[None, :], (batch_size, sequence_length))\n\n    if attention_mask is None:\n        attention_mask = jnp.ones((batch_size, sequence_length))\n\n    rng_s = {}\n    if dropout_rng is not None:\n        rng_s[\"dropout\"] = dropout_rng\n\n    inputs = {\"params\": params or self.params} if add_params_field else params or self.params\n    rng_s['params'] = jax.random.key(0)\n    if past_key_values:\n        inputs[\"cache\"] = past_key_values\n        mutable = [\"cache\"]\n    else:\n        mutable = False\n\n    outputs = self.module.apply(\n        inputs,\n        jnp.array(input_ids, dtype=\"i4\"),\n        jnp.array(attention_mask, dtype=\"i4\"),\n        jnp.array(position_ids, dtype=\"i4\"),\n        not train,\n        None,\n        False,\n        output_attentions,\n        output_hidden_states,\n        return_dict,\n        rngs=rng_s,\n        mutable=mutable,\n    )\n\n    if past_key_values is not None and return_dict:\n        outputs, past_key_values = outputs\n        outputs[\"past_key_values\"] = unfreeze(past_key_values[\"cache\"])\n        return outputs\n    elif past_key_values is not None and not return_dict:\n        outputs, past_key_values = outputs\n        outputs = outputs[:1] + (unfreeze(past_key_values[\"cache\"]),) + outputs[1:]\n\n    return outputs\n</code></pre>"},{"location":"lib-python-EasyDel-modules-mistral-modelling_mistral_flax/#lib.python.EasyDel.modules.mistral.modelling_mistral_flax.FlaxMistralPretrainedModel.init_weights","title":"<code>init_weights(rng, input_shape, params=None)</code>","text":"<p>The init_weights function is used to initialize the weights of a model. It takes in an rng, which is a random number generator key that can be used to generate random numbers. The input_shape parameter specifies the shape of the inputs that will be fed into this model. The params parameter allows you to pass in pre-trained weights for your model, if you have them available.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>rng</code> <code>PRNGKey</code> <p>jax.random.PRNGKey: Initialize the weights of the model</p> required <code>input_shape</code> <code>Tuple</code> <p>Tuple: Initialize the input_ids, attention_mask and position_ids</p> required <code>params</code> <code>FrozenDict</code> <p>flax.core.FrozenDict: Pass in the parameters of a pre-trained model</p> <code>None</code> <p>Returns:</p> Type Description <code>FrozenDict</code> <p>A frozendict of parameters</p> Source code in <code>lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>def init_weights(\n        self,\n        rng: jax.random.PRNGKey,\n        input_shape: Tuple,\n        params: flax.core.FrozenDict = None\n) -&gt; flax.core.FrozenDict:\n\n    \"\"\"\n    The init_weights function is used to initialize the weights of a model.\n    It takes in an rng, which is a random number generator key that can be used to generate random numbers.\n    The input_shape parameter specifies the shape of the inputs that will be fed into this model.\n    The params parameter allows you to pass in pre-trained weights for your model, if you have them available.\n\n    :param self: Access variables that belong to the class\n    :param rng: jax.random.PRNGKey: Initialize the weights of the model\n    :param input_shape: Tuple: Initialize the input_ids, attention_mask and position_ids\n    :param params: flax.core.FrozenDict: Pass in the parameters of a pre-trained model\n    :return: A frozendict of parameters\n\n    \"\"\"\n    input_ids = jnp.zeros(input_shape, dtype=\"i4\")\n    attention_mask = jnp.ones_like(input_ids)\n    position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape)\n    params_rng, dropout_rng = jax.random.split(rng)\n    rng_s = {\"params\": params_rng, \"dropout\": dropout_rng}\n\n    if self.config.add_cross_attention:\n        encoder_hidden_states = jnp.zeros(input_shape + (self.config.hidden_size,))\n        encoder_attention_mask = attention_mask\n        module_init_outputs = self.module.init(\n            rng_s,\n            input_ids,\n            attention_mask,\n            position_ids,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            return_dict=False,\n        )\n    else:\n        module_init_outputs = self.module.init(rng_s, input_ids, attention_mask, position_ids, return_dict=False)\n\n    random_params = module_init_outputs[\"params\"]\n\n    if params is not None:\n        random_params = flatten_dict(unfreeze(random_params))\n        params = flatten_dict(unfreeze(params))\n        for missing_key in self._missing_keys:\n            params[missing_key] = random_params[missing_key]\n        self._missing_keys = set()\n        return freeze(unflatten_dict(params))\n    else:\n        return random_params\n</code></pre>"},{"location":"lib-python-EasyDel-modules-mistral-modelling_mistral_flax/#lib.python.EasyDel.modules.mistral.modelling_mistral_flax.MistralConfig","title":"<code>MistralConfig</code>","text":"<p>             Bases: <code>PretrainedConfig</code>, <code>JaxBaseClassModel</code></p> Source code in <code>lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>class MistralConfig(PretrainedConfig, JaxBaseClassModel):\n    def __init__(\n            self,\n            vocab_size=32000,\n            hidden_size=4096,\n            intermediate_size=14336,\n            num_hidden_layers=32,\n            num_attention_heads=32,\n            num_key_value_heads=8,\n            hidden_act=\"silu\",\n            max_position_embeddings=4096 * 32,\n            initializer_range=0.02,\n            rms_norm_eps=1e-6,\n            use_cache=True,\n            pad_token_id=None,\n            bos_token_id=1,\n            eos_token_id=2,\n            tie_word_embeddings=False,\n            rope_theta=10000.0,\n            sliding_window=4096,\n            gradient_checkpointing: str = 'nothing_saveable',\n            use_pjit_attention_force: bool = False,\n            use_flash_attention: bool = False,\n            use_sacn_mlp: bool = False,\n            flash_attn_query_chunk_size: int = 1024,\n            flash_attn_key_chunk_size: int = 1024,\n            scan_mlp_chunk_size: int = 1024,\n            number_rep_kv: int = 1,\n            attn_pdrop: float = 0.0,\n            c_max_position_embeddings: int = 4096,\n            freq_max_position_embeddings: int = 4096,\n            bits: Optional[int] = None,\n            axis_dims: Sequence[int] = (1, -1, 1, 1),\n            axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"mp\"),\n            **kwargs,\n    ):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It allows the class to initialize the attributes of a class.\n        The self parameter is a reference to the current instance of the class, and is used to access variables that belong to the class.\n\n        :param self: Represent the instance of the class\n        :param vocab_size: Define the size of the vocabulary\n        :param hidden_size: Determine the size of the embedding layers\n        :param intermediate_size: Define the size of the intermediate layer in each transformer block\n        :param num_hidden_layers: Determine the number of layers in the encoder and decoder\n        :param num_attention_heads: Determine the number of attention heads in each layer\n        :param num_key_value_heads: Specify the number of heads for key and value\n        :param hidden_act: Specify the activation function used in the hidden layers\n        :param max_position_embeddings: Set the maximum length of the sequence\n        :param initializer_range: Initialize the weights of the model\n        :param rms_norm_eps: Avoid division by zero in the rms normalization\n        :param use_cache: Determine whether to use the cache in the decoder\n        :param pad_token_id: Specify the token id of the padding token\n        :param bos_token_id: Specify the beginning of sentence token id\n        :param eos_token_id: Specify the end of sentence token\n        :param tie_word_embeddings: Tie the word embeddings and the output layer\n        :param rope_theta: Control the number of tokens in a rope\n        :param sliding_window: Control the number of tokens that are processed in parallel\n        :param gradient_checkpointing: str: Specify whether to use gradient checkpointing\n        :param use_pjit_attention_force: bool: Force the use of pjit attention\n        :param use_flash_attention: bool: Enable the flash attention mechanism\n        :param use_sacn_mlp: bool: Determine whether or not to use the scan_mlp function\n        :param flash_attn_query_chunk_size: int: Determine the number of rows in each chunk\n        :param flash_attn_key_chunk_size: int: Control the size of chunks that are used for the key matrix in flash attention\n        :param scan_mlp_chunk_size: int: Specify the chunk size of the scan mlp\n        :param number_rep_kv: int: Specify the number of times to repeat the key and value vectors\n        :param attn_pdrop: float: Set the dropout rate for the attention layer\n        :param c_max_position_embeddings: int: Set the maximum number of tokens in a sequence\n        :param freq_max_position_embeddings: int: Set the maximum number of frequency bins that can be used in the model\n        :param bits: Optional[int]: Specify the number of bits used for quantization\n        :param axis_dims: Sequence[int]: Specify the dimension of each axis\n        :param axis_names: Sequence[str]: Specify the names of each axis in the tensor\n        :param &amp;quot;fsdp&amp;quot;: Specify the frequency dimension of the input\n        :param &amp;quot;tp&amp;quot;: Determine the number of time-steps in the input sequence\n        :param &amp;quot;mp&amp;quot;): Define the maximum position embeddings\n        :param **kwargs: Pass a variable number of keyword arguments to a function\n        :param : Define the number of layers in the model\n        :return: An instance of the class\n\n        \"\"\"\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.sliding_window = sliding_window\n        self.bits = bits\n        # for backward compatibility\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.use_flash_attention = use_flash_attention\n        self.number_rep_kv = number_rep_kv\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.use_sacn_mlp = use_sacn_mlp\n        self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n        self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.attn_pdrop = attn_pdrop\n        self.c_max_position_embeddings = c_max_position_embeddings\n        self.freq_max_position_embeddings = freq_max_position_embeddings\n\n        super().__init__(\n            axis_names=axis_names,\n            axis_dims=axis_dims,\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n    @staticmethod\n    def get_partition_rules(fully_fsdp: bool = True):\n        \"\"\"\n        The get_partition_rules function is used to define the partitioning scheme for a model.\n        It returns a list of tuples, where each tuple contains two elements:\n          1) A regex string that matches the name of one or more parameters in the model.\n          2) A PartitionScheme object that defines how those parameters should be partitioned.\n\n        :param fully_fsdp: bool: Determine whether to use the fully_fsdp partitioning scheme or not\n        :return: A list of tuples\n\n        \"\"\"\n        return (\n\n            (\"model/embed_tokens/embedding\", PS(\"tp\", (\"fsdp\", \"mp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PS((\"fsdp\", \"mp\"), \"dp\")),\n            (\"self_attn/o_proj/kernel\", PS(\"tp\", (\"fsdp\", \"mp\"))),\n\n            (\"mlp/gate_proj/kernel\", PS((\"fsdp\", \"mp\"), \"dp\")),\n            (\"mlp/down_proj/kernel\", PS(\"tp\", (\"fsdp\", \"mp\"))),\n            (\"mlp/up_proj/kernel\", PS((\"fsdp\", \"mp\"), \"dp\")),\n\n            (\"input_layernorm/kernel\", PS(None)),\n            (\"post_attention_layernorm/kernel\", PS(None)),\n\n            (\"model/norm/kernel\", PS(None)),\n            (\"lm_head/kernel\", PS((\"fsdp\", \"mp\"), \"dp\")),\n            ('.*', PS(None)),\n        ) if not fully_fsdp else (\n\n            (\"model/embed_tokens/embedding\", PS((\"fsdp\", \"mp\"))),\n\n            (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PS((\"fsdp\", \"mp\"))),\n            (\"self_attn/o_proj/kernel\", PS((\"fsdp\", \"mp\"))),\n\n            (\"mlp/gate_proj/kernel\", PS((\"fsdp\", \"mp\"))),\n            (\"mlp/down_proj/kernel\", PS((\"fsdp\", \"mp\"))),\n            (\"mlp/up_proj/kernel\", PS((\"fsdp\", \"mp\"))),\n\n            (\"input_layernorm/kernel\", PS(None)),\n            (\"post_attention_layernorm/kernel\", PS(None)),\n\n            (\"model/norm/kernel\", PS(None)),\n            (\"lm_head/kernel\", PS((\"fsdp\", \"mp\"))),\n            ('.*', PS('fsdp')),\n        )\n\n    def add_jax_args(self,\n                     gradient_checkpointing: str = 'nothing_saveable',\n                     use_pjit_attention_force: bool = False,\n                     use_flash_attention: bool = False,\n                     use_sacn_mlp: bool = False,\n                     flash_attn_query_chunk_size: int = 1024,\n                     flash_attn_key_chunk_size: int = 1024,\n                     scan_mlp_chunk_size: int = 1024,\n                     number_rep_kv: int = 1,\n                     attn_pdrop: float = 0.0,\n                     c_max_position_embeddings: int = 4096,\n                     freq_max_position_embeddings: int = None,\n                     bits: Optional[int] = None,\n                     axis_dims: Sequence[int] = (1, -1, 1, 1),\n                     axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"mp\"),\n                     q_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n                     k_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n                     v_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n                     b_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\", None),\n                     a_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n                     backend: Optional[str] = None,\n                     **kwargs,\n                     ):\n        \"\"\"\n        The add_jax_args function adds the following arguments to the model:\n\n        :param self: Bind the attributes and methods of a class to an instance of that class\n        :param gradient_checkpointing: str: Determine whether or not to use gradient checkpointing\n        :param use_pjit_attention_force: bool: Determine whether to use the pjit_attention_force function\n        :param use_flash_attention: bool: Determine if the flash attention module is used or not\n        :param use_sacn_mlp: bool: Determine whether to use the scan_mlp function or not\n        :param flash_attn_query_chunk_size: int: Specify the number of tokens that will be processed at a time\n        :param flash_attn_key_chunk_size: int: Chunk the keys for flash attention\n        :param scan_mlp_chunk_size: int: Chunk the input to the mlp\n        :param number_rep_kv: int: Control the number of times that the key and value vectors are repeated\n        :param attn_pdrop: float: Set the dropout rate for the attention layer\n        :param c_max_position_embeddings: int: Set the maximum number of positional embeddings for the causal axis\n        :param freq_max_position_embeddings: int: Set the maximum length of the frequency axis\n        :param bits: Optional[int]: Specify the number of bits to use for quantization\n        :param axis_dims: Sequence[int]: Specify the dimensions of each axis in the tensor\n        :param axis_names: Sequence[str]: Name the axes of the tensors\n        :param axis_dims: Sequence[int]: Specify the dimension of each axis\n        :param axis_names: Sequence[str]: Name the axes of the tensor\n        :param q_ps: jax.sharding.PartitionSpec: Specify the partitioning of the query tensor\n        :param k_ps: jax.sharding.PartitionSpec: Partition the key matrix\n        :param v_ps: jax.sharding.PartitionSpec: Specify the partitioning of the value tensor\n        :param b_ps: jax.sharding.PartitionSpec: Specify the Attention Bias partition spec\n        :param a_ps: jax.sharding.PartitionSpec: Specify the partitioning of the attention weights\n        :param backend: typing.Optional[str]: backend to use for model\n        :param : Enable gradient checkpointing\n        :return: A tuple of the following:\n\n        \"\"\"\n        self.use_flash_attention = use_flash_attention\n        self.number_rep_kv = number_rep_kv\n        self.gradient_checkpointing = gradient_checkpointing\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.use_sacn_mlp = use_sacn_mlp\n        self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n        self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n        self.scan_mlp_chunk_size = scan_mlp_chunk_size\n        self.attn_pdrop = attn_pdrop\n        self.c_max_position_embeddings = c_max_position_embeddings\n        self.freq_max_position_embeddings = freq_max_position_embeddings\n        self.bits = bits\n        self.axis_names = axis_names\n        self.axis_dims = axis_dims\n        self.q_ps = q_ps\n        self.k_ps = k_ps\n        self.v_ps = v_ps\n        self.b_ps = b_ps\n        self.a_ps = a_ps\n        self.backend = backend\n\n    @staticmethod\n    def get_weight_decay_exclusions():\n        return tuple()\n\n    @staticmethod\n    def rng_keys():\n        return 'params', 'dropout', 'fcm'\n</code></pre>"},{"location":"lib-python-EasyDel-modules-mistral-modelling_mistral_flax/#lib.python.EasyDel.modules.mistral.modelling_mistral_flax.MistralConfig.__init__","title":"<code>__init__(vocab_size=32000, hidden_size=4096, intermediate_size=14336, num_hidden_layers=32, num_attention_heads=32, num_key_value_heads=8, hidden_act='silu', max_position_embeddings=4096 * 32, initializer_range=0.02, rms_norm_eps=1e-06, use_cache=True, pad_token_id=None, bos_token_id=1, eos_token_id=2, tie_word_embeddings=False, rope_theta=10000.0, sliding_window=4096, gradient_checkpointing='nothing_saveable', use_pjit_attention_force=False, use_flash_attention=False, use_sacn_mlp=False, flash_attn_query_chunk_size=1024, flash_attn_key_chunk_size=1024, scan_mlp_chunk_size=1024, number_rep_kv=1, attn_pdrop=0.0, c_max_position_embeddings=4096, freq_max_position_embeddings=4096, bits=None, axis_dims=(1, -1, 1, 1), axis_names=('dp', 'fsdp', 'tp', 'mp'), **kwargs)</code>","text":"<p>The init function is called when the class is instantiated. It allows the class to initialize the attributes of a class. The self parameter is a reference to the current instance of the class, and is used to access variables that belong to the class.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>vocab_size</code> <p>Define the size of the vocabulary</p> <code>32000</code> <code>hidden_size</code> <p>Determine the size of the embedding layers</p> <code>4096</code> <code>intermediate_size</code> <p>Define the size of the intermediate layer in each transformer block</p> <code>14336</code> <code>num_hidden_layers</code> <p>Determine the number of layers in the encoder and decoder</p> <code>32</code> <code>num_attention_heads</code> <p>Determine the number of attention heads in each layer</p> <code>32</code> <code>num_key_value_heads</code> <p>Specify the number of heads for key and value</p> <code>8</code> <code>hidden_act</code> <p>Specify the activation function used in the hidden layers</p> <code>'silu'</code> <code>max_position_embeddings</code> <p>Set the maximum length of the sequence</p> <code>4096 * 32</code> <code>initializer_range</code> <p>Initialize the weights of the model</p> <code>0.02</code> <code>rms_norm_eps</code> <p>Avoid division by zero in the rms normalization</p> <code>1e-06</code> <code>use_cache</code> <p>Determine whether to use the cache in the decoder</p> <code>True</code> <code>pad_token_id</code> <p>Specify the token id of the padding token</p> <code>None</code> <code>bos_token_id</code> <p>Specify the beginning of sentence token id</p> <code>1</code> <code>eos_token_id</code> <p>Specify the end of sentence token</p> <code>2</code> <code>tie_word_embeddings</code> <p>Tie the word embeddings and the output layer</p> <code>False</code> <code>rope_theta</code> <p>Control the number of tokens in a rope</p> <code>10000.0</code> <code>sliding_window</code> <p>Control the number of tokens that are processed in parallel</p> <code>4096</code> <code>gradient_checkpointing</code> <code>str</code> <p>str: Specify whether to use gradient checkpointing</p> <code>'nothing_saveable'</code> <code>use_pjit_attention_force</code> <code>bool</code> <p>bool: Force the use of pjit attention</p> <code>False</code> <code>use_flash_attention</code> <code>bool</code> <p>bool: Enable the flash attention mechanism</p> <code>False</code> <code>use_sacn_mlp</code> <code>bool</code> <p>bool: Determine whether or not to use the scan_mlp function</p> <code>False</code> <code>flash_attn_query_chunk_size</code> <code>int</code> <p>int: Determine the number of rows in each chunk</p> <code>1024</code> <code>flash_attn_key_chunk_size</code> <code>int</code> <p>int: Control the size of chunks that are used for the key matrix in flash attention</p> <code>1024</code> <code>scan_mlp_chunk_size</code> <code>int</code> <p>int: Specify the chunk size of the scan mlp</p> <code>1024</code> <code>number_rep_kv</code> <code>int</code> <p>int: Specify the number of times to repeat the key and value vectors</p> <code>1</code> <code>attn_pdrop</code> <code>float</code> <p>float: Set the dropout rate for the attention layer</p> <code>0.0</code> <code>c_max_position_embeddings</code> <code>int</code> <p>int: Set the maximum number of tokens in a sequence</p> <code>4096</code> <code>freq_max_position_embeddings</code> <code>int</code> <p>int: Set the maximum number of frequency bins that can be used in the model</p> <code>4096</code> <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Specify the number of bits used for quantization</p> <code>None</code> <code>axis_dims</code> <code>Sequence[int]</code> <p>Sequence[int]: Specify the dimension of each axis</p> <code>(1, -1, 1, 1)</code> <code>axis_names</code> <code>Sequence[str]</code> <p>Sequence[str]: Specify the names of each axis in the tensor</p> <code>('dp', 'fsdp', 'tp', 'mp')</code> <code>&amp;quot;fsdp&amp;quot;</code> <p>Specify the frequency dimension of the input</p> required <code>&amp;quot;tp&amp;quot;</code> <p>Determine the number of time-steps in the input sequence</p> required <code>&amp;quot;mp&amp;quot;)</code> <p>Define the maximum position embeddings</p> required <code>**kwargs</code> <p>Pass a variable number of keyword arguments to a function</p> <code>{}</code> <code></code> <p>Define the number of layers in the model</p> required <p>Returns:</p> Type Description <p>An instance of the class</p> Source code in <code>lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>def __init__(\n        self,\n        vocab_size=32000,\n        hidden_size=4096,\n        intermediate_size=14336,\n        num_hidden_layers=32,\n        num_attention_heads=32,\n        num_key_value_heads=8,\n        hidden_act=\"silu\",\n        max_position_embeddings=4096 * 32,\n        initializer_range=0.02,\n        rms_norm_eps=1e-6,\n        use_cache=True,\n        pad_token_id=None,\n        bos_token_id=1,\n        eos_token_id=2,\n        tie_word_embeddings=False,\n        rope_theta=10000.0,\n        sliding_window=4096,\n        gradient_checkpointing: str = 'nothing_saveable',\n        use_pjit_attention_force: bool = False,\n        use_flash_attention: bool = False,\n        use_sacn_mlp: bool = False,\n        flash_attn_query_chunk_size: int = 1024,\n        flash_attn_key_chunk_size: int = 1024,\n        scan_mlp_chunk_size: int = 1024,\n        number_rep_kv: int = 1,\n        attn_pdrop: float = 0.0,\n        c_max_position_embeddings: int = 4096,\n        freq_max_position_embeddings: int = 4096,\n        bits: Optional[int] = None,\n        axis_dims: Sequence[int] = (1, -1, 1, 1),\n        axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"mp\"),\n        **kwargs,\n):\n    \"\"\"\n    The __init__ function is called when the class is instantiated.\n    It allows the class to initialize the attributes of a class.\n    The self parameter is a reference to the current instance of the class, and is used to access variables that belong to the class.\n\n    :param self: Represent the instance of the class\n    :param vocab_size: Define the size of the vocabulary\n    :param hidden_size: Determine the size of the embedding layers\n    :param intermediate_size: Define the size of the intermediate layer in each transformer block\n    :param num_hidden_layers: Determine the number of layers in the encoder and decoder\n    :param num_attention_heads: Determine the number of attention heads in each layer\n    :param num_key_value_heads: Specify the number of heads for key and value\n    :param hidden_act: Specify the activation function used in the hidden layers\n    :param max_position_embeddings: Set the maximum length of the sequence\n    :param initializer_range: Initialize the weights of the model\n    :param rms_norm_eps: Avoid division by zero in the rms normalization\n    :param use_cache: Determine whether to use the cache in the decoder\n    :param pad_token_id: Specify the token id of the padding token\n    :param bos_token_id: Specify the beginning of sentence token id\n    :param eos_token_id: Specify the end of sentence token\n    :param tie_word_embeddings: Tie the word embeddings and the output layer\n    :param rope_theta: Control the number of tokens in a rope\n    :param sliding_window: Control the number of tokens that are processed in parallel\n    :param gradient_checkpointing: str: Specify whether to use gradient checkpointing\n    :param use_pjit_attention_force: bool: Force the use of pjit attention\n    :param use_flash_attention: bool: Enable the flash attention mechanism\n    :param use_sacn_mlp: bool: Determine whether or not to use the scan_mlp function\n    :param flash_attn_query_chunk_size: int: Determine the number of rows in each chunk\n    :param flash_attn_key_chunk_size: int: Control the size of chunks that are used for the key matrix in flash attention\n    :param scan_mlp_chunk_size: int: Specify the chunk size of the scan mlp\n    :param number_rep_kv: int: Specify the number of times to repeat the key and value vectors\n    :param attn_pdrop: float: Set the dropout rate for the attention layer\n    :param c_max_position_embeddings: int: Set the maximum number of tokens in a sequence\n    :param freq_max_position_embeddings: int: Set the maximum number of frequency bins that can be used in the model\n    :param bits: Optional[int]: Specify the number of bits used for quantization\n    :param axis_dims: Sequence[int]: Specify the dimension of each axis\n    :param axis_names: Sequence[str]: Specify the names of each axis in the tensor\n    :param &amp;quot;fsdp&amp;quot;: Specify the frequency dimension of the input\n    :param &amp;quot;tp&amp;quot;: Determine the number of time-steps in the input sequence\n    :param &amp;quot;mp&amp;quot;): Define the maximum position embeddings\n    :param **kwargs: Pass a variable number of keyword arguments to a function\n    :param : Define the number of layers in the model\n    :return: An instance of the class\n\n    \"\"\"\n    self.vocab_size = vocab_size\n    self.max_position_embeddings = max_position_embeddings\n    self.hidden_size = hidden_size\n    self.intermediate_size = intermediate_size\n    self.num_hidden_layers = num_hidden_layers\n    self.num_attention_heads = num_attention_heads\n    self.sliding_window = sliding_window\n    self.bits = bits\n    # for backward compatibility\n    if num_key_value_heads is None:\n        num_key_value_heads = num_attention_heads\n\n    self.num_key_value_heads = num_key_value_heads\n    self.hidden_act = hidden_act\n    self.initializer_range = initializer_range\n    self.rms_norm_eps = rms_norm_eps\n    self.use_cache = use_cache\n    self.rope_theta = rope_theta\n    self.use_flash_attention = use_flash_attention\n    self.number_rep_kv = number_rep_kv\n    self.gradient_checkpointing = gradient_checkpointing\n    self.use_pjit_attention_force = use_pjit_attention_force\n    self.use_sacn_mlp = use_sacn_mlp\n    self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n    self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n    self.scan_mlp_chunk_size = scan_mlp_chunk_size\n    self.attn_pdrop = attn_pdrop\n    self.c_max_position_embeddings = c_max_position_embeddings\n    self.freq_max_position_embeddings = freq_max_position_embeddings\n\n    super().__init__(\n        axis_names=axis_names,\n        axis_dims=axis_dims,\n        pad_token_id=pad_token_id,\n        bos_token_id=bos_token_id,\n        eos_token_id=eos_token_id,\n        tie_word_embeddings=tie_word_embeddings,\n        **kwargs,\n    )\n</code></pre>"},{"location":"lib-python-EasyDel-modules-mistral-modelling_mistral_flax/#lib.python.EasyDel.modules.mistral.modelling_mistral_flax.MistralConfig.add_jax_args","title":"<code>add_jax_args(gradient_checkpointing='nothing_saveable', use_pjit_attention_force=False, use_flash_attention=False, use_sacn_mlp=False, flash_attn_query_chunk_size=1024, flash_attn_key_chunk_size=1024, scan_mlp_chunk_size=1024, number_rep_kv=1, attn_pdrop=0.0, c_max_position_embeddings=4096, freq_max_position_embeddings=None, bits=None, axis_dims=(1, -1, 1, 1), axis_names=('dp', 'fsdp', 'tp', 'mp'), q_ps=jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None), k_ps=jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None), v_ps=jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None), b_ps=jax.sharding.PartitionSpec(('dp', 'fsdp'), None, 'tp', None), a_ps=jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None), backend=None, **kwargs)</code>","text":"<p>The add_jax_args function adds the following arguments to the model:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Bind the attributes and methods of a class to an instance of that class</p> required <code>gradient_checkpointing</code> <code>str</code> <p>str: Determine whether or not to use gradient checkpointing</p> <code>'nothing_saveable'</code> <code>use_pjit_attention_force</code> <code>bool</code> <p>bool: Determine whether to use the pjit_attention_force function</p> <code>False</code> <code>use_flash_attention</code> <code>bool</code> <p>bool: Determine if the flash attention module is used or not</p> <code>False</code> <code>use_sacn_mlp</code> <code>bool</code> <p>bool: Determine whether to use the scan_mlp function or not</p> <code>False</code> <code>flash_attn_query_chunk_size</code> <code>int</code> <p>int: Specify the number of tokens that will be processed at a time</p> <code>1024</code> <code>flash_attn_key_chunk_size</code> <code>int</code> <p>int: Chunk the keys for flash attention</p> <code>1024</code> <code>scan_mlp_chunk_size</code> <code>int</code> <p>int: Chunk the input to the mlp</p> <code>1024</code> <code>number_rep_kv</code> <code>int</code> <p>int: Control the number of times that the key and value vectors are repeated</p> <code>1</code> <code>attn_pdrop</code> <code>float</code> <p>float: Set the dropout rate for the attention layer</p> <code>0.0</code> <code>c_max_position_embeddings</code> <code>int</code> <p>int: Set the maximum number of positional embeddings for the causal axis</p> <code>4096</code> <code>freq_max_position_embeddings</code> <code>int</code> <p>int: Set the maximum length of the frequency axis</p> <code>None</code> <code>bits</code> <code>Optional[int]</code> <p>Optional[int]: Specify the number of bits to use for quantization</p> <code>None</code> <code>axis_dims</code> <code>Sequence[int]</code> <p>Sequence[int]: Specify the dimensions of each axis in the tensor</p> <code>(1, -1, 1, 1)</code> <code>axis_names</code> <code>Sequence[str]</code> <p>Sequence[str]: Name the axes of the tensors</p> <code>('dp', 'fsdp', 'tp', 'mp')</code> <code>q_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the partitioning of the query tensor</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>k_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Partition the key matrix</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>v_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the partitioning of the value tensor</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>b_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the Attention Bias partition spec</p> <code>PartitionSpec(('dp', 'fsdp'), None, 'tp', None)</code> <code>a_ps</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: Specify the partitioning of the attention weights</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp', 'tp', None)</code> <code>backend</code> <code>Optional[str]</code> <p>typing.Optional[str]: backend to use for model</p> <code>None</code> <code></code> <p>Enable gradient checkpointing</p> required <p>Returns:</p> Type Description <p>A tuple of the following:</p> Source code in <code>lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>def add_jax_args(self,\n                 gradient_checkpointing: str = 'nothing_saveable',\n                 use_pjit_attention_force: bool = False,\n                 use_flash_attention: bool = False,\n                 use_sacn_mlp: bool = False,\n                 flash_attn_query_chunk_size: int = 1024,\n                 flash_attn_key_chunk_size: int = 1024,\n                 scan_mlp_chunk_size: int = 1024,\n                 number_rep_kv: int = 1,\n                 attn_pdrop: float = 0.0,\n                 c_max_position_embeddings: int = 4096,\n                 freq_max_position_embeddings: int = None,\n                 bits: Optional[int] = None,\n                 axis_dims: Sequence[int] = (1, -1, 1, 1),\n                 axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"mp\"),\n                 q_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n                 k_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n                 v_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n                 b_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), None, \"tp\", None),\n                 a_ps: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec((\"dp\", \"fsdp\"), \"mp\", \"tp\", None),\n                 backend: Optional[str] = None,\n                 **kwargs,\n                 ):\n    \"\"\"\n    The add_jax_args function adds the following arguments to the model:\n\n    :param self: Bind the attributes and methods of a class to an instance of that class\n    :param gradient_checkpointing: str: Determine whether or not to use gradient checkpointing\n    :param use_pjit_attention_force: bool: Determine whether to use the pjit_attention_force function\n    :param use_flash_attention: bool: Determine if the flash attention module is used or not\n    :param use_sacn_mlp: bool: Determine whether to use the scan_mlp function or not\n    :param flash_attn_query_chunk_size: int: Specify the number of tokens that will be processed at a time\n    :param flash_attn_key_chunk_size: int: Chunk the keys for flash attention\n    :param scan_mlp_chunk_size: int: Chunk the input to the mlp\n    :param number_rep_kv: int: Control the number of times that the key and value vectors are repeated\n    :param attn_pdrop: float: Set the dropout rate for the attention layer\n    :param c_max_position_embeddings: int: Set the maximum number of positional embeddings for the causal axis\n    :param freq_max_position_embeddings: int: Set the maximum length of the frequency axis\n    :param bits: Optional[int]: Specify the number of bits to use for quantization\n    :param axis_dims: Sequence[int]: Specify the dimensions of each axis in the tensor\n    :param axis_names: Sequence[str]: Name the axes of the tensors\n    :param axis_dims: Sequence[int]: Specify the dimension of each axis\n    :param axis_names: Sequence[str]: Name the axes of the tensor\n    :param q_ps: jax.sharding.PartitionSpec: Specify the partitioning of the query tensor\n    :param k_ps: jax.sharding.PartitionSpec: Partition the key matrix\n    :param v_ps: jax.sharding.PartitionSpec: Specify the partitioning of the value tensor\n    :param b_ps: jax.sharding.PartitionSpec: Specify the Attention Bias partition spec\n    :param a_ps: jax.sharding.PartitionSpec: Specify the partitioning of the attention weights\n    :param backend: typing.Optional[str]: backend to use for model\n    :param : Enable gradient checkpointing\n    :return: A tuple of the following:\n\n    \"\"\"\n    self.use_flash_attention = use_flash_attention\n    self.number_rep_kv = number_rep_kv\n    self.gradient_checkpointing = gradient_checkpointing\n    self.use_pjit_attention_force = use_pjit_attention_force\n    self.use_sacn_mlp = use_sacn_mlp\n    self.flash_attn_query_chunk_size = flash_attn_query_chunk_size\n    self.flash_attn_key_chunk_size = flash_attn_key_chunk_size\n    self.scan_mlp_chunk_size = scan_mlp_chunk_size\n    self.attn_pdrop = attn_pdrop\n    self.c_max_position_embeddings = c_max_position_embeddings\n    self.freq_max_position_embeddings = freq_max_position_embeddings\n    self.bits = bits\n    self.axis_names = axis_names\n    self.axis_dims = axis_dims\n    self.q_ps = q_ps\n    self.k_ps = k_ps\n    self.v_ps = v_ps\n    self.b_ps = b_ps\n    self.a_ps = a_ps\n    self.backend = backend\n</code></pre>"},{"location":"lib-python-EasyDel-modules-mistral-modelling_mistral_flax/#lib.python.EasyDel.modules.mistral.modelling_mistral_flax.MistralConfig.get_partition_rules","title":"<code>get_partition_rules(fully_fsdp=True)</code>  <code>staticmethod</code>","text":"<p>The get_partition_rules function is used to define the partitioning scheme for a model. It returns a list of tuples, where each tuple contains two elements:   1) A regex string that matches the name of one or more parameters in the model.   2) A PartitionScheme object that defines how those parameters should be partitioned.</p> <p>Parameters:</p> Name Type Description Default <code>fully_fsdp</code> <code>bool</code> <p>bool: Determine whether to use the fully_fsdp partitioning scheme or not</p> <code>True</code> <p>Returns:</p> Type Description <p>A list of tuples</p> Source code in <code>lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>@staticmethod\ndef get_partition_rules(fully_fsdp: bool = True):\n    \"\"\"\n    The get_partition_rules function is used to define the partitioning scheme for a model.\n    It returns a list of tuples, where each tuple contains two elements:\n      1) A regex string that matches the name of one or more parameters in the model.\n      2) A PartitionScheme object that defines how those parameters should be partitioned.\n\n    :param fully_fsdp: bool: Determine whether to use the fully_fsdp partitioning scheme or not\n    :return: A list of tuples\n\n    \"\"\"\n    return (\n\n        (\"model/embed_tokens/embedding\", PS(\"tp\", (\"fsdp\", \"mp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PS((\"fsdp\", \"mp\"), \"dp\")),\n        (\"self_attn/o_proj/kernel\", PS(\"tp\", (\"fsdp\", \"mp\"))),\n\n        (\"mlp/gate_proj/kernel\", PS((\"fsdp\", \"mp\"), \"dp\")),\n        (\"mlp/down_proj/kernel\", PS(\"tp\", (\"fsdp\", \"mp\"))),\n        (\"mlp/up_proj/kernel\", PS((\"fsdp\", \"mp\"), \"dp\")),\n\n        (\"input_layernorm/kernel\", PS(None)),\n        (\"post_attention_layernorm/kernel\", PS(None)),\n\n        (\"model/norm/kernel\", PS(None)),\n        (\"lm_head/kernel\", PS((\"fsdp\", \"mp\"), \"dp\")),\n        ('.*', PS(None)),\n    ) if not fully_fsdp else (\n\n        (\"model/embed_tokens/embedding\", PS((\"fsdp\", \"mp\"))),\n\n        (\"self_attn/(q_proj|k_proj|v_proj)/kernel\", PS((\"fsdp\", \"mp\"))),\n        (\"self_attn/o_proj/kernel\", PS((\"fsdp\", \"mp\"))),\n\n        (\"mlp/gate_proj/kernel\", PS((\"fsdp\", \"mp\"))),\n        (\"mlp/down_proj/kernel\", PS((\"fsdp\", \"mp\"))),\n        (\"mlp/up_proj/kernel\", PS((\"fsdp\", \"mp\"))),\n\n        (\"input_layernorm/kernel\", PS(None)),\n        (\"post_attention_layernorm/kernel\", PS(None)),\n\n        (\"model/norm/kernel\", PS(None)),\n        (\"lm_head/kernel\", PS((\"fsdp\", \"mp\"))),\n        ('.*', PS('fsdp')),\n    )\n</code></pre>"},{"location":"lib-python-EasyDel-modules-mistral-modelling_mistral_flax/#lib.python.EasyDel.modules.mistral.modelling_mistral_flax.matmul_4d_loop","title":"<code>matmul_4d_loop(x, y)</code>","text":"<p>Computes the matrix product of two 4D arrays x and y using a loop.</p> Source code in <code>lib/python/EasyDel/modules/mistral/modelling_mistral_flax.py</code> <pre><code>def matmul_4d_loop(x, y):\n    \"\"\"Computes the matrix product of two 4D arrays x and y using a loop.\"\"\"\n    result = jnp.zeros(*x.shape[:-2] + x.shape[-2] + y.shape[-1])\n    for i in range(x.shape[0]):\n        for j in range(y.shape[1]):\n            for key in range(x.shape[2]):\n                for l in range(y.shape[3]):\n                    result[i, j, key, l] += x[i, j, key, :] * y[key, l, :, :]\n    return result\n</code></pre>"},{"location":"lib-python-EasyDel-modules-mosaic_mpt-modelling_mpt_flax/","title":"modules.mosaic_mpt.modelling_mpt_flax","text":""},{"location":"lib-python-EasyDel-modules-mosaic_mpt-modelling_mpt_flax/#lib.python.EasyDel.modules.mosaic_mpt.modelling_mpt_flax.FlaxMptAttention","title":"<code>FlaxMptAttention</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>lib/python/EasyDel/modules/mosaic_mpt/modelling_mpt_flax.py</code> <pre><code>class FlaxMptAttention(nn.Module):\n    config: MptConfig\n    dtype: jnp.dtype = jnp.float32\n    param_dtype: jnp.dtype = jnp.float32\n    precision: Optional[Union[jax.lax.Precision, str]] = None\n\n    def setup(self) -&gt; None:\n\n        if self.config.bits is not None:\n            _dot_general_cls = q_config.fully_quantized(\n                fwd_bits=self.config.bits,\n                bwd_bits=self.config.bits\n            )\n        else:\n            _dot_general_cls = None\n\n        dot_general_cls = q_flax.QDotGeneral(_dot_general_cls)\n        self.w_qkv = nn.Dense(\n            self.config.d_model * 3,\n            kernel_init=jax.nn.initializers.normal(),\n            use_bias=self.config.use_bias,\n            dot_general=dot_general_cls,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision)\n        self.wo = nn.Dense(\n            self.config.d_model,\n            kernel_init=jax.nn.initializers.normal(),\n            use_bias=self.config.use_bias,\n            dtype=self.dtype,\n            param_dtype=self.param_dtype,\n            precision=self.precision,\n            dot_general=dot_general_cls\n        )\n        if self.config.qk_ln:\n            self.q_ln = nn.LayerNorm(use_bias=self.config.use_norm_bias)\n            self.k_ln = nn.LayerNorm(use_bias=self.config.use_norm_bias)\n        self.causal_mask = nn.make_causal_mask(jnp.ones((1, self.config.max_seq_len)))\n\n    @nn.compact\n    def _concatenate_to_cache(self, key, query, value, attention_mask):\n        is_initialized = self.has_variable('cache', 'key')\n        cache_key = self.variable('cache', 'key', jnp.zeros, key.shape, key.dtype)\n        cache_value = self.variable('cache', 'value', jnp.zeros, value.shape, value.dtype)\n        cache_index = self.variable('cache', 'index', lambda: jnp.array(0, dtype=jnp.int32))\n        if is_initialized:\n            *b, s, h, d = cache_key.value.shape\n            cur_index = cache_index.value\n            indices = (0,) * len(b) + (cur_index, 0, 0)\n            key = jax.lax.dynamic_update_slice(cache_key.value, key, indices)\n            value = jax.lax.dynamic_update_slice(cache_value.value, value, indices)\n            cache_value.value = value\n            cache_key.value = key\n            num_updated_vector = query.shape[1]\n            cache_index.value = cache_index.value + num_updated_vector\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(s) &lt; cur_index + num_updated_vector,\n                tuple(b) + (1, num_updated_vector, s),\n            )\n\n            attention_mask = nn.combine_masks(pad_mask, attention_mask)\n        return key, value, attention_mask\n\n    def __call__(self,\n                 hidden_states: chex.Array,\n                 attention_mask: chex.Array,\n                 position_ids: chex.Array,\n                 attn_bias: chex.Array = None,\n                 init_cache: bool = False\n                 ):\n\n        \"\"\"\n        The __call__ function is the main function of a JAX module.\n        It takes in inputs and returns outputs, just like any other Python function.\n        The difference is that __call__ can also take in state (e.g., parameters) from the module itself,\n        and it can update that state as part of its computation.\n\n        :param self: Access variables that belong to the class\n        :param hidden_states: chex.Array: Pass the input to the attention layer\n        :param attention_mask: chex.Array: Mask out certain positions in the sequence\n        :param position_ids: chex.Array: Specify the position of each token in the sequence\n        :param attn_bias: chex.Array: Add a bias to the attention scores\n        :param init_cache: bool: Initialize the cache\n        :return: The output of the attention layer\n\n        \"\"\"\n        inp_shape = hidden_states.shape\n        b, s, ds = inp_shape\n        qkv = self.w_qkv(hidden_states)\n        q, k, v = jnp.split(qkv, 3, -1)\n        if self.config.qk_ln:\n            q = self.q_ln(q)\n            k = self.k_ln(k)\n        if self.config.use_pjit_attention_force:\n            q = with_sharding_constraint(q, PartitionSpec(('dp', 'fsdp'), None, 'mp'))\n            k = with_sharding_constraint(k, PartitionSpec(('dp', 'fsdp'), None, 'mp'))\n            v = with_sharding_constraint(v, PartitionSpec(('dp', 'fsdp'), None, 'mp'))\n        q = rearrange(q, 'b s (h d) -&gt; b s h d', h=self.config.n_heads)\n        k = rearrange(k, 'b s (h d) -&gt; b s h d', h=self.config.n_heads)\n        v = rearrange(v, 'b s (h d) -&gt; b s h d', h=self.config.n_heads)\n        attention_mask = attention_mask.reshape(b, 1, 1, -1)\n        if self.has_variable('cache', 'key') or init_cache:\n            k, v, attention_mask = self._concatenate_to_cache(key=k, value=v, query=q, attention_mask=attention_mask)\n\n        q_l = q.shape[1]\n        k_l = k.shape[1]\n        dropout_rng = None\n        deterministic = False\n        if deterministic:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        if self.config.use_flash_attention and not (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n\n            if attention_mask.ndim == 2:\n                attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n            if attention_mask.shape[1] != self.config.num_attention_heads:\n                attention_mask = attention_mask.repeat(self.config.num_attention_heads, 1, )\n            attention_bias = jax.lax.select(\n                attention_mask &gt; 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n            )\n            attn_weights = None\n            rtp_axis = (0, 2, 1, 3)\n            attn_output = smart_flash_attention(\n                q=jnp.transpose(q, rtp_axis),\n                k=jnp.transpose(k, rtp_axis),\n                v=jnp.transpose(b, rtp_axis),\n                bias=attention_bias + attn_bias,\n                block_q=self.config.flash_attn_query_chunk_size,\n                block_k=self.config.flash_attn_key_chunk_size,\n                block_b=1,\n                num_attention_heads=self.config.num_attention_heads,\n                precision=self.precision,\n                dtype=self.dtype,\n                causal=False,\n                mesh=self.config.jax_mesh(),\n                dropout_rng=dropout_rng,\n                deterministic=deterministic,\n                q_seq_len=q_l,\n                kv_seq_len=k_l,\n                attn_pdrop=self.config.attn_pdrop,\n                head_dims=self.head_dim,\n                force_float32_tpu=True\n            )\n            attn_output = jnp.transpose(attn_output, rtp_axis)\n        else:\n            d = q.shape[-1]\n            attn_output = jnp.einsum('...qhd,...khd-&gt;...hqk', q, k, precision=self.precision) * jax.lax.rsqrt(\n                jnp.asarray(d).astype(v.dtype))\n            if self.config.use_pjit_attention_force:\n                attn_output = with_sharding_constraint(attn_output, PartitionSpec(('dp', 'fsdp'), 'mp', None, None))\n            if attn_bias is not None:\n                attn_output += attn_bias[:, :, :, :attn_output.shape[-1]]\n            mask = jnp.where(self.causal_mask == 1, 0, jnp.finfo(attn_output).min)\n            if attention_mask is not None:\n                attention_mask = jnp.where(\n                    attention_mask == 1,\n                    0,\n                    jnp.finfo(attn_output).min\n                )\n                attn_output += attention_mask\n            attn_output += mask[:, :, :attn_output.shape[-2], :attn_output.shape[-1]]\n            attn_output = nn.softmax(attn_output, -1)\n            attn_output = jnp.einsum('...hqk,...khd-&gt;...qhd', attn_output, v)\n        return self.wo(attn_output.reshape(inp_shape))\n</code></pre>"},{"location":"lib-python-EasyDel-modules-mosaic_mpt-modelling_mpt_flax/#lib.python.EasyDel.modules.mosaic_mpt.modelling_mpt_flax.FlaxMptAttention.__call__","title":"<code>__call__(hidden_states, attention_mask, position_ids, attn_bias=None, init_cache=False)</code>","text":"<p>The call function is the main function of a JAX module. It takes in inputs and returns outputs, just like any other Python function. The difference is that call can also take in state (e.g., parameters) from the module itself, and it can update that state as part of its computation.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>hidden_states</code> <code>Array</code> <p>chex.Array: Pass the input to the attention layer</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask out certain positions in the sequence</p> required <code>position_ids</code> <code>Array</code> <p>chex.Array: Specify the position of each token in the sequence</p> required <code>attn_bias</code> <code>Array</code> <p>chex.Array: Add a bias to the attention scores</p> <code>None</code> <code>init_cache</code> <code>bool</code> <p>bool: Initialize the cache</p> <code>False</code> <p>Returns:</p> Type Description <p>The output of the attention layer</p> Source code in <code>lib/python/EasyDel/modules/mosaic_mpt/modelling_mpt_flax.py</code> <pre><code>def __call__(self,\n             hidden_states: chex.Array,\n             attention_mask: chex.Array,\n             position_ids: chex.Array,\n             attn_bias: chex.Array = None,\n             init_cache: bool = False\n             ):\n\n    \"\"\"\n    The __call__ function is the main function of a JAX module.\n    It takes in inputs and returns outputs, just like any other Python function.\n    The difference is that __call__ can also take in state (e.g., parameters) from the module itself,\n    and it can update that state as part of its computation.\n\n    :param self: Access variables that belong to the class\n    :param hidden_states: chex.Array: Pass the input to the attention layer\n    :param attention_mask: chex.Array: Mask out certain positions in the sequence\n    :param position_ids: chex.Array: Specify the position of each token in the sequence\n    :param attn_bias: chex.Array: Add a bias to the attention scores\n    :param init_cache: bool: Initialize the cache\n    :return: The output of the attention layer\n\n    \"\"\"\n    inp_shape = hidden_states.shape\n    b, s, ds = inp_shape\n    qkv = self.w_qkv(hidden_states)\n    q, k, v = jnp.split(qkv, 3, -1)\n    if self.config.qk_ln:\n        q = self.q_ln(q)\n        k = self.k_ln(k)\n    if self.config.use_pjit_attention_force:\n        q = with_sharding_constraint(q, PartitionSpec(('dp', 'fsdp'), None, 'mp'))\n        k = with_sharding_constraint(k, PartitionSpec(('dp', 'fsdp'), None, 'mp'))\n        v = with_sharding_constraint(v, PartitionSpec(('dp', 'fsdp'), None, 'mp'))\n    q = rearrange(q, 'b s (h d) -&gt; b s h d', h=self.config.n_heads)\n    k = rearrange(k, 'b s (h d) -&gt; b s h d', h=self.config.n_heads)\n    v = rearrange(v, 'b s (h d) -&gt; b s h d', h=self.config.n_heads)\n    attention_mask = attention_mask.reshape(b, 1, 1, -1)\n    if self.has_variable('cache', 'key') or init_cache:\n        k, v, attention_mask = self._concatenate_to_cache(key=k, value=v, query=q, attention_mask=attention_mask)\n\n    q_l = q.shape[1]\n    k_l = k.shape[1]\n    dropout_rng = None\n    deterministic = False\n    if deterministic:\n        dropout_rng = self.make_rng(\"dropout\")\n\n    if self.config.use_flash_attention and not (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n\n        if attention_mask.ndim == 2:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        if attention_mask.shape[1] != self.config.num_attention_heads:\n            attention_mask = attention_mask.repeat(self.config.num_attention_heads, 1, )\n        attention_bias = jax.lax.select(\n            attention_mask &gt; 0,\n            jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n            jnp.full(attention_mask.shape, jnp.finfo(self.dtype).min).astype(self.dtype),\n        )\n        attn_weights = None\n        rtp_axis = (0, 2, 1, 3)\n        attn_output = smart_flash_attention(\n            q=jnp.transpose(q, rtp_axis),\n            k=jnp.transpose(k, rtp_axis),\n            v=jnp.transpose(b, rtp_axis),\n            bias=attention_bias + attn_bias,\n            block_q=self.config.flash_attn_query_chunk_size,\n            block_k=self.config.flash_attn_key_chunk_size,\n            block_b=1,\n            num_attention_heads=self.config.num_attention_heads,\n            precision=self.precision,\n            dtype=self.dtype,\n            causal=False,\n            mesh=self.config.jax_mesh(),\n            dropout_rng=dropout_rng,\n            deterministic=deterministic,\n            q_seq_len=q_l,\n            kv_seq_len=k_l,\n            attn_pdrop=self.config.attn_pdrop,\n            head_dims=self.head_dim,\n            force_float32_tpu=True\n        )\n        attn_output = jnp.transpose(attn_output, rtp_axis)\n    else:\n        d = q.shape[-1]\n        attn_output = jnp.einsum('...qhd,...khd-&gt;...hqk', q, k, precision=self.precision) * jax.lax.rsqrt(\n            jnp.asarray(d).astype(v.dtype))\n        if self.config.use_pjit_attention_force:\n            attn_output = with_sharding_constraint(attn_output, PartitionSpec(('dp', 'fsdp'), 'mp', None, None))\n        if attn_bias is not None:\n            attn_output += attn_bias[:, :, :, :attn_output.shape[-1]]\n        mask = jnp.where(self.causal_mask == 1, 0, jnp.finfo(attn_output).min)\n        if attention_mask is not None:\n            attention_mask = jnp.where(\n                attention_mask == 1,\n                0,\n                jnp.finfo(attn_output).min\n            )\n            attn_output += attention_mask\n        attn_output += mask[:, :, :attn_output.shape[-2], :attn_output.shape[-1]]\n        attn_output = nn.softmax(attn_output, -1)\n        attn_output = jnp.einsum('...hqk,...khd-&gt;...qhd', attn_output, v)\n    return self.wo(attn_output.reshape(inp_shape))\n</code></pre>"},{"location":"lib-python-EasyDel-modules-opt-modelling_opt_flax/","title":"modules.opt.modelling_opt_flax","text":"<p>Flax OPT model.</p>"},{"location":"lib-python-EasyDel-modules-opt-modelling_opt_flax/#lib.python.EasyDel.modules.opt.modelling_opt_flax.FlaxOPTLearnedPositionalEmbedding","title":"<code>FlaxOPTLearnedPositionalEmbedding</code>","text":"<p>             Bases: <code>Embed</code></p> Source code in <code>lib/python/EasyDel/modules/opt/modelling_opt_flax.py</code> <pre><code>class FlaxOPTLearnedPositionalEmbedding(nn.Embed):\n\n    def setup(self):\n        self.offset = 2\n        self.embedding = self.param(\n            \"embedding\", self.embedding_init, (self.num_embeddings + self.offset, self.features), self.param_dtype\n        )\n\n    def __call__(self, positions):\n        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n\n        return super().__call__(positions + self.offset)\n</code></pre>"},{"location":"lib-python-EasyDel-modules-opt-modelling_opt_flax/#lib.python.EasyDel.modules.opt.modelling_opt_flax.FlaxOPTLearnedPositionalEmbedding.__call__","title":"<code>__call__(positions)</code>","text":"<p><code>input_ids_shape</code> is expected to be [bsz x seqlen].</p> Source code in <code>lib/python/EasyDel/modules/opt/modelling_opt_flax.py</code> <pre><code>def __call__(self, positions):\n    \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n\n    return super().__call__(positions + self.offset)\n</code></pre>"},{"location":"lib-python-EasyDel-modules-palm-modelling_palm_flax/","title":"modules.palm.modelling_palm_flax","text":""},{"location":"lib-python-EasyDel-modules-t5-modelling_t5_flax/","title":"modules.t5.modelling_t5_flax","text":"<p>Flax T5 model.</p>"},{"location":"lib-python-EasyDel-modules-t5-modelling_t5_flax/#lib.python.EasyDel.modules.t5.modelling_t5_flax.FlaxT5Attention","title":"<code>FlaxT5Attention</code>","text":"<p>             Bases: <code>Module</code></p> Source code in <code>lib/python/EasyDel/modules/t5/modelling_t5_flax.py</code> <pre><code>class FlaxT5Attention(nn.Module):\n    config: T5Config\n    has_relative_attention_bias: bool = False\n    causal: bool = False\n    dtype: jnp.dtype = jnp.bfloat16  # the dtype of the computation\n\n    def setup(self):\n        self.relative_attention_num_buckets = self.config.relative_attention_num_buckets\n        self.relative_attention_max_distance = self.config.relative_attention_max_distance\n        self.d_model = self.config.d_model\n        self.key_value_proj_dim = self.config.d_kv\n        self.n_heads = self.config.num_heads\n        self.dropout = self.config.dropout_rate\n        self.inner_dim = self.n_heads * self.key_value_proj_dim\n\n        q_init_std = self.config.initializer_factor * ((self.inner_dim * self.key_value_proj_dim) ** -0.5)\n        kv_init_std = self.config.initializer_factor * (self.inner_dim ** -0.5)\n        o_init_std = self.config.initializer_factor * (self.inner_dim ** -0.5)\n\n        self.q = nn.Dense(\n            self.inner_dim,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(q_init_std),\n            dtype=self.dtype,\n        )\n        self.k = nn.Dense(\n            self.inner_dim,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(kv_init_std),\n            dtype=self.dtype,\n        )\n        self.v = nn.Dense(\n            self.inner_dim,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(kv_init_std),\n            dtype=self.dtype,\n        )\n        self.o = nn.Dense(\n            self.d_model,\n            use_bias=False,\n            kernel_init=jax.nn.initializers.normal(o_init_std),\n            dtype=self.dtype,\n        )\n\n        if self.has_relative_attention_bias:\n            self.relative_attention_bias = nn.Embed(\n                self.relative_attention_num_buckets,\n                self.n_heads,\n                embedding_init=jax.nn.initializers.normal(kv_init_std),\n                dtype=self.dtype,\n            )\n\n    @staticmethod\n    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n\n        relative_buckets = 0\n        if bidirectional:\n            num_buckets //= 2\n            relative_buckets += (relative_position &gt; 0) * num_buckets\n            relative_position = jnp.abs(relative_position)\n        else:\n            relative_position = -jnp.clip(relative_position, a_max=0)\n        # now relative_position is in the range [0, inf)\n\n        # half of the buckets are for exact increments in positions\n        max_exact = num_buckets // 2\n        is_small = relative_position &lt; max_exact\n\n        relative_position_if_large = max_exact + (\n                jnp.log(relative_position / max_exact) / jnp.log(max_distance / max_exact) * (num_buckets - max_exact)\n        )\n        relative_position_if_large = jnp.clip(relative_position_if_large, a_max=num_buckets - 1)\n\n        relative_buckets += jnp.where(is_small, relative_position, relative_position_if_large)\n\n        return relative_buckets.astype(\"i4\")\n\n    def compute_bias(self, query_length, key_length):\n        \"\"\"Compute binned relative position bias\"\"\"\n        context_position = jnp.arange(query_length, dtype=\"i4\")[:, None]\n        memory_position = jnp.arange(key_length, dtype=\"i4\")[None, :]\n\n        relative_position = memory_position - context_position\n        relative_position_bucket = self._relative_position_bucket(\n            relative_position,\n            bidirectional=(not self.causal),\n            num_buckets=self.relative_attention_num_buckets,\n            max_distance=self.relative_attention_max_distance,\n        )\n\n        values = self.relative_attention_bias(relative_position_bucket)\n        values = values.transpose((2, 0, 1))[None, :, :, :]\n        return values\n\n    def _split_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.n_heads, self.key_value_proj_dim))\n\n    def _merge_heads(self, hidden_states):\n        return hidden_states.reshape(hidden_states.shape[:2] + (self.inner_dim,))\n\n    @nn.compact\n    def _concatenate_to_cache(self, key, value, query, attention_mask):\n\n        is_initialized = self.has_variable(\"cache\", \"cached_key\")\n        cached_key = self.variable(\"cache\", \"cached_key\", jnp.zeros, key.shape, key.dtype)\n        cached_value = self.variable(\"cache\", \"cached_value\", jnp.zeros, value.shape, value.dtype)\n        cache_index = self.variable(\"cache\", \"cache_index\", lambda: jax.Array(0, dtype=jnp.int32))\n\n        if is_initialized:\n            *batch_dims, max_length, num_heads, depth_per_head = cached_key.value.shape\n            # update key, value caches with our new 1d spatial slices\n            cur_index = cache_index.value\n            indices = (0,) * len(batch_dims) + (cur_index, 0, 0)\n            key = jax.lax.dynamic_update_slice(cached_key.value, key, indices)\n            value = jax.lax.dynamic_update_slice(cached_value.value, value, indices)\n            cached_key.value = key\n            cached_value.value = value\n            num_updated_cache_vectors = query.shape[1]\n            cache_index.value = cache_index.value + num_updated_cache_vectors\n            # causal mask for cached decoder self-attention: our single query position should only attend to those key positions\n            # that have already been generated and cached, not the remaining zero elements.\n            pad_mask = jnp.broadcast_to(\n                jnp.arange(max_length) &lt; cur_index + num_updated_cache_vectors,\n                tuple(batch_dims) + (1, num_updated_cache_vectors, max_length),\n            )\n            attention_mask = combine_masks(pad_mask, attention_mask)\n        return key, value, attention_mask\n\n    def _create_position_bias(\n            self, key_states, query_states, attention_mask, init_cache, seq_length, causal_attention_mask_shift\n    ):\n        cache_is_filled = self.causal and self.has_variable(\"cache\", \"cached_key\") and (not init_cache)\n        key_length = key_states.shape[1]\n        query_length = key_length if cache_is_filled else query_states.shape[1]\n\n        if self.has_relative_attention_bias:\n            position_bias = self.compute_bias(query_length, key_length)\n        elif attention_mask is not None:\n            position_bias = jnp.zeros_like(attention_mask)\n        else:\n            position_bias = jnp.zeros((1, self.n_heads, query_length, key_length), dtype=self.dtype)\n\n        # if key and values are already calculated, only the last query position bias should be taken\n        if cache_is_filled:\n            max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n            position_bias = jax.lax.dynamic_slice(\n                position_bias,\n                (0, 0, causal_attention_mask_shift, 0),\n                (1, self.n_heads, seq_length, max_decoder_length),\n            )\n        return position_bias\n\n    def __call__(\n            self,\n            hidden_states,\n            attention_mask=None,\n            key_value_states=None,\n            position_bias=None,\n            use_cache=False,\n            output_attentions=False,\n            deterministic=True,\n            init_cache=False,\n    ):\n\n        batch_size, seq_length = hidden_states.shape[:2]\n\n        # q, k, v projections\n        query_states = self.q(hidden_states)  # (batch_size, n_heads, seq_length, dim_per_head)\n        key_states = self.k(hidden_states) if key_value_states is None else self.k(key_value_states)\n        value_states = self.v(hidden_states) if key_value_states is None else self.v(key_value_states)\n        if self.config.use_pjit_attention_force:\n            query_states = with_sharding_constraint(query_states, PartitionSpec(('dp', 'fsdp'), None, 'mp'))\n            key_states = with_sharding_constraint(key_states, PartitionSpec(('dp', 'fsdp'), None, 'mp'))\n            value_states = with_sharding_constraint(value_states, PartitionSpec(('dp', 'fsdp'), None, 'mp'))\n\n        # reshape to (batch_size, seq_length, n_heads, head_dim)\n        query_states = self._split_heads(query_states)\n        key_states = self._split_heads(key_states)\n        value_states = self._split_heads(value_states)\n\n        # counter-act scaling in dot_product_attention_weights function\n        query_states *= jnp.sqrt(query_states.shape[-1])\n\n        # for fast decoding causal attention mask should be shifted\n        causal_attention_mask_shift = (\n            self.variables[\"cache\"][\"cache_index\"] if (self.has_variable(\"cache\", \"cached_key\") and self.causal) else 0\n        )\n        # create causal attention_mask; attention_mask has to be defined when model is causal\n        if self.causal:\n            causal_attention_mask = make_causal_mask(attention_mask, dtype=\"bool\")\n\n            # fast decoding for generate requires special attention_mask\n            if self.has_variable(\"cache\", \"cached_key\"):\n                max_decoder_length = self.variables[\"cache\"][\"cached_key\"].shape[1]\n                causal_attention_mask = jax.lax.dynamic_slice(\n                    causal_attention_mask,\n                    (0, 0, causal_attention_mask_shift, 0),\n                    (1, 1, seq_length, max_decoder_length),\n                )\n\n            # broadcast causal attention mask &amp; attention mask to fit for merge\n            causal_attention_mask = jnp.broadcast_to(\n                causal_attention_mask, (batch_size,) + causal_attention_mask.shape[1:]\n            )\n            attention_mask = jnp.broadcast_to(\n                jnp.expand_dims(attention_mask, axis=(-3, -2)), causal_attention_mask.shape\n            )\n            attention_mask = combine_masks(attention_mask, causal_attention_mask)\n        elif attention_mask is not None:\n            attention_mask = jnp.expand_dims(attention_mask, axis=(-3, -2))\n\n        # During fast autoregressive decoding, we feed one position at a time,\n        # and cache the keys and values step by step.\n        if self.causal and (self.has_variable(\"cache\", \"cached_key\") or init_cache):\n            key_states, value_states, attention_attention_mask = self._concatenate_to_cache(\n                key_states, value_states, query_states, attention_mask\n            )\n\n        # replace masked positions with -10_000\n        if attention_mask is not None:\n            mask_value = jnp.finfo(self.dtype).min\n            attention_mask = jax.lax.select(\n                attention_mask &gt; 0,\n                jnp.full(attention_mask.shape, 0.0).astype(self.dtype),\n                jnp.full(attention_mask.shape, mask_value).astype(self.dtype),\n            )\n\n        if position_bias is None:\n            # compute position bias (only for first layer)\n            position_bias = self._create_position_bias(\n                key_states, query_states, attention_mask, init_cache, seq_length, causal_attention_mask_shift\n            )\n\n            if attention_mask is not None:\n                position_bias = position_bias + attention_mask\n\n        # create dropout rng\n        dropout_rng = None\n        if not deterministic and self.dropout &gt; 0.0:\n            dropout_rng = self.make_rng(\"dropout\")\n\n        # Softmax(QK^T)\n        attn_weights = dot_product_attention_weights(\n            query_states,\n            key_states,\n            bias=position_bias,\n            dropout_rng=dropout_rng,\n            dropout_rate=self.dropout,\n            broadcast_dropout=True,\n            deterministic=deterministic,\n            dtype=self.dtype,\n        )\n\n        if self.config.use_pjit_attention_force:\n            attn_weights = with_sharding_constraint(attn_weights, PartitionSpec(\n                ('dp', 'fsdp'), 'mp', None, None\n            ))\n\n        # multiply with value states\n        attn_output = jnp.einsum(\"...hqk,...khd-&gt;...qhd\", attn_weights, value_states)\n\n        # bring back to (batch_size, seq_length, d_model)\n        attn_output = self._merge_heads(attn_output)\n\n        # apply output matrix\n        attn_output = self.o(attn_output)\n\n        outputs = (attn_output, position_bias)\n\n        if output_attentions:\n            outputs = outputs + (attn_weights,)\n\n        return outputs\n</code></pre>"},{"location":"lib-python-EasyDel-modules-t5-modelling_t5_flax/#lib.python.EasyDel.modules.t5.modelling_t5_flax.FlaxT5Attention.compute_bias","title":"<code>compute_bias(query_length, key_length)</code>","text":"<p>Compute binned relative position bias</p> Source code in <code>lib/python/EasyDel/modules/t5/modelling_t5_flax.py</code> <pre><code>def compute_bias(self, query_length, key_length):\n    \"\"\"Compute binned relative position bias\"\"\"\n    context_position = jnp.arange(query_length, dtype=\"i4\")[:, None]\n    memory_position = jnp.arange(key_length, dtype=\"i4\")[None, :]\n\n    relative_position = memory_position - context_position\n    relative_position_bucket = self._relative_position_bucket(\n        relative_position,\n        bidirectional=(not self.causal),\n        num_buckets=self.relative_attention_num_buckets,\n        max_distance=self.relative_attention_max_distance,\n    )\n\n    values = self.relative_attention_bias(relative_position_bucket)\n    values = values.transpose((2, 0, 1))[None, :, :, :]\n    return values\n</code></pre>"},{"location":"lib-python-EasyDel-modules-t5-modelling_t5_flax/#lib.python.EasyDel.modules.t5.modelling_t5_flax.shift_tokens_right","title":"<code>shift_tokens_right(input_ids, pad_token_id, decoder_start_token_id)</code>","text":"<p>Shift input ids one token to the right.</p> Source code in <code>lib/python/EasyDel/modules/t5/modelling_t5_flax.py</code> <pre><code>def shift_tokens_right(input_ids: np.array, pad_token_id: int, decoder_start_token_id: int) -&gt; np.ndarray:\n    \"\"\"\n    Shift input ids one token to the right.\n    \"\"\"\n    shifted_input_ids = jnp.zeros_like(input_ids)\n    shifted_input_ids = shifted_input_ids.at[:, 1:].set(input_ids[:, :-1])\n    shifted_input_ids = shifted_input_ids.at[:, 0].set(decoder_start_token_id)\n\n    shifted_input_ids = jnp.where(shifted_input_ids == -100, pad_token_id, shifted_input_ids)\n    return shifted_input_ids\n</code></pre>"},{"location":"lib-python-EasyDel-rlhf-ppo/","title":"rlhf.ppo","text":""},{"location":"lib-python-EasyDel-rlhf-reward/","title":"rlhf.reward","text":""},{"location":"lib-python-EasyDel-rlhf-trainer/","title":"rlhf.trainer","text":""},{"location":"lib-python-EasyDel-rlhf-trainer/#lib.python.EasyDel.rlhf.trainer.RLHFTrainer","title":"<code>RLHFTrainer</code>","text":"Source code in <code>lib/python/EasyDel/rlhf/trainer.py</code> <pre><code>class RLHFTrainer:\n\n    def __init__(self,\n                 config: RLHFConfig,\n                 dataset: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset],\n                 tokenizer: Callable,\n                 model: AVAILABLE_MODELS_FOR_RLHF,\n                 reward_model: RewardModel,\n                 critic_model: Optional[AVAILABLE_MODELS_FOR_RLHF] = None,\n                 actor_critic: Optional[ActorCritic] = None\n                 ):\n        self.model = model\n        if config.track_memory:\n            tracker.initialise_tracking()\n        if actor_critic is None:\n            actor_critic = ActorCritic(\n                model=model,\n                critic_model=critic_model,\n                pooled_values=False,\n                dtype=config.dtype,\n                param_dtype=config.param_dtype,\n                precision=config.precision\n\n            )\n        self.actor_critic = actor_critic\n        self.reward_model = reward_model\n        self.critic_model = critic_model\n\n        self.tokenizer = tokenizer\n        self.dataset = dataset\n        self.config = config\n\n        self.actor_optim, self.actor_scheduler = RLHFConfig.get_optimizer_and_scheduler(\n            learning_rate=config.actor_lr,\n            gradient_accumulation_steps=config.gradient_accumulation_steps,\n            steps=int(1e5) * 5,\n            scheduler=config.scheduler,\n            optimizer=config.optimizer,\n            learning_rate_end=config.actor_lr - 1e6\n        )\n\n        self.critic_optim, self.critic_scheduler = RLHFConfig.get_optimizer_and_scheduler(\n            learning_rate=config.critic_lr,\n            gradient_accumulation_steps=config.gradient_accumulation_steps,\n            steps=int(1e5) * 5,\n            scheduler=config.scheduler,\n            optimizer=config.optimizer,\n            learning_rate_end=config.critic_lr - 1e6\n        )\n        self.mesh = config.get_mesh()\n\n    def init_params(\n            self,\n            params_lm: core.FrozenDict = None,\n            params_actor: core.FrozenDict = None,\n            params_critic: core.FrozenDict = None,\n            params_reward: core.FrozenDict = None\n    ):\n        with ((jax.default_device(jax.devices(self.config.backend_offload)[0]))):\n\n            if params_actor is None:\n                params_actor = self.actor_critic.init(\n                    {\n                        'params': jax.random.PRNGKey(\n                            0\n                        )\n                    },\n                    input_ids=jnp.ones((1, 1), dtype=jnp.int32),\n                    attention_mask=jnp.ones((1, 1), dtype=jnp.int32)\n\n                )\n\n                params_actor = flax.traverse_util.unflatten_dict(\n                    params_actor\n                )['params']\n            else:\n                params_actor = flax.traverse_util.unflatten_dict(\n                    flax.core.unfreeze(params_actor)\n                )['params']\n\n            if params_reward is None:\n                params_reward = self.reward_model.init(\n                    {\n                        'params': jax.random.PRNGKey(\n                            0\n                        )\n                    },\n                    input_ids=jnp.ones((1, 1), dtype=jnp.int32),\n                    attention_mask=jnp.ones((1, 1), dtype=jnp.int32)\n\n                )\n\n                params_reward = flax.traverse_util.unflatten_dict(\n                    params_reward\n                )['params']\n            else:\n                params_reward = flax.traverse_util.unflatten_dict(\n                    flax.core.unfreeze(params_reward)\n                )['params']\n\n            if params_critic is None:\n                params_critic = self.actor_critic.init(\n                    {\n                        'params': jax.random.PRNGKey(\n                            0\n                        )\n                    },\n                    input_ids=jnp.ones((1, 1), dtype=jnp.int32),\n                    attention_mask=jnp.ones((1, 1), dtype=jnp.int32)\n\n                )\n\n                params_critic = flax.traverse_util.unflatten_dict(\n                    params_critic\n                )['params']\n            else:\n                params_critic = flax.traverse_util.unflatten_dict(\n                    flax.core.unfreeze(params_actor)\n                )['params']\n\n            if params_lm is not None:\n                params_lm = flax.traverse_util.unflatten_dict(flax.core.unfreeze(params_lm)['params'])\n                lm_keys = [k for k in params_lm.keys()]\n\n                print(\n                    f'\\033[1;31mLoadable Parameters from the Lm Params are {len(lm_keys)}'\n                )\n                for key, _ in params_actor.items():\n                    if key in lm_keys:\n                        params_actor[key] = params_lm[key]\n                for key, _ in params_critic.items():\n                    if key in lm_keys:\n                        params_critic[key] = params_lm[key]\n                for key, _ in params_reward.items():\n                    if key in lm_keys:\n                        params_reward[key] = params_lm[key]\n\n            return (\n                params_lm,\n                params_actor,\n                params_critic,\n                params_reward\n            )\n\n    def configure_funcs(\n            self,\n            partition_rules,\n            params_lm: core.FrozenDict = None,\n            params_actor: core.FrozenDict = None,\n            params_critic: core.FrozenDict = None,\n            params_reward: core.FrozenDict = None\n    ):\n\n        params_lm, params_actor, params_critic, params_reward = self.init_params(\n            params_lm=params_lm,\n            params_actor=params_actor,\n            params_critic=params_critic,\n            params_reward=params_reward\n        )\n\n        def create_train_state(\n                params_actor_,\n                params_critic_,\n                params_reward_\n        ) -&gt; Union[Any, TrainStateRLHF]:\n            return TrainStateRLHF.create(\n                actor_params=params_actor_,\n                critic_params=params_critic_,\n                reward_params=params_reward_,\n                actor_optim=self.actor_optim,\n                critic_optim=self.critic_optim,\n                apply_fn_critic=self.critic_model.apply,\n                apply_fn_actor=self.actor_critic.apply,\n                apply_fn_reward=self.reward_model.apply\n            )\n\n        shape = jax.eval_shape(\n            create_train_state(\n                params_actor_=params_actor,\n                params_critic_=params_critic,\n                params_reward_=params_reward\n            )\n        )\n        partition_specs = fjformer.match_partition_rules(params=shape, rules=partition_rules)\n        shard_fns, _ = fjformer.make_shard_and_gather_fns(\n            partition_specs=partition_specs,\n            dtype_specs=self.config.dtype\n        )\n\n        sharded_create_train_state = pjit.pjit(\n            create_train_state,\n            in_shardings=(PartitionSpec(), PartitionSpec(), PartitionSpec()),\n            out_shardings=(partition_specs,),\n            backend=self.config.backend\n        )\n\n        return sharded_create_train_state(\n            params_actor, params_critic, params_reward\n        ), partition_specs\n\n    def learn(\n            self,\n            memory: typing.Deque[Memory],\n            train_state: TrainStateRLHF,\n    ):\n        \"\"\"\n        Memory Must be Deque of all prevision memories\n        train_state : TrainStateRLHF Type\n        \"\"\"\n        memory_dataloader = create_dataloader(\n            memory,\n            self.config.minibatch_size\n        )\n\n        def forward(\n                train_state: TrainStateRLHF,\n                input_ids,\n                pm,\n                rewards,\n                old_values,\n                attention_mask,\n                old_action_probs,\n                old_log_probs,\n        ):\n            def calculate_loss(params):\n                global rewards\n                global old_values\n\n                action_masks = ~pm &amp; attention_mask\n                action_logits, values = train_state.apply_fn_actor(\n                    params[\"params\"],\n                    input_ids=input_ids,\n                    attention_mask=attention_mask\n                )\n                action_logits = shift(action_logits, shift=1, axis=-2)\n                action_len = old_log_probs.shape[-1]\n                action_probs = jax.nn.softmax(action_logits, axis=-1)\n                action_log_probs = log_prob(action_probs, input_ids)\n                action_log_probs = action_log_probs[:, -action_len:]\n                entropies = masked_entropy(action_probs, attention_mask=action_masks)\n                kl_penalty = masked_mean(\n                    jnp.sum((old_action_probs * (jnp.log(old_action_probs) - jnp.log(action_probs))), axis=-1),\n                    attention_mask=attention_mask\n                ) * self.config.kl_div_loss_weight\n\n                rewards = rewards - kl_penalty\n                normalize_kwargs = dict()\n\n                if old_values.ndim == 2:\n                    old_values, values = map(lambda t: shift(t, shift=1, axis=-2), (old_values, values))\n\n                    old_values = old_values[:, -action_len:]\n                    values = values[:, -action_len:]\n                    rewards = einops.rearrange(rewards, 'b -&gt; b 1')\n                    normalize_kwargs = dict(axis=-1, attention_mask=action_masks[:, -action_len:])\n\n                if values.ndim &lt; rewards.ndim:\n                    values = einops.rearrange(values, '... -&gt; ... 1')\n\n                ratios = (action_log_probs - old_log_probs).exp()\n                advantages = masked_normalize(rewards - old_values, **normalize_kwargs)\n\n                if advantages.ndim == 1:\n                    advantages = einops.rearrange(advantages, 'b -&gt; b 1')\n\n                surr1 = ratios * advantages\n                surr2 = jnp.clip(ratios, 1 - self.config.eps_clip, 1 + self.config.eps_clip)\n                policy_loss = -jnp.minimum(surr1, surr2) - self.config.beta_s * entropies  # Policy Loss\n                loss = jnp.mean(policy_loss)  # Loss\n\n                value_loss = jnp.mean(\n                    clipped_value_loss(values, rewards, old_values, self.config.value_clip))  # VLoss\n                return loss, value_loss  # I need gradient for these two losses\n\n            grad, (loss_, value_loss_) = jax.value_and_grad(calculate_loss)(train_state.actor_params)\n            train_state = train_state.apply_gradients(\n                grad_actor=grad,  # Based on Loss\n                grads_critic=jax.grad(lambda p: value_loss_)(train_state.critic_params)  # Based on Value Loss\n            )\n            return train_state, (loss_, value_loss_)\n\n        pbar = tqdm.autonotebook.tqdm(\n            total=self.config.epochs * len(memory_dataloader)\n        )\n        if self.config.track_memory:\n            mem_res = tracker.get_mem()\n        else:\n            mem_res = 'Tracking Option is OFF'\n        for _ in range(\n                self.config.epochs\n        ):\n            for (\n                    input_ids_,\n                    pm_,\n                    attention_mask_,\n                    old_action_probs_,\n                    old_log_probs_,\n                    rewards_,\n                    old_values_\n            ) in memory_dataloader:\n                train_state, loss = forward(\n                    train_state=train_state,\n                    attention_mask=attention_mask_,\n                    input_ids=input_ids_,\n                    pm=pm_,\n                    rewards=rewards_,\n                    old_values=old_values_,\n                    old_log_probs=old_log_probs_,\n                    old_action_probs=old_action_probs_\n                )\n\n                pbar.set_postfix(\n                    loss=loss\n                )\n                if self.config.track_memory:\n                    IPython.display.clear_output(True)\n                    pbar.display(mem_res)\n                pbar.update(1)\n        return train_state\n\n    def train(\n            self,\n            params_actor: core.FrozenDict,\n            params_critic: core.FrozenDict,\n            params_reward: core.FrozenDict,\n            params_lm: core.FrozenDict,\n            partition_rules,\n            num_episodes=50000,\n            max_time_steps=500,\n            update_time_steps=5000,\n            max_batch_size=16,\n            max_sequence_length=2048,\n            eos_token=None,\n            temperature=1.,\n    ):\n        with self.mesh:\n\n            time = 0\n            memories = collections.deque([])\n            max_train_eps = self.dataset['train'].num_rows\n            assert max_train_eps &gt; max_batch_size\n            train_state, partition_specs = self.configure_funcs(\n                partition_rules=partition_rules,\n                params_lm=params_lm,\n                params_actor=params_actor,\n                params_critic=params_critic,\n                params_reward=params_reward\n            )\n\n            def get_rand():\n                return int(random.random() * max_train_eps)\n\n            @functools.partial(\n                pjit.pjit,\n                in_shardings=(\n                        partition_specs,\n                        PartitionSpec(),\n                        PartitionSpec()\n                ),\n                out_shardings=(\n                        PartitionSpec(),\n                        PartitionSpec(),\n                        PartitionSpec(),\n                        PartitionSpec(),\n                        PartitionSpec(),\n                        PartitionSpec(),\n                        PartitionSpec()\n                )\n            )\n            def step(\n                    train_state_: TrainStateRLHF,\n                    input_ids_,\n                    attention_mask_\n            ):\n                action_, sequence_, attention_mask_, prompt_mask_, action_logits_, value_ = self.actor_critic.generate(\n                    params=train_state_.actor_params,\n                    input_ids=einops.rearrange(\n                        input_ids_, 'n -&gt; 1 n'\n                    ),\n                    attention_mask_=attention_mask_,\n                    max_sequence_length=max_sequence_length,\n                    eos_token=eos_token,\n                    temperature=temperature,\n                    return_values=True\n                )\n                action_logits_ = shift(action_logits_, shift=1, axis=-2)\n                action_probs_ = jax.nn.softmax(action_logits_, axis=-1)\n                action_len_ = action_.shape[-1]\n                action_log_prob_ = log_prob(action_probs_, sequence_)\n                action_log_prob_ = action_log_prob_[:, -action_len_:]\n                action_ = einops.rearrange(action_, '1 ... -&gt; ...')\n                sequence_ = jnp.concatenate(\n                    (\n                        input_ids_, action_\n                    ), axis=0\n                )\n                prompt_length_ = len(input_ids_)\n                prompt_mask_ = jnp.arange(sequence_.shape[-1]) &lt; prompt_length_\n\n                sequence_ = einops.rearrange(sequence_, 'n -&gt; 1 n')\n                prompt_mask_ = einops.rearrange(prompt_mask_, 'n -&gt; 1 n')\n\n                reward_ = train_state_.apply_fn_reward(\n                    train_state_.reward_params,\n                    sequence_,\n                    prompt_mask=prompt_mask_,\n                    attention_mask=attention_mask_,\n                    sample=True\n                )\n                return (\n                    sequence_,\n                    prompt_mask_,\n                    attention_mask_,\n                    action_probs_,\n                    action_log_prob_,\n                    reward_,\n                    value_\n                )\n\n            sharded_step = step(\n                train_state_=train_state,\n                input_ids_=jnp.ones((1, 128), dtype=jnp.int32),\n                attention_mask_=jnp.ones((1, 128), dtype=jnp.int32),\n            )\n            rearrange_ = lambda t: einops.rearrange(t, '1 ... -&gt; ...')\n            pbar = tqdm.tqdm(iterable=range(num_episodes), desc='Episode')\n            for _ in pbar:\n                for time_step in range(max_time_steps):\n                    time += 1\n                    index = get_rand()\n                    index = index - max_batch_size if index &gt; max_batch_size else index\n                    input_ids = self.dataset['train'][index:index + max_batch_size]['input_ids']\n                    attention_mask = self.dataset['train'][index:index + max_batch_size]['attention_mask']\n\n                    (\n                        sequence,\n                        prompt_mask,\n                        attention_mask,\n                        action_probs,\n                        action_log_prob,\n                        reward,\n                        value\n                    ) = sharded_step(\n                        train_state,\n                        input_ids,\n                        attention_mask\n                    )\n                    pbar.set_postfix(\n                        time=time, time_step=f\"{time_step}/{max_time_steps}\"\n                    )\n                    memories.append(Memory(*map(rearrange_, (\n                        sequence,\n                        prompt_mask,\n                        attention_mask,\n                        action_probs,\n                        action_log_prob,\n                        reward,\n                        value\n                    ))))\n\n                    # learn from the stored memories\n\n                    if time % update_time_steps == 0:\n                        train_state = self.learn(\n                            memories,\n                            train_state=train_state\n                        )\n                        memories.clear()\n        return train_state\n</code></pre>"},{"location":"lib-python-EasyDel-rlhf-trainer/#lib.python.EasyDel.rlhf.trainer.RLHFTrainer.learn","title":"<code>learn(memory, train_state)</code>","text":"<p>Memory Must be Deque of all prevision memories train_state : TrainStateRLHF Type</p> Source code in <code>lib/python/EasyDel/rlhf/trainer.py</code> <pre><code>def learn(\n        self,\n        memory: typing.Deque[Memory],\n        train_state: TrainStateRLHF,\n):\n    \"\"\"\n    Memory Must be Deque of all prevision memories\n    train_state : TrainStateRLHF Type\n    \"\"\"\n    memory_dataloader = create_dataloader(\n        memory,\n        self.config.minibatch_size\n    )\n\n    def forward(\n            train_state: TrainStateRLHF,\n            input_ids,\n            pm,\n            rewards,\n            old_values,\n            attention_mask,\n            old_action_probs,\n            old_log_probs,\n    ):\n        def calculate_loss(params):\n            global rewards\n            global old_values\n\n            action_masks = ~pm &amp; attention_mask\n            action_logits, values = train_state.apply_fn_actor(\n                params[\"params\"],\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            action_logits = shift(action_logits, shift=1, axis=-2)\n            action_len = old_log_probs.shape[-1]\n            action_probs = jax.nn.softmax(action_logits, axis=-1)\n            action_log_probs = log_prob(action_probs, input_ids)\n            action_log_probs = action_log_probs[:, -action_len:]\n            entropies = masked_entropy(action_probs, attention_mask=action_masks)\n            kl_penalty = masked_mean(\n                jnp.sum((old_action_probs * (jnp.log(old_action_probs) - jnp.log(action_probs))), axis=-1),\n                attention_mask=attention_mask\n            ) * self.config.kl_div_loss_weight\n\n            rewards = rewards - kl_penalty\n            normalize_kwargs = dict()\n\n            if old_values.ndim == 2:\n                old_values, values = map(lambda t: shift(t, shift=1, axis=-2), (old_values, values))\n\n                old_values = old_values[:, -action_len:]\n                values = values[:, -action_len:]\n                rewards = einops.rearrange(rewards, 'b -&gt; b 1')\n                normalize_kwargs = dict(axis=-1, attention_mask=action_masks[:, -action_len:])\n\n            if values.ndim &lt; rewards.ndim:\n                values = einops.rearrange(values, '... -&gt; ... 1')\n\n            ratios = (action_log_probs - old_log_probs).exp()\n            advantages = masked_normalize(rewards - old_values, **normalize_kwargs)\n\n            if advantages.ndim == 1:\n                advantages = einops.rearrange(advantages, 'b -&gt; b 1')\n\n            surr1 = ratios * advantages\n            surr2 = jnp.clip(ratios, 1 - self.config.eps_clip, 1 + self.config.eps_clip)\n            policy_loss = -jnp.minimum(surr1, surr2) - self.config.beta_s * entropies  # Policy Loss\n            loss = jnp.mean(policy_loss)  # Loss\n\n            value_loss = jnp.mean(\n                clipped_value_loss(values, rewards, old_values, self.config.value_clip))  # VLoss\n            return loss, value_loss  # I need gradient for these two losses\n\n        grad, (loss_, value_loss_) = jax.value_and_grad(calculate_loss)(train_state.actor_params)\n        train_state = train_state.apply_gradients(\n            grad_actor=grad,  # Based on Loss\n            grads_critic=jax.grad(lambda p: value_loss_)(train_state.critic_params)  # Based on Value Loss\n        )\n        return train_state, (loss_, value_loss_)\n\n    pbar = tqdm.autonotebook.tqdm(\n        total=self.config.epochs * len(memory_dataloader)\n    )\n    if self.config.track_memory:\n        mem_res = tracker.get_mem()\n    else:\n        mem_res = 'Tracking Option is OFF'\n    for _ in range(\n            self.config.epochs\n    ):\n        for (\n                input_ids_,\n                pm_,\n                attention_mask_,\n                old_action_probs_,\n                old_log_probs_,\n                rewards_,\n                old_values_\n        ) in memory_dataloader:\n            train_state, loss = forward(\n                train_state=train_state,\n                attention_mask=attention_mask_,\n                input_ids=input_ids_,\n                pm=pm_,\n                rewards=rewards_,\n                old_values=old_values_,\n                old_log_probs=old_log_probs_,\n                old_action_probs=old_action_probs_\n            )\n\n            pbar.set_postfix(\n                loss=loss\n            )\n            if self.config.track_memory:\n                IPython.display.clear_output(True)\n                pbar.display(mem_res)\n            pbar.update(1)\n    return train_state\n</code></pre>"},{"location":"lib-python-EasyDel-rlhf-utils/","title":"rlhf.utils","text":""},{"location":"lib-python-EasyDel-rlhf-utils/#lib.python.EasyDel.rlhf.utils.masked_kl_div","title":"<code>masked_kl_div(prob1, prob2, attention_mask=None, reduce_batch=False)</code>","text":"<p>need to account for variable sequence lengths, therefore not using the built-in functional version</p> Source code in <code>lib/python/EasyDel/rlhf/utils.py</code> <pre><code>def masked_kl_div(prob1, prob2, attention_mask=None, reduce_batch=False):\n    \"\"\"\n    need to account for variable sequence lengths, therefore not using the built-in functional version\n    \"\"\"\n    kl_divs = (prob1 * (log(prob1) - log(prob2))).sum(axis=-1)\n    loss = masked_mean(kl_divs, attention_mask)\n\n    if reduce_batch:\n        return loss.mean()\n\n    return loss\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/","title":"serve.jax_serve","text":""},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer","title":"<code>JAXServer</code>","text":"<p>             Bases: <code>object</code></p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>class JAXServer(object):\n\n    def __init__(self, config=None):\n\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It sets up all the attributes that will be used by other methods in the class.\n\n\n        :param self: Refer to the current instance of a class\n        :param config: Pass the jaxserverconfig object\n        :return: A fastapi object\n\n        \"\"\"\n        self.process_uvicorn, self.prefix_tokenizer, self.params, self.tokenizer, self.model, \\\n            self.rules, self._generate, self._greedy_generate = [None] * 8\n        assert config is None or isinstance(config, JaxServerConfig), 'config can be None or JaxServerConfig Type'\n        if config is None:\n            self.config = JaxServerConfig()\n        else:\n            self.config = config\n        self._funcs_generated = False\n        self.number_of_served_request_until_last_up_time = 0\n\n        self.rng_generator = RNG(42)\n        initialise_tracking(0.5)\n        array = jnp.ones((len(jax.devices()), 1)).reshape(self.config.mesh_axes_shape)\n        self.mesh = Mesh(mesh_utils.create_device_mesh(array.shape), self.config.mesh_axes_names)\n\n        self.app = FastAPI()\n        self.app.post('/chat')(self.forward_chat)\n        self.app.post('/instruct')(self.forward_instruct)\n        self.app.get('/status')(self.status)\n        self.gradio_app_chat = self.create_gradio_ui_chat()\n        self.gradio_app_instruct = self.create_gradio_ui_instruct()\n        self.app = gr.mount_gradio_app(self.app, self.gradio_app_chat, '/gradio_chat')\n        self.app = gr.mount_gradio_app(self.app, self.gradio_app_instruct, '/gradio_instruct')\n\n    def status(self):\n        \"\"\"\n        The status function returns a dictionary with the following keys:\n            config: A dictionary containing all of the configuration parameters for this server.\n            devices: A string describing which devices are available to JAX.\n            number_of_backends: The number of backends available to JAX.  This is usually equal to the number of GPUs on your machine, but can be less if you have not installed CUDA or if you have disabled some GPUs in your system BIOS settings (e.g., because they are defective).  It can also be more than one if you have multiple machines connected via MPI and running under Horov\n\n        :param self: Represent the instance of the class\n        :return: A dictionary with the following keys:\n\n        \"\"\"\n        return {\n            'config': {k: v for k, v in self.config.__dict__.items()},\n            'devices': f\"{jax.devices()}\",\n            'number_of_backends': len(jax.devices()),\n            'status': 'Ready',\n            'number_of_served_request_until_last_up_time': f\"{self.number_of_served_request_until_last_up_time}\",\n            'memory': f\"{get_mem()}\"\n        }\n\n    @staticmethod\n    def get_memory():\n        \"\"\"\n        The get_memory function returns the total memory of the system in bytes.\n\n\n        :return: The amount of memory used by the program\n\n        \"\"\"\n        return get_mem()\n\n    def configure_generate_functions(self, model, tokenizer):\n\n        \"\"\"\n        The configure_generate_functions function is used to configure the generation functions for a given model.\n\n        :param self: Access variables within the class\n        :param model: Generate the model\n        :param tokenizer: Get the eos_token_id, pad_token_id and bos token id\n        :return: A function that takes in three parameters:\n\n        \"\"\"\n        assert self.rules is not None, 'you should first shard params with using ``shard_params`` method'\n\n        if tokenizer.pad_token is None:\n            logging.info(\n                'Tokenizer does not contain padding token setting padding token to eos token for open end generation')\n            tokenizer.pad_token = tokenizer.eos_token\n\n        try:\n            tokenizer.padding_side = 'left'\n            tokenizer.truncation_side = 'left'\n            self.prefix_tokenizer = copy.deepcopy(tokenizer)\n            tokenizer.padding_side = 'right'\n            tokenizer.truncation_side = 'right'\n            self.tokenizer = copy.deepcopy(tokenizer)\n        except:\n            prefix_str(\n                'Warning', f'The class Model of Tokenizer {type(tokenizer)} do not support deepcopy option '\n            )\n            if self.config.use_prefix_tokenizer:\n                tokenizer.padding_side = 'left'\n                tokenizer.truncation_side = 'left'\n            else:\n                tokenizer.padding_side = 'right'\n                tokenizer.truncation_side = 'right'\n            self.prefix_tokenizer = tokenizer\n\n        @functools.partial(\n            pjit,\n            in_shardings=(self.rules, Ps(), Ps()),\n            out_shardings=(Ps())\n        )\n        def greedy_generate(parameters, input_ids, attention_mask):\n            input_ids = with_sharding_constraint(input_ids, Ps(('dp', 'fsdp')))\n            attention_mask = with_sharding_constraint(attention_mask, Ps(('dp', 'fsdp')))\n            predict = model.generate(\n                input_ids,\n                attention_mask=attention_mask,\n                params=parameters,\n                generation_config=GenerationConfig(\n                    max_new_tokens=self.config.max_stream_tokens,\n                    eos_token_id=tokenizer.eos_token_id,\n                    pad_token_id=tokenizer.pad_token_id,\n                    bos_token_id=tokenizer.bos_token_id,\n\n                    do_sample=False,\n                    num_beams=1,\n                )\n            ).sequences[:, input_ids.shape[1]:]\n            return predict\n\n        @functools.partial(\n            pjit,\n            in_shardings=(self.rules, Ps(), Ps()),\n            out_shardings=(Ps())\n        )\n        def generate(parameters, input_ids, attention_mask):\n            input_ids = with_sharding_constraint(input_ids, Ps(('dp', 'fsdp')))\n            attention_mask = with_sharding_constraint(attention_mask, Ps(('dp', 'fsdp')))\n            predict = model.generate(\n                input_ids,\n                attention_mask=attention_mask,\n                params=parameters,\n                generation_config=GenerationConfig(\n                    max_new_tokens=self.config.max_stream_tokens,\n\n                    eos_token_id=tokenizer.eos_token_id,\n                    pad_token_id=tokenizer.pad_token_id,\n                    bos_token_id=tokenizer.bos_token_id,\n\n                    temperature=self.config.temperature,\n                    do_sample=True,\n                    num_beams=1,\n                    top_p=self.config.top_p,\n                    top_k=self.config.top_k,\n                )\n            ).sequences[:, input_ids.shape[1]:]\n            return predict\n\n        self._generate = generate\n        self._greedy_generate = greedy_generate\n        self._funcs_generated = True\n\n    def auto_configure(self, model, params, tokenizer, partition_rules):\n        \"\"\"\n        The auto_configure function is a helper function that will automatically configure the model for distributed training.\n        It does this by:\n            1) sharding the parameters of the model based on partition_rules, and then\n            2) configuring generate functions to be used in distributed training.\n\n        :param self: Represent the instance of the class\n        :param model: Configure the model\n        :param params: Store the parameters that are used to configure the model\n        :param tokenizer: Tokenize the input text\n        :param partition_rules: Specify how the parameters should be partitioned\n        :return: A dictionary with the following keys:\n\n        \"\"\"\n        self.shard_params(params=params, partition_rules=partition_rules)\n        self.configure_generate_functions(model, tokenizer)\n\n    def generate(self,\n                 params: Union[flax.core.FrozenDict, dict],\n                 input_ids: chex.Array,\n                 attention_mask: chex.Array,\n                 ):\n        \"\"\"\n        The generate function is used to generate a sequence of tokens from the model.\n\n        :param self: Access variables that belong to the class\n        :param params: Union[flax.core.FrozenDict, dict]: Pass the parameters of the model to be used in generating text\n        :param input_ids: chex.Array: Pass the input to the model\n        :param attention_mask: chex.Array: Mask the padding tokens\n        :return: The logits of the model\n\n        \"\"\"\n        if not self._funcs_generated:\n            raise NotImplementedError(\n                'this method will be implemented automatically after using ``configure_generate_functions`` function'\n            )\n        else:\n            with self.mesh:\n                return self._generate(\n                    params, input_ids, attention_mask\n                )\n\n    @classmethod\n    def load(\n            cls,\n            model: transformers.FlaxPreTrainedModel,\n            config_model: transformers.PretrainedConfig,\n            tokenizer: transformers.PreTrainedTokenizer,\n            path: typing.Union[str, os.PathLike],\n            config=None,\n            add_params_field: bool = True,\n            init_shape: tuple = (1, 1),\n            do_memory_log: bool = False,\n            verbose: bool = True\n    ):\n        \"\"\"\n        The load function is used to load a pretrained model from disk.\n\n        :param cls: Refer to the class itself\n        :param model: transformers.FlaxPreTrainedModel: Initialize the server\n        :param config_model: transformers.PretrainedConfig: Get the partition rules\n        :param tokenizer: transformers.PreTrainedTokenizer: Load the tokenizer from the model\n        :param path: typing.Union[str, os.PathLike]: Specify the path to the checkpoint file\n        :param config: Configure the server\n        :param add_params_field: bool: Add a params field to the server\n        :param init_shape: tuple: Specify the shape of the input to be used for generating shard_fns\n        :param do_memory_log: bool: Log the memory usage of the server\n        :param verbose: bool: Print the compilation process\n        :return: A server\n\n        \"\"\"\n        assert hasattr(model,\n                       'init_weights'), 'model must contain init_weights func in order to init params for shard_fns'\n        assert hasattr(config_model,\n                       'get_partition_rules'), 'config_model must contain get_partition_rules functions'\n        server = cls(config=config)\n        logging.info(\n            'running _init() func in order to make shard_fns'\n        )\n        with jax.default_device(jax.devices('cpu')[0]):\n            def _init():\n                return model.init_weights(jax.random.PRNGKey(0), init_shape)\n\n            shape = jax.eval_shape(_init)\n        logging.info(\n            'matching partition rules'\n        )\n        rules = match_partition_rules(params=shape, rules=config_model.get_partition_rules(True))\n\n        with server.mesh:\n            shard_fns, _ = make_shard_and_gather_fns(rules, get_float_dtype_by_name(server.config.dtype))\n            logging.info(\n                'loading checkpoints'\n            )\n\n            shard_fns = flax.traverse_util.flatten_dict(shard_fns)\n            server.params = {}\n            with open(path, 'rb') as stream:\n                unpacker = msgpack.Unpacker(stream, read_size=83886080, max_buffer_size=0)\n                pbar = tqdm.tqdm(unpacker)\n                for key, value in pbar:\n                    key = tuple(key)\n                    tensor = from_bytes(None, value)\n                    tensor = shard_fns[key](tensor)\n                    server.params[key] = tensor\n                    if do_memory_log:\n                        pbar.write(server.get_memory())\n                    pbar.set_description('Sharding Params')\n        server.params = flax.traverse_util.unflatten_dict(server.params)\n        server.params = {'params': server.params} if add_params_field else server.params\n\n        server.rules = {'params': rules} if add_params_field else rules\n        logging.info(\n            'configuring generate functions for the server'\n        )\n        server.configure_generate_functions(model, tokenizer)\n\n        if server.config.pre_compile:\n            server.compile(verbose=verbose)\n        return server\n\n    @classmethod\n    def load_from_params(\n            cls,\n            model: transformers.FlaxPreTrainedModel,\n            config_model: transformers.PretrainedConfig,\n            tokenizer: transformers.PreTrainedTokenizer,\n            params: typing.Dict,\n            config=None,\n            add_params_field: bool = True,\n            do_memory_log: bool = False,\n            verbose: bool = True\n    ):\n        \"\"\"\n        The load_from_params function is used to load a model from the parameters of a pretrained model.\n        It takes in the following arguments:\n            - cls: The class of the server you are loading, this should be Server or TPU_Server depending on what backend you want to use.\n            - model: A FlaxPreTrainedModel object that contains all of your models functions and parameters. This can be found in transformers/flax_utils/models/*model*.py\n                where *model* is replaced with whatever transformer you are using (e.g., bert). You can also create your own custom\n\n        :param cls: Create a new instance of the class\n        :param model: transformers.FlaxPreTrainedModel: Load the model\n        :param config_model: transformers.PretrainedConfig: Get the partition rules\n        :param tokenizer: transformers.PreTrainedTokenizer: Tokenize the input text\n        :param params: typing.Dict: Pass in the parameters of the model\n        :param config: Pass in the config file for the server\n        :param add_params_field: bool: Add a params field to the server\n        :param do_memory_log: bool: Log the memory usage of the server\n        :param verbose: bool: Print out the status of the compilation\n        :return: A server object\n\n        \"\"\"\n        assert hasattr(model,\n                       'init_weights'), 'model must contain init_weights func in order to init params for shard_fns'\n        assert hasattr(config_model,\n                       'get_partition_rules'), 'config_model must contain get_partition_rules functions'\n        server = cls(config=config)\n\n        with server.mesh:\n            logging.info(\n                'matching partition rules'\n            )\n            rules = match_partition_rules(params=params, rules=config_model.get_partition_rules(True))\n            shard_fns, _ = make_shard_and_gather_fns(rules, get_float_dtype_by_name(server.config.dtype))\n            logging.info(\n                'sharding parameters across all of the chosen backend(tpu/gpu/cpu)s'\n            )\n            params = flax.traverse_util.flatten_dict(params)\n            shard_fns = flax.traverse_util.flatten_dict(shard_fns)\n            pbar = tqdm.tqdm(params.keys())\n            for key in pbar:\n\n                key = tuple(key)\n                params[key] = shard_fns[key](params[key])\n\n                if do_memory_log:\n                    pbar.write(server.get_memory())\n                pbar.set_description('Sharding Params')\n            server.params = flax.traverse_util.unflatten_dict(params)\n            server.params = {'params': server.params} if add_params_field else server.params\n        server.rules = {'params': rules} if add_params_field else rules\n        logging.info(\n            'configuring generate functions for the server'\n        )\n        server.configure_generate_functions(model, tokenizer)\n        if server.config.pre_compile:\n            server.compile(verbose=verbose)\n        return server\n\n    def compile(self, verbose: bool = True) -&gt; bool:\n        \"\"\"\n        The compile function is used to compile the model for use in inference.\n        It does this by running through all possible combinations of rules and actions,\n        and compiling them into functions that can be called later on during inference.\n        This allows us to avoid having to recompile the model every time we want to run it,\n        which would be very slow.\n\n        :param self: Represent the instance of the class\n        :param verbose: bool: Print out the compiling process\n        :return: True, but what does it do?\n\n        \"\"\"\n        assert self._funcs_generated, 'funcs are not generated yet'\n        assert self.rules is not None, 'rules should not be None'\n        if self.config.use_prefix_tokenizer:\n            if verbose:\n                print('\\033[1;91mCompiling Model Forwards Greedy/NonGreedy(Generate)')\n                print('Compiling Greedy Funcs')\n\n            r, a = [None] * 2\n            for r, a in self.process(\n                    string='',\n                    max_new_tokens=self.config.max_stream_tokens,\n                    greedy=True\n            ):\n                ...\n            print('Compiling NonGreedy(Generate) Funcs\\033[1;0m')\n            for r, a in self.process(\n                    string='',\n                    max_new_tokens=self.config.max_stream_tokens,\n                    greedy=False\n            ):\n                ...\n\n        else:\n            print(\n                '\\033[1;91mSkip Compiling the compiling process is useless '\n                'when you are not using prefix tokenizer\\033[1;0m')\n        return True\n\n    def greedy_generate(self,\n                        params: Union[flax.core.FrozenDict, dict],\n                        input_ids: chex.Array,\n                        attention_mask: chex.Array,\n                        ):\n        \"\"\"\n        The greedy_generate function is a helper function that takes in the model parameters, input_ids and attention_mask\n        and returns the generated tokens. It uses greedy search to generate tokens one at a time.\n\n\n        :param self: Refer to the object itself\n        :param params: Union[flax.core.FrozenDict, dict]: Pass the parameters to the model\n        :param input_ids: chex.Array: Pass in the input sequence\n        :param attention_mask: chex.Array: Mask the input tokens\n        :param : Specify the parameters of the model\n        :return:  generated_ids\n\n        \"\"\"\n        if not self._funcs_generated:\n            raise NotImplementedError(\n                'this method will be implemented automatically after using ``configure_generate_functions`` function'\n            )\n        else:\n            with self.mesh:\n                return self._greedy_generate(\n                    params, input_ids, attention_mask\n                )\n\n    def shard_params(self, params, partition_rules):\n\n        \"\"\"\n        The shard_params function takes in a set of parameters and a partition rule.\n        The partition rule is used to determine how the parameters should be sharded across devices.\n        For example, if we have two devices, one with 4GB of memory and another with 8GB of memory,\n        we may want to shard our model such that the device with more memory has more parameters on it.\n        This function returns an updated version of params where each parameter is now stored on its own device.\n\n        :param self: Bind the instance of the class to a method\n        :param params: Pass the parameters of the model to be sharded\n        :param partition_rules: Specify how the parameters should be partitioned\n        :return: The sharded parameters\n\n        \"\"\"\n        logging.log(\n            logging.INFO,\n            'the parameters will be sharded and ba saved inside server you can access them by ``JAXServer.params``')\n        rules = match_partition_rules(params=params, rules=partition_rules)\n        self.rules = rules\n        shard_fns, _ = make_shard_and_gather_fns(rules, get_float_dtype_by_name(self.config.dtype))\n\n        with self.mesh:\n            self.params = jax.tree_map(\n                lambda f, p: f(p), shard_fns, params\n            )\n\n        return self.params\n\n    def forward_chat(self, data: ChatRequest):\n\n        \"\"\"\n        The forward_chat function is the main function of this class.\n        It takes in a ChatRequest object, which contains a prompt and history.\n        The prompt is the user's input to be processed by the chatbot, while history\n        is an array of previous inputs and outputs from both sides (user and bot).\n        The forward_chat function then formats these inputs into one string that can be processed by our model.\n        This formatted string is then passed through our process() method, which returns an output response as well as how many tokens were used to generate it.\n\n        :param self: Access the attributes and methods of the class\n        :param data: ChatRequest: Pass in the data from the request\n        :return: A dictionary with the following keys:\n\n        \"\"\"\n        if not self._funcs_generated:\n            return {\n                'status': \"down\"\n            }\n\n        string = self.format_chat(\n            prompt=data.prompt,\n            system=None,\n            history=data.history\n        )\n\n        response, used_tokens = [None] * 2\n        for response, used_tokens in self.process(\n                string=string,\n                greedy=data.greedy,\n                max_new_tokens=None\n        ):\n            ...\n        self.number_of_served_request_until_last_up_time += 1\n        return {\n            'input': f'{string}',\n            'response': response,\n            'tokens_used': used_tokens,\n        }\n\n    @staticmethod\n    def format_instruct(system: str, instruction: str) -&gt; str:\n        \"\"\"\n        Here you will get the system and instruction from user, and you can apply your prompting style\n        \"\"\"\n        raise NotImplementedError()\n\n    @staticmethod\n    def format_chat(history: typing.List[str], prompt: str, system: typing.Union[str, None]) -&gt; str:\n        \"\"\"\n        Here you will get the system, prompt and history from user, and you can apply your prompting style\n        \"\"\"\n        raise NotImplementedError()\n\n    def forward_instruct(self, data: InstructRequest):\n        \"\"\"\n        The forward_instruct function is the main function of this class.\n        It takes in a InstructRequest object, which contains the system and instruction to be processed.\n        The function then formats the input string using format_instruct, and passes it into process().\n        process() returns a tuple containing (response, used_tokens). The response is returned as part of\n        the response dictionary. If no valid responses are found by process(), None will be returned instead.\n\n        :param self: Bind the method to the object\n        :param data: InstructRequest: Pass the system and instruction to the function\n        :return: A dictionary with three keys:\n\n        \"\"\"\n        if not self._funcs_generated:\n            return {\n                'status': \"down\"\n            }\n\n        response, used_tokens = [None] * 2\n        string = self.format_instruct(\n            system=data.system,\n            instruction=data.instruction\n        )\n        for response, used_tokens in self.process(\n                string=string,\n                greedy=data.greedy,\n                max_new_tokens=None\n        ):\n            ...\n        self.number_of_served_request_until_last_up_time += 1\n        return {\n            'input': f'{string}',\n            'response': response,\n            'tokens_used': used_tokens,\n        }\n\n    def forward_instruct_non_api(self, prompt, system, greedy):\n        \"\"\"\n        The forward_instruct_non_api function is a wrapper for the forward_instruct function.\n        It takes in a prompt, system, and greedy flag as arguments and returns the response from\n        the forward_instruct function. The purpose of this wrapper is to allow users to call\n        forward_instruct without having to create an InstructRequest object.\n\n        :param self: Represent the instance of the class\n        :param prompt: Pass the instruction to the system\n        :param system: Specify which system to use for the instruction\n        :param greedy: Determine whether the system should return\n        :return: The response from the forward_instruct function\n\n        \"\"\"\n        data = InstructRequest(\n            prompt=prompt,\n            system=system,\n            greedy=greedy\n        )\n        return self.forward_instruct(data)\n\n    def forward_chat_non_api(self, prompt, history, greedy):\n        \"\"\"\n        The forward_chat_non_api function is a wrapper for the forward_chat function.\n        It takes in a prompt, history, and greedy parameter and returns the response from\n        the forward_chat function. The purpose of this wrapper is to allow users to use\n        the chatbot without having to create ChatRequest objects.\n\n        :param self: Represent the instance of the class\n        :param prompt: Pass the user's input to the model\n        :param history: Pass the history of the conversation to the model\n        :param greedy: Determine whether the model should use a greedy search\n        :return: A chat-response object\n\n        \"\"\"\n        data = ChatRequest(\n            prompt=prompt,\n            history=history,\n            greedy=greedy\n        )\n        return self.forward_chat(data)\n\n    def process(self,\n                string: str,\n                *,\n                greedy: bool = False,\n                max_new_tokens: int = None,\n                **kwargs\n                ):\n        \"\"\"\n        The process function is the main function of a model. It takes in an input string and returns a list of strings\n        that are generated from that input string. The process function can be called multiple times with different inputs,\n        and each time it will return a new set of outputs based on those inputs.\n\n        :param self: Access the class attributes\n        :param string: str: Pass the string that we want to generate\n        :param *: Pass a variable number of arguments to a function\n        :param greedy: bool: Determine whether to use the greedy or non-greedy version of the generate function\n        :param max_new_tokens: int: Set the number of tokens to generate\n        :param **kwargs: Pass any additional parameters to the process function\n        :return: A generator that yields the predicted text and the number of tokens generated\n\n        \"\"\"\n        tokens = self.prefix_tokenizer(\n            string,\n            max_length=self.config.max_length - self.config.max_stream_tokens,\n            padding='max_length',\n            return_tensors='jax'\n        ) \\\n            if self.config.use_prefix_tokenizer else \\\n            self.tokenizer(\n                string,\n                return_tensors='jax'\n            )\n\n        input_ids = tokens.input_ids\n        attention_mask = tokens.attention_mask\n        num_generated_tokens = 0\n        pad = self.config.max_length - self.config.max_stream_tokens\n\n        for _ in range((max_new_tokens or self.config.max_new_tokens) // self.config.max_stream_tokens):\n            predicted_token = self.greedy_generate(\n                params=self.params,\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            ) if greedy else self.generate(\n                params=self.params,\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n\n            num_generated_tokens += predicted_token.shape[-1]\n\n            input_ids = jnp.concatenate(\n                (input_ids, predicted_token), axis=-1\n            )[:, -pad:]\n            attention_mask = jnp.concatenate(\n                (attention_mask, jnp.ones((len(attention_mask), self.config.max_stream_tokens), dtype=jnp.int32)),\n                axis=-1\n            )[:, -pad:]\n\n            yield self.tokenizer.decode(input_ids[0][-num_generated_tokens:],\n                                        skip_special_tokens=True), num_generated_tokens\n            if predicted_token[0][-1] == self.tokenizer.eos_token_id or predicted_token[0][\n                -1] == self.prefix_tokenizer.eos_token_id:\n                break\n\n    def process_gradio_chat(self, prompt, history, max_new_tokens, system, greedy):\n        \"\"\"\n        The process_gradio_chat function is a wrapper for the process function.\n        It takes in a prompt, history, max_new_tokens and system as arguments.\n        The string variable is set to the output of format_chat with the given history and prompt.\n        If stream tokens are not enabled then it will append an empty response to history and iterate through all responses from process until there are no more left (the last one). It will then return an empty string along with this new updated version of history. If stream tokens are enabled it appends an empty response to the end of our current list of histories (history) and iterates through\n\n        :param self: Refer to the object itself\n        :param prompt: Add the user's input to the history\n        :param history: Keep track of the conversation\n        :param max_new_tokens: Limit the number of tokens that can be generated by the model\n        :param system: Determine whether the message is from the user or system\n        :param greedy: Determine if the model should generate a response token by token or all at once\n        :return: A tuple of two values:\n\n        \"\"\"\n        string = self.format_chat(history=history, prompt=prompt, system=system)\n\n        if not self.config.stream_tokens_for_gradio:\n            response = ''\n            for response, _ in self.process(\n                    string=string,\n                    greedy=greedy,\n                    max_new_tokens=max_new_tokens,\n            ):\n                pass\n            history.append([prompt, response])\n        else:\n            history.append([prompt, ''])\n            for response, _ in self.process(\n                    string=string,\n                    greedy=greedy,\n                    max_new_tokens=max_new_tokens,\n            ):\n                history[-1][-1] = response\n                yield '', history\n        return '', history\n\n    def process_gradio_instruct(self, instruction, system, max_new_tokens, greedy):\n        \"\"\"\n        The process_gradio_instruct function is a wrapper for the process function.\n        It takes in an instruction and system, formats them into a string, and then passes that string to the process function.\n        The response from this call to process is returned as output.\n\n        :param self: Refer to the instance of the class\n        :param instruction: Pass in the instruction from the user\n        :param system: Determine which system to use for the instruction\n        :param max_new_tokens: Limit the number of new tokens that can be added to the vocabulary\n        :param greedy: Determine whether the model should be greedy or not\n        :return: A tuple of two strings:\n\n        \"\"\"\n        string = self.format_instruct(instruction=instruction, system=system)\n        if not self.config.stream_tokens_for_gradio:\n            response = ''\n            for response, _ in self.process(\n                    string=string,\n                    greedy=greedy,\n                    max_new_tokens=max_new_tokens,\n            ):\n                pass\n\n        else:\n            response = ''\n            for response, _ in self.process(\n                    string=string,\n                    greedy=greedy,\n                    max_new_tokens=max_new_tokens,\n                    stream=True\n            ):\n                yield '', response\n        return '', response\n\n    def create_gradio_ui_chat(self):\n        \"\"\"\n        The create_gradio_ui_chat function creates a Gradio UI for the chatbot.\n\n        :param self: Represent the instance of the class\n        :return: A block\n\n        \"\"\"\n        with gr.Blocks(\n                theme=seafoam) as block:\n            gr.Markdown(\"# &lt;h1&gt; &lt;center&gt;Powered by [EasyDeL](https://github.com/erfanzar/EasyDel) &lt;/center&gt; &lt;/h1&gt;\")\n            with gr.Row():\n                history = gr.Chatbot(elem_id=\"EasyDel\", label=\"EasyDel\", container=True, height=600)\n\n            with gr.Row():\n                with gr.Column():\n                    prompt = gr.Textbox(show_label=False, placeholder='Message Box', container=False)\n                with gr.Column():\n                    with gr.Row():\n                        submit = gr.Button(variant=\"primary\")\n                        stop = gr.Button(value='Stop ')\n                        clear = gr.Button(value='Clear Conversation')\n\n            with gr.Row():\n                with gr.Accordion('Advanced Options', open=False):\n                    max_new_tokens = gr.Slider(value=self.config.max_new_tokens, maximum=10000,\n                                               minimum=self.config.max_stream_tokens,\n                                               label='Max New Tokens', step=self.config.max_stream_tokens, )\n\n                    system = gr.Textbox(show_label=False, placeholder='System Prompt', container=False, value='')\n                    greedy = gr.Checkbox(value=False, label='Greedy Search')\n\n            inputs = [prompt, history, max_new_tokens, system, greedy]\n            sub_event = submit.click(fn=self.process_gradio_chat, inputs=inputs, outputs=[prompt, history])\n\n            def clear_():\n                return []\n\n            clear.click(fn=clear_, outputs=[history])\n            txt_event = prompt.submit(fn=self.process_gradio_chat, inputs=inputs, outputs=[prompt, history])\n\n            stop.click(fn=None, inputs=None, outputs=None, cancels=[txt_event, sub_event])\n\n        block.queue()\n        return block\n\n    def create_gradio_ui_instruct(self):\n        \"\"\"\n        The create_gradio_ui_instruct function creates a Gradio UI for the EasyDeL model.\n        The function takes in an instance of the EasyDeL class and returns a Gradio UI object.\n\n\n        :param self: Represent the instance of the class\n        :return: A block\n\n        \"\"\"\n        with gr.Blocks(\n                theme=seafoam) as block:\n            gr.Markdown(\"# &lt;h1&gt; &lt;center&gt;Powered by [EasyDeL](https://github.com/erfanzar/EasyDel) &lt;/center&gt; &lt;/h1&gt;\")\n            with gr.Row():\n                pred = gr.TextArea(elem_id=\"EasyDel\", label=\"EasyDel\", container=True)\n\n            with gr.Row():\n                submit = gr.Button(variant=\"primary\")\n                stop = gr.Button(value='Stop ')\n                clear = gr.Button(value='Clear Conversation')\n            with gr.Column():\n                prompt = gr.Textbox(show_label=False, placeholder='Instruct Message', container=False)\n\n            with gr.Row():\n                with gr.Accordion('Advanced Options', open=False):\n                    system = gr.Textbox(value='',\n                                        show_label=False, placeholder='System Message', container=False)\n                    max_new_tokens = gr.Slider(value=self.config.max_new_tokens, maximum=10000,\n                                               minimum=self.config.max_stream_tokens,\n                                               label='Max New Tokens', step=self.config.max_stream_tokens, )\n\n                    greedy = gr.Checkbox(value=False, label='Greedy Search')\n\n            inputs = [prompt, system, max_new_tokens, greedy]\n            sub_event = submit.click(fn=self.process_gradio_instruct, inputs=inputs, outputs=[prompt, pred])\n\n            def clear_():\n                return ''\n\n            clear.click(fn=clear_, outputs=[pred])\n            txt_event = prompt.submit(fn=self.process_gradio_instruct, inputs=inputs, outputs=[prompt, pred])\n\n            stop.click(fn=None, inputs=None, outputs=None, cancels=[txt_event, sub_event])\n\n        block.queue()\n        return block\n\n    def fire(self):\n        \"\"\"\n        The fire function is a wrapper around the uvicorn.run function that allows you to run your model in a separate process\n        from the main one. This is useful for running models on GPUs, as it prevents any other processes from using them while\n        the model is being served.\n\n        :param self: Refer to the instance of the class\n        :return: A process, which is a child of the main process\n\n        \"\"\"\n        assert self._funcs_generated, 'you have to first add your model and parameters into server before using fire ' \\\n                                      'with using ``configure_generate_functions``'\n\n        def run():\n            uvicorn.run(self.app, host=self.config.host, port=self.config.port)\n\n        self.process_uvicorn = mp.Process(target=run)\n        self.process_uvicorn.start()\n\n    def end(self):\n        \"\"\"\n        The end function is used to stop the server.\n            It will wait for the process to end before returning.\n\n        :param self: Represent the instance of the class\n        :return: The process_uvicorn\n\n        \"\"\"\n        if self.process_uvicorn is not None:\n            self.process_uvicorn.join()\n        else:\n            logging.warning('you have to fire server before ending that this command will be ignored')\n\n    def launch(self,\n               share_chat: bool = False,\n               share_inst: bool = False\n               ):\n        \"\"\"\n        The launch function is a wrapper for the launch function of the gradio.Interface class.\n        It takes two boolean arguments: share_chat and share_inst, which are used to determine whether to\n        share the chatbot interface and/or instruction interface respectively. If both are set to False, then neither\n        interface will be shared (this is useful if you want to run your app locally). If one or both of them are True,\n        then they will be shared on Gradio's servers with a unique URL that can be accessed by anyone in order for them\n        to interact with your app.\n\n        :param self: Represent the instance of the class\n        :param share_chat: bool: Determine if the chatbot should be shared\n        :param share_inst: bool: Share the instructions for the app\n        :return: A dictionary with the share urls for chat and instructions\n\n        \"\"\"\n        share_kwargs = {}\n        assert not share_chat or not share_inst, 'you have to pass at least one of sharing options True'\n        if share_chat:\n            self.gradio_app_chat.launch(share=True)\n            share_kwargs['chat'] = self.gradio_app_chat.share_url\n        if share_inst:\n            self.gradio_app_instruct.launch(share=True)\n            share_kwargs['inst'] = self.gradio_app_instruct.share_url\n        return share_kwargs\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.__init__","title":"<code>__init__(config=None)</code>","text":"<p>The init function is called when the class is instantiated. It sets up all the attributes that will be used by other methods in the class.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the current instance of a class</p> required <code>config</code> <p>Pass the jaxserverconfig object</p> <code>None</code> <p>Returns:</p> Type Description <p>A fastapi object</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def __init__(self, config=None):\n\n    \"\"\"\n    The __init__ function is called when the class is instantiated.\n    It sets up all the attributes that will be used by other methods in the class.\n\n\n    :param self: Refer to the current instance of a class\n    :param config: Pass the jaxserverconfig object\n    :return: A fastapi object\n\n    \"\"\"\n    self.process_uvicorn, self.prefix_tokenizer, self.params, self.tokenizer, self.model, \\\n        self.rules, self._generate, self._greedy_generate = [None] * 8\n    assert config is None or isinstance(config, JaxServerConfig), 'config can be None or JaxServerConfig Type'\n    if config is None:\n        self.config = JaxServerConfig()\n    else:\n        self.config = config\n    self._funcs_generated = False\n    self.number_of_served_request_until_last_up_time = 0\n\n    self.rng_generator = RNG(42)\n    initialise_tracking(0.5)\n    array = jnp.ones((len(jax.devices()), 1)).reshape(self.config.mesh_axes_shape)\n    self.mesh = Mesh(mesh_utils.create_device_mesh(array.shape), self.config.mesh_axes_names)\n\n    self.app = FastAPI()\n    self.app.post('/chat')(self.forward_chat)\n    self.app.post('/instruct')(self.forward_instruct)\n    self.app.get('/status')(self.status)\n    self.gradio_app_chat = self.create_gradio_ui_chat()\n    self.gradio_app_instruct = self.create_gradio_ui_instruct()\n    self.app = gr.mount_gradio_app(self.app, self.gradio_app_chat, '/gradio_chat')\n    self.app = gr.mount_gradio_app(self.app, self.gradio_app_instruct, '/gradio_instruct')\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.auto_configure","title":"<code>auto_configure(model, params, tokenizer, partition_rules)</code>","text":"<p>The auto_configure function is a helper function that will automatically configure the model for distributed training. It does this by:     1) sharding the parameters of the model based on partition_rules, and then     2) configuring generate functions to be used in distributed training.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>model</code> <p>Configure the model</p> required <code>params</code> <p>Store the parameters that are used to configure the model</p> required <code>tokenizer</code> <p>Tokenize the input text</p> required <code>partition_rules</code> <p>Specify how the parameters should be partitioned</p> required <p>Returns:</p> Type Description <p>A dictionary with the following keys:</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def auto_configure(self, model, params, tokenizer, partition_rules):\n    \"\"\"\n    The auto_configure function is a helper function that will automatically configure the model for distributed training.\n    It does this by:\n        1) sharding the parameters of the model based on partition_rules, and then\n        2) configuring generate functions to be used in distributed training.\n\n    :param self: Represent the instance of the class\n    :param model: Configure the model\n    :param params: Store the parameters that are used to configure the model\n    :param tokenizer: Tokenize the input text\n    :param partition_rules: Specify how the parameters should be partitioned\n    :return: A dictionary with the following keys:\n\n    \"\"\"\n    self.shard_params(params=params, partition_rules=partition_rules)\n    self.configure_generate_functions(model, tokenizer)\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.compile","title":"<code>compile(verbose=True)</code>","text":"<p>The compile function is used to compile the model for use in inference. It does this by running through all possible combinations of rules and actions, and compiling them into functions that can be called later on during inference. This allows us to avoid having to recompile the model every time we want to run it, which would be very slow.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>verbose</code> <code>bool</code> <p>bool: Print out the compiling process</p> <code>True</code> <p>Returns:</p> Type Description <code>bool</code> <p>True, but what does it do?</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def compile(self, verbose: bool = True) -&gt; bool:\n    \"\"\"\n    The compile function is used to compile the model for use in inference.\n    It does this by running through all possible combinations of rules and actions,\n    and compiling them into functions that can be called later on during inference.\n    This allows us to avoid having to recompile the model every time we want to run it,\n    which would be very slow.\n\n    :param self: Represent the instance of the class\n    :param verbose: bool: Print out the compiling process\n    :return: True, but what does it do?\n\n    \"\"\"\n    assert self._funcs_generated, 'funcs are not generated yet'\n    assert self.rules is not None, 'rules should not be None'\n    if self.config.use_prefix_tokenizer:\n        if verbose:\n            print('\\033[1;91mCompiling Model Forwards Greedy/NonGreedy(Generate)')\n            print('Compiling Greedy Funcs')\n\n        r, a = [None] * 2\n        for r, a in self.process(\n                string='',\n                max_new_tokens=self.config.max_stream_tokens,\n                greedy=True\n        ):\n            ...\n        print('Compiling NonGreedy(Generate) Funcs\\033[1;0m')\n        for r, a in self.process(\n                string='',\n                max_new_tokens=self.config.max_stream_tokens,\n                greedy=False\n        ):\n            ...\n\n    else:\n        print(\n            '\\033[1;91mSkip Compiling the compiling process is useless '\n            'when you are not using prefix tokenizer\\033[1;0m')\n    return True\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.configure_generate_functions","title":"<code>configure_generate_functions(model, tokenizer)</code>","text":"<p>The configure_generate_functions function is used to configure the generation functions for a given model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables within the class</p> required <code>model</code> <p>Generate the model</p> required <code>tokenizer</code> <p>Get the eos_token_id, pad_token_id and bos token id</p> required <p>Returns:</p> Type Description <p>A function that takes in three parameters:</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def configure_generate_functions(self, model, tokenizer):\n\n    \"\"\"\n    The configure_generate_functions function is used to configure the generation functions for a given model.\n\n    :param self: Access variables within the class\n    :param model: Generate the model\n    :param tokenizer: Get the eos_token_id, pad_token_id and bos token id\n    :return: A function that takes in three parameters:\n\n    \"\"\"\n    assert self.rules is not None, 'you should first shard params with using ``shard_params`` method'\n\n    if tokenizer.pad_token is None:\n        logging.info(\n            'Tokenizer does not contain padding token setting padding token to eos token for open end generation')\n        tokenizer.pad_token = tokenizer.eos_token\n\n    try:\n        tokenizer.padding_side = 'left'\n        tokenizer.truncation_side = 'left'\n        self.prefix_tokenizer = copy.deepcopy(tokenizer)\n        tokenizer.padding_side = 'right'\n        tokenizer.truncation_side = 'right'\n        self.tokenizer = copy.deepcopy(tokenizer)\n    except:\n        prefix_str(\n            'Warning', f'The class Model of Tokenizer {type(tokenizer)} do not support deepcopy option '\n        )\n        if self.config.use_prefix_tokenizer:\n            tokenizer.padding_side = 'left'\n            tokenizer.truncation_side = 'left'\n        else:\n            tokenizer.padding_side = 'right'\n            tokenizer.truncation_side = 'right'\n        self.prefix_tokenizer = tokenizer\n\n    @functools.partial(\n        pjit,\n        in_shardings=(self.rules, Ps(), Ps()),\n        out_shardings=(Ps())\n    )\n    def greedy_generate(parameters, input_ids, attention_mask):\n        input_ids = with_sharding_constraint(input_ids, Ps(('dp', 'fsdp')))\n        attention_mask = with_sharding_constraint(attention_mask, Ps(('dp', 'fsdp')))\n        predict = model.generate(\n            input_ids,\n            attention_mask=attention_mask,\n            params=parameters,\n            generation_config=GenerationConfig(\n                max_new_tokens=self.config.max_stream_tokens,\n                eos_token_id=tokenizer.eos_token_id,\n                pad_token_id=tokenizer.pad_token_id,\n                bos_token_id=tokenizer.bos_token_id,\n\n                do_sample=False,\n                num_beams=1,\n            )\n        ).sequences[:, input_ids.shape[1]:]\n        return predict\n\n    @functools.partial(\n        pjit,\n        in_shardings=(self.rules, Ps(), Ps()),\n        out_shardings=(Ps())\n    )\n    def generate(parameters, input_ids, attention_mask):\n        input_ids = with_sharding_constraint(input_ids, Ps(('dp', 'fsdp')))\n        attention_mask = with_sharding_constraint(attention_mask, Ps(('dp', 'fsdp')))\n        predict = model.generate(\n            input_ids,\n            attention_mask=attention_mask,\n            params=parameters,\n            generation_config=GenerationConfig(\n                max_new_tokens=self.config.max_stream_tokens,\n\n                eos_token_id=tokenizer.eos_token_id,\n                pad_token_id=tokenizer.pad_token_id,\n                bos_token_id=tokenizer.bos_token_id,\n\n                temperature=self.config.temperature,\n                do_sample=True,\n                num_beams=1,\n                top_p=self.config.top_p,\n                top_k=self.config.top_k,\n            )\n        ).sequences[:, input_ids.shape[1]:]\n        return predict\n\n    self._generate = generate\n    self._greedy_generate = greedy_generate\n    self._funcs_generated = True\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.create_gradio_ui_chat","title":"<code>create_gradio_ui_chat()</code>","text":"<p>The create_gradio_ui_chat function creates a Gradio UI for the chatbot.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A block</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def create_gradio_ui_chat(self):\n    \"\"\"\n    The create_gradio_ui_chat function creates a Gradio UI for the chatbot.\n\n    :param self: Represent the instance of the class\n    :return: A block\n\n    \"\"\"\n    with gr.Blocks(\n            theme=seafoam) as block:\n        gr.Markdown(\"# &lt;h1&gt; &lt;center&gt;Powered by [EasyDeL](https://github.com/erfanzar/EasyDel) &lt;/center&gt; &lt;/h1&gt;\")\n        with gr.Row():\n            history = gr.Chatbot(elem_id=\"EasyDel\", label=\"EasyDel\", container=True, height=600)\n\n        with gr.Row():\n            with gr.Column():\n                prompt = gr.Textbox(show_label=False, placeholder='Message Box', container=False)\n            with gr.Column():\n                with gr.Row():\n                    submit = gr.Button(variant=\"primary\")\n                    stop = gr.Button(value='Stop ')\n                    clear = gr.Button(value='Clear Conversation')\n\n        with gr.Row():\n            with gr.Accordion('Advanced Options', open=False):\n                max_new_tokens = gr.Slider(value=self.config.max_new_tokens, maximum=10000,\n                                           minimum=self.config.max_stream_tokens,\n                                           label='Max New Tokens', step=self.config.max_stream_tokens, )\n\n                system = gr.Textbox(show_label=False, placeholder='System Prompt', container=False, value='')\n                greedy = gr.Checkbox(value=False, label='Greedy Search')\n\n        inputs = [prompt, history, max_new_tokens, system, greedy]\n        sub_event = submit.click(fn=self.process_gradio_chat, inputs=inputs, outputs=[prompt, history])\n\n        def clear_():\n            return []\n\n        clear.click(fn=clear_, outputs=[history])\n        txt_event = prompt.submit(fn=self.process_gradio_chat, inputs=inputs, outputs=[prompt, history])\n\n        stop.click(fn=None, inputs=None, outputs=None, cancels=[txt_event, sub_event])\n\n    block.queue()\n    return block\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.create_gradio_ui_instruct","title":"<code>create_gradio_ui_instruct()</code>","text":"<p>The create_gradio_ui_instruct function creates a Gradio UI for the EasyDeL model. The function takes in an instance of the EasyDeL class and returns a Gradio UI object.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A block</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def create_gradio_ui_instruct(self):\n    \"\"\"\n    The create_gradio_ui_instruct function creates a Gradio UI for the EasyDeL model.\n    The function takes in an instance of the EasyDeL class and returns a Gradio UI object.\n\n\n    :param self: Represent the instance of the class\n    :return: A block\n\n    \"\"\"\n    with gr.Blocks(\n            theme=seafoam) as block:\n        gr.Markdown(\"# &lt;h1&gt; &lt;center&gt;Powered by [EasyDeL](https://github.com/erfanzar/EasyDel) &lt;/center&gt; &lt;/h1&gt;\")\n        with gr.Row():\n            pred = gr.TextArea(elem_id=\"EasyDel\", label=\"EasyDel\", container=True)\n\n        with gr.Row():\n            submit = gr.Button(variant=\"primary\")\n            stop = gr.Button(value='Stop ')\n            clear = gr.Button(value='Clear Conversation')\n        with gr.Column():\n            prompt = gr.Textbox(show_label=False, placeholder='Instruct Message', container=False)\n\n        with gr.Row():\n            with gr.Accordion('Advanced Options', open=False):\n                system = gr.Textbox(value='',\n                                    show_label=False, placeholder='System Message', container=False)\n                max_new_tokens = gr.Slider(value=self.config.max_new_tokens, maximum=10000,\n                                           minimum=self.config.max_stream_tokens,\n                                           label='Max New Tokens', step=self.config.max_stream_tokens, )\n\n                greedy = gr.Checkbox(value=False, label='Greedy Search')\n\n        inputs = [prompt, system, max_new_tokens, greedy]\n        sub_event = submit.click(fn=self.process_gradio_instruct, inputs=inputs, outputs=[prompt, pred])\n\n        def clear_():\n            return ''\n\n        clear.click(fn=clear_, outputs=[pred])\n        txt_event = prompt.submit(fn=self.process_gradio_instruct, inputs=inputs, outputs=[prompt, pred])\n\n        stop.click(fn=None, inputs=None, outputs=None, cancels=[txt_event, sub_event])\n\n    block.queue()\n    return block\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.end","title":"<code>end()</code>","text":"<p>The end function is used to stop the server.     It will wait for the process to end before returning.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>The process_uvicorn</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def end(self):\n    \"\"\"\n    The end function is used to stop the server.\n        It will wait for the process to end before returning.\n\n    :param self: Represent the instance of the class\n    :return: The process_uvicorn\n\n    \"\"\"\n    if self.process_uvicorn is not None:\n        self.process_uvicorn.join()\n    else:\n        logging.warning('you have to fire server before ending that this command will be ignored')\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.fire","title":"<code>fire()</code>","text":"<p>The fire function is a wrapper around the uvicorn.run function that allows you to run your model in a separate process from the main one. This is useful for running models on GPUs, as it prevents any other processes from using them while the model is being served.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the instance of the class</p> required <p>Returns:</p> Type Description <p>A process, which is a child of the main process</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def fire(self):\n    \"\"\"\n    The fire function is a wrapper around the uvicorn.run function that allows you to run your model in a separate process\n    from the main one. This is useful for running models on GPUs, as it prevents any other processes from using them while\n    the model is being served.\n\n    :param self: Refer to the instance of the class\n    :return: A process, which is a child of the main process\n\n    \"\"\"\n    assert self._funcs_generated, 'you have to first add your model and parameters into server before using fire ' \\\n                                  'with using ``configure_generate_functions``'\n\n    def run():\n        uvicorn.run(self.app, host=self.config.host, port=self.config.port)\n\n    self.process_uvicorn = mp.Process(target=run)\n    self.process_uvicorn.start()\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.format_chat","title":"<code>format_chat(history, prompt, system)</code>  <code>staticmethod</code>","text":"<p>Here you will get the system, prompt and history from user, and you can apply your prompting style</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>@staticmethod\ndef format_chat(history: typing.List[str], prompt: str, system: typing.Union[str, None]) -&gt; str:\n    \"\"\"\n    Here you will get the system, prompt and history from user, and you can apply your prompting style\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.format_instruct","title":"<code>format_instruct(system, instruction)</code>  <code>staticmethod</code>","text":"<p>Here you will get the system and instruction from user, and you can apply your prompting style</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>@staticmethod\ndef format_instruct(system: str, instruction: str) -&gt; str:\n    \"\"\"\n    Here you will get the system and instruction from user, and you can apply your prompting style\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.forward_chat","title":"<code>forward_chat(data)</code>","text":"<p>The forward_chat function is the main function of this class. It takes in a ChatRequest object, which contains a prompt and history. The prompt is the user's input to be processed by the chatbot, while history is an array of previous inputs and outputs from both sides (user and bot). The forward_chat function then formats these inputs into one string that can be processed by our model. This formatted string is then passed through our process() method, which returns an output response as well as how many tokens were used to generate it.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the attributes and methods of the class</p> required <code>data</code> <code>ChatRequest</code> <p>ChatRequest: Pass in the data from the request</p> required <p>Returns:</p> Type Description <p>A dictionary with the following keys:</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def forward_chat(self, data: ChatRequest):\n\n    \"\"\"\n    The forward_chat function is the main function of this class.\n    It takes in a ChatRequest object, which contains a prompt and history.\n    The prompt is the user's input to be processed by the chatbot, while history\n    is an array of previous inputs and outputs from both sides (user and bot).\n    The forward_chat function then formats these inputs into one string that can be processed by our model.\n    This formatted string is then passed through our process() method, which returns an output response as well as how many tokens were used to generate it.\n\n    :param self: Access the attributes and methods of the class\n    :param data: ChatRequest: Pass in the data from the request\n    :return: A dictionary with the following keys:\n\n    \"\"\"\n    if not self._funcs_generated:\n        return {\n            'status': \"down\"\n        }\n\n    string = self.format_chat(\n        prompt=data.prompt,\n        system=None,\n        history=data.history\n    )\n\n    response, used_tokens = [None] * 2\n    for response, used_tokens in self.process(\n            string=string,\n            greedy=data.greedy,\n            max_new_tokens=None\n    ):\n        ...\n    self.number_of_served_request_until_last_up_time += 1\n    return {\n        'input': f'{string}',\n        'response': response,\n        'tokens_used': used_tokens,\n    }\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.forward_chat_non_api","title":"<code>forward_chat_non_api(prompt, history, greedy)</code>","text":"<p>The forward_chat_non_api function is a wrapper for the forward_chat function. It takes in a prompt, history, and greedy parameter and returns the response from the forward_chat function. The purpose of this wrapper is to allow users to use the chatbot without having to create ChatRequest objects.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>prompt</code> <p>Pass the user's input to the model</p> required <code>history</code> <p>Pass the history of the conversation to the model</p> required <code>greedy</code> <p>Determine whether the model should use a greedy search</p> required <p>Returns:</p> Type Description <p>A chat-response object</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def forward_chat_non_api(self, prompt, history, greedy):\n    \"\"\"\n    The forward_chat_non_api function is a wrapper for the forward_chat function.\n    It takes in a prompt, history, and greedy parameter and returns the response from\n    the forward_chat function. The purpose of this wrapper is to allow users to use\n    the chatbot without having to create ChatRequest objects.\n\n    :param self: Represent the instance of the class\n    :param prompt: Pass the user's input to the model\n    :param history: Pass the history of the conversation to the model\n    :param greedy: Determine whether the model should use a greedy search\n    :return: A chat-response object\n\n    \"\"\"\n    data = ChatRequest(\n        prompt=prompt,\n        history=history,\n        greedy=greedy\n    )\n    return self.forward_chat(data)\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.forward_instruct","title":"<code>forward_instruct(data)</code>","text":"<p>The forward_instruct function is the main function of this class. It takes in a InstructRequest object, which contains the system and instruction to be processed. The function then formats the input string using format_instruct, and passes it into process(). process() returns a tuple containing (response, used_tokens). The response is returned as part of the response dictionary. If no valid responses are found by process(), None will be returned instead.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Bind the method to the object</p> required <code>data</code> <code>InstructRequest</code> <p>InstructRequest: Pass the system and instruction to the function</p> required <p>Returns:</p> Type Description <p>A dictionary with three keys:</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def forward_instruct(self, data: InstructRequest):\n    \"\"\"\n    The forward_instruct function is the main function of this class.\n    It takes in a InstructRequest object, which contains the system and instruction to be processed.\n    The function then formats the input string using format_instruct, and passes it into process().\n    process() returns a tuple containing (response, used_tokens). The response is returned as part of\n    the response dictionary. If no valid responses are found by process(), None will be returned instead.\n\n    :param self: Bind the method to the object\n    :param data: InstructRequest: Pass the system and instruction to the function\n    :return: A dictionary with three keys:\n\n    \"\"\"\n    if not self._funcs_generated:\n        return {\n            'status': \"down\"\n        }\n\n    response, used_tokens = [None] * 2\n    string = self.format_instruct(\n        system=data.system,\n        instruction=data.instruction\n    )\n    for response, used_tokens in self.process(\n            string=string,\n            greedy=data.greedy,\n            max_new_tokens=None\n    ):\n        ...\n    self.number_of_served_request_until_last_up_time += 1\n    return {\n        'input': f'{string}',\n        'response': response,\n        'tokens_used': used_tokens,\n    }\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.forward_instruct_non_api","title":"<code>forward_instruct_non_api(prompt, system, greedy)</code>","text":"<p>The forward_instruct_non_api function is a wrapper for the forward_instruct function. It takes in a prompt, system, and greedy flag as arguments and returns the response from the forward_instruct function. The purpose of this wrapper is to allow users to call forward_instruct without having to create an InstructRequest object.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>prompt</code> <p>Pass the instruction to the system</p> required <code>system</code> <p>Specify which system to use for the instruction</p> required <code>greedy</code> <p>Determine whether the system should return</p> required <p>Returns:</p> Type Description <p>The response from the forward_instruct function</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def forward_instruct_non_api(self, prompt, system, greedy):\n    \"\"\"\n    The forward_instruct_non_api function is a wrapper for the forward_instruct function.\n    It takes in a prompt, system, and greedy flag as arguments and returns the response from\n    the forward_instruct function. The purpose of this wrapper is to allow users to call\n    forward_instruct without having to create an InstructRequest object.\n\n    :param self: Represent the instance of the class\n    :param prompt: Pass the instruction to the system\n    :param system: Specify which system to use for the instruction\n    :param greedy: Determine whether the system should return\n    :return: The response from the forward_instruct function\n\n    \"\"\"\n    data = InstructRequest(\n        prompt=prompt,\n        system=system,\n        greedy=greedy\n    )\n    return self.forward_instruct(data)\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.generate","title":"<code>generate(params, input_ids, attention_mask)</code>","text":"<p>The generate function is used to generate a sequence of tokens from the model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access variables that belong to the class</p> required <code>params</code> <code>Union[FrozenDict, dict]</code> <p>Union[flax.core.FrozenDict, dict]: Pass the parameters of the model to be used in generating text</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass the input to the model</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask the padding tokens</p> required <p>Returns:</p> Type Description <p>The logits of the model</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def generate(self,\n             params: Union[flax.core.FrozenDict, dict],\n             input_ids: chex.Array,\n             attention_mask: chex.Array,\n             ):\n    \"\"\"\n    The generate function is used to generate a sequence of tokens from the model.\n\n    :param self: Access variables that belong to the class\n    :param params: Union[flax.core.FrozenDict, dict]: Pass the parameters of the model to be used in generating text\n    :param input_ids: chex.Array: Pass the input to the model\n    :param attention_mask: chex.Array: Mask the padding tokens\n    :return: The logits of the model\n\n    \"\"\"\n    if not self._funcs_generated:\n        raise NotImplementedError(\n            'this method will be implemented automatically after using ``configure_generate_functions`` function'\n        )\n    else:\n        with self.mesh:\n            return self._generate(\n                params, input_ids, attention_mask\n            )\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.get_memory","title":"<code>get_memory()</code>  <code>staticmethod</code>","text":"<p>The get_memory function returns the total memory of the system in bytes.</p> <p>Returns:</p> Type Description <p>The amount of memory used by the program</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>@staticmethod\ndef get_memory():\n    \"\"\"\n    The get_memory function returns the total memory of the system in bytes.\n\n\n    :return: The amount of memory used by the program\n\n    \"\"\"\n    return get_mem()\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.greedy_generate","title":"<code>greedy_generate(params, input_ids, attention_mask)</code>","text":"<p>The greedy_generate function is a helper function that takes in the model parameters, input_ids and attention_mask and returns the generated tokens. It uses greedy search to generate tokens one at a time.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>params</code> <code>Union[FrozenDict, dict]</code> <p>Union[flax.core.FrozenDict, dict]: Pass the parameters to the model</p> required <code>input_ids</code> <code>Array</code> <p>chex.Array: Pass in the input sequence</p> required <code>attention_mask</code> <code>Array</code> <p>chex.Array: Mask the input tokens</p> required <code></code> <p>Specify the parameters of the model</p> required <p>Returns:</p> Type Description <p>generated_ids</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def greedy_generate(self,\n                    params: Union[flax.core.FrozenDict, dict],\n                    input_ids: chex.Array,\n                    attention_mask: chex.Array,\n                    ):\n    \"\"\"\n    The greedy_generate function is a helper function that takes in the model parameters, input_ids and attention_mask\n    and returns the generated tokens. It uses greedy search to generate tokens one at a time.\n\n\n    :param self: Refer to the object itself\n    :param params: Union[flax.core.FrozenDict, dict]: Pass the parameters to the model\n    :param input_ids: chex.Array: Pass in the input sequence\n    :param attention_mask: chex.Array: Mask the input tokens\n    :param : Specify the parameters of the model\n    :return:  generated_ids\n\n    \"\"\"\n    if not self._funcs_generated:\n        raise NotImplementedError(\n            'this method will be implemented automatically after using ``configure_generate_functions`` function'\n        )\n    else:\n        with self.mesh:\n            return self._greedy_generate(\n                params, input_ids, attention_mask\n            )\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.launch","title":"<code>launch(share_chat=False, share_inst=False)</code>","text":"<p>The launch function is a wrapper for the launch function of the gradio.Interface class. It takes two boolean arguments: share_chat and share_inst, which are used to determine whether to share the chatbot interface and/or instruction interface respectively. If both are set to False, then neither interface will be shared (this is useful if you want to run your app locally). If one or both of them are True, then they will be shared on Gradio's servers with a unique URL that can be accessed by anyone in order for them to interact with your app.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>share_chat</code> <code>bool</code> <p>bool: Determine if the chatbot should be shared</p> <code>False</code> <code>share_inst</code> <code>bool</code> <p>bool: Share the instructions for the app</p> <code>False</code> <p>Returns:</p> Type Description <p>A dictionary with the share urls for chat and instructions</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def launch(self,\n           share_chat: bool = False,\n           share_inst: bool = False\n           ):\n    \"\"\"\n    The launch function is a wrapper for the launch function of the gradio.Interface class.\n    It takes two boolean arguments: share_chat and share_inst, which are used to determine whether to\n    share the chatbot interface and/or instruction interface respectively. If both are set to False, then neither\n    interface will be shared (this is useful if you want to run your app locally). If one or both of them are True,\n    then they will be shared on Gradio's servers with a unique URL that can be accessed by anyone in order for them\n    to interact with your app.\n\n    :param self: Represent the instance of the class\n    :param share_chat: bool: Determine if the chatbot should be shared\n    :param share_inst: bool: Share the instructions for the app\n    :return: A dictionary with the share urls for chat and instructions\n\n    \"\"\"\n    share_kwargs = {}\n    assert not share_chat or not share_inst, 'you have to pass at least one of sharing options True'\n    if share_chat:\n        self.gradio_app_chat.launch(share=True)\n        share_kwargs['chat'] = self.gradio_app_chat.share_url\n    if share_inst:\n        self.gradio_app_instruct.launch(share=True)\n        share_kwargs['inst'] = self.gradio_app_instruct.share_url\n    return share_kwargs\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.load","title":"<code>load(model, config_model, tokenizer, path, config=None, add_params_field=True, init_shape=(1, 1), do_memory_log=False, verbose=True)</code>  <code>classmethod</code>","text":"<p>The load function is used to load a pretrained model from disk.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>Refer to the class itself</p> required <code>model</code> <code>FlaxPreTrainedModel</code> <p>transformers.FlaxPreTrainedModel: Initialize the server</p> required <code>config_model</code> <code>PretrainedConfig</code> <p>transformers.PretrainedConfig: Get the partition rules</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>transformers.PreTrainedTokenizer: Load the tokenizer from the model</p> required <code>path</code> <code>Union[str, PathLike]</code> <p>typing.Union[str, os.PathLike]: Specify the path to the checkpoint file</p> required <code>config</code> <p>Configure the server</p> <code>None</code> <code>add_params_field</code> <code>bool</code> <p>bool: Add a params field to the server</p> <code>True</code> <code>init_shape</code> <code>tuple</code> <p>tuple: Specify the shape of the input to be used for generating shard_fns</p> <code>(1, 1)</code> <code>do_memory_log</code> <code>bool</code> <p>bool: Log the memory usage of the server</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>bool: Print the compilation process</p> <code>True</code> <p>Returns:</p> Type Description <p>A server</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>@classmethod\ndef load(\n        cls,\n        model: transformers.FlaxPreTrainedModel,\n        config_model: transformers.PretrainedConfig,\n        tokenizer: transformers.PreTrainedTokenizer,\n        path: typing.Union[str, os.PathLike],\n        config=None,\n        add_params_field: bool = True,\n        init_shape: tuple = (1, 1),\n        do_memory_log: bool = False,\n        verbose: bool = True\n):\n    \"\"\"\n    The load function is used to load a pretrained model from disk.\n\n    :param cls: Refer to the class itself\n    :param model: transformers.FlaxPreTrainedModel: Initialize the server\n    :param config_model: transformers.PretrainedConfig: Get the partition rules\n    :param tokenizer: transformers.PreTrainedTokenizer: Load the tokenizer from the model\n    :param path: typing.Union[str, os.PathLike]: Specify the path to the checkpoint file\n    :param config: Configure the server\n    :param add_params_field: bool: Add a params field to the server\n    :param init_shape: tuple: Specify the shape of the input to be used for generating shard_fns\n    :param do_memory_log: bool: Log the memory usage of the server\n    :param verbose: bool: Print the compilation process\n    :return: A server\n\n    \"\"\"\n    assert hasattr(model,\n                   'init_weights'), 'model must contain init_weights func in order to init params for shard_fns'\n    assert hasattr(config_model,\n                   'get_partition_rules'), 'config_model must contain get_partition_rules functions'\n    server = cls(config=config)\n    logging.info(\n        'running _init() func in order to make shard_fns'\n    )\n    with jax.default_device(jax.devices('cpu')[0]):\n        def _init():\n            return model.init_weights(jax.random.PRNGKey(0), init_shape)\n\n        shape = jax.eval_shape(_init)\n    logging.info(\n        'matching partition rules'\n    )\n    rules = match_partition_rules(params=shape, rules=config_model.get_partition_rules(True))\n\n    with server.mesh:\n        shard_fns, _ = make_shard_and_gather_fns(rules, get_float_dtype_by_name(server.config.dtype))\n        logging.info(\n            'loading checkpoints'\n        )\n\n        shard_fns = flax.traverse_util.flatten_dict(shard_fns)\n        server.params = {}\n        with open(path, 'rb') as stream:\n            unpacker = msgpack.Unpacker(stream, read_size=83886080, max_buffer_size=0)\n            pbar = tqdm.tqdm(unpacker)\n            for key, value in pbar:\n                key = tuple(key)\n                tensor = from_bytes(None, value)\n                tensor = shard_fns[key](tensor)\n                server.params[key] = tensor\n                if do_memory_log:\n                    pbar.write(server.get_memory())\n                pbar.set_description('Sharding Params')\n    server.params = flax.traverse_util.unflatten_dict(server.params)\n    server.params = {'params': server.params} if add_params_field else server.params\n\n    server.rules = {'params': rules} if add_params_field else rules\n    logging.info(\n        'configuring generate functions for the server'\n    )\n    server.configure_generate_functions(model, tokenizer)\n\n    if server.config.pre_compile:\n        server.compile(verbose=verbose)\n    return server\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.load_from_params","title":"<code>load_from_params(model, config_model, tokenizer, params, config=None, add_params_field=True, do_memory_log=False, verbose=True)</code>  <code>classmethod</code>","text":"<p>The load_from_params function is used to load a model from the parameters of a pretrained model. It takes in the following arguments:     - cls: The class of the server you are loading, this should be Server or TPU_Server depending on what backend you want to use.     - model: A FlaxPreTrainedModel object that contains all of your models functions and parameters. This can be found in transformers/flax_utils/models/model.py         where model is replaced with whatever transformer you are using (e.g., bert). You can also create your own custom</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <p>Create a new instance of the class</p> required <code>model</code> <code>FlaxPreTrainedModel</code> <p>transformers.FlaxPreTrainedModel: Load the model</p> required <code>config_model</code> <code>PretrainedConfig</code> <p>transformers.PretrainedConfig: Get the partition rules</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>transformers.PreTrainedTokenizer: Tokenize the input text</p> required <code>params</code> <code>Dict</code> <p>typing.Dict: Pass in the parameters of the model</p> required <code>config</code> <p>Pass in the config file for the server</p> <code>None</code> <code>add_params_field</code> <code>bool</code> <p>bool: Add a params field to the server</p> <code>True</code> <code>do_memory_log</code> <code>bool</code> <p>bool: Log the memory usage of the server</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>bool: Print out the status of the compilation</p> <code>True</code> <p>Returns:</p> Type Description <p>A server object</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>@classmethod\ndef load_from_params(\n        cls,\n        model: transformers.FlaxPreTrainedModel,\n        config_model: transformers.PretrainedConfig,\n        tokenizer: transformers.PreTrainedTokenizer,\n        params: typing.Dict,\n        config=None,\n        add_params_field: bool = True,\n        do_memory_log: bool = False,\n        verbose: bool = True\n):\n    \"\"\"\n    The load_from_params function is used to load a model from the parameters of a pretrained model.\n    It takes in the following arguments:\n        - cls: The class of the server you are loading, this should be Server or TPU_Server depending on what backend you want to use.\n        - model: A FlaxPreTrainedModel object that contains all of your models functions and parameters. This can be found in transformers/flax_utils/models/*model*.py\n            where *model* is replaced with whatever transformer you are using (e.g., bert). You can also create your own custom\n\n    :param cls: Create a new instance of the class\n    :param model: transformers.FlaxPreTrainedModel: Load the model\n    :param config_model: transformers.PretrainedConfig: Get the partition rules\n    :param tokenizer: transformers.PreTrainedTokenizer: Tokenize the input text\n    :param params: typing.Dict: Pass in the parameters of the model\n    :param config: Pass in the config file for the server\n    :param add_params_field: bool: Add a params field to the server\n    :param do_memory_log: bool: Log the memory usage of the server\n    :param verbose: bool: Print out the status of the compilation\n    :return: A server object\n\n    \"\"\"\n    assert hasattr(model,\n                   'init_weights'), 'model must contain init_weights func in order to init params for shard_fns'\n    assert hasattr(config_model,\n                   'get_partition_rules'), 'config_model must contain get_partition_rules functions'\n    server = cls(config=config)\n\n    with server.mesh:\n        logging.info(\n            'matching partition rules'\n        )\n        rules = match_partition_rules(params=params, rules=config_model.get_partition_rules(True))\n        shard_fns, _ = make_shard_and_gather_fns(rules, get_float_dtype_by_name(server.config.dtype))\n        logging.info(\n            'sharding parameters across all of the chosen backend(tpu/gpu/cpu)s'\n        )\n        params = flax.traverse_util.flatten_dict(params)\n        shard_fns = flax.traverse_util.flatten_dict(shard_fns)\n        pbar = tqdm.tqdm(params.keys())\n        for key in pbar:\n\n            key = tuple(key)\n            params[key] = shard_fns[key](params[key])\n\n            if do_memory_log:\n                pbar.write(server.get_memory())\n            pbar.set_description('Sharding Params')\n        server.params = flax.traverse_util.unflatten_dict(params)\n        server.params = {'params': server.params} if add_params_field else server.params\n    server.rules = {'params': rules} if add_params_field else rules\n    logging.info(\n        'configuring generate functions for the server'\n    )\n    server.configure_generate_functions(model, tokenizer)\n    if server.config.pre_compile:\n        server.compile(verbose=verbose)\n    return server\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.process","title":"<code>process(string, *, greedy=False, max_new_tokens=None, **kwargs)</code>","text":"<p>The process function is the main function of a model. It takes in an input string and returns a list of strings that are generated from that input string. The process function can be called multiple times with different inputs, and each time it will return a new set of outputs based on those inputs.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the class attributes</p> required <code>string</code> <code>str</code> <p>str: Pass the string that we want to generate</p> required <code>*</code> <p>Pass a variable number of arguments to a function</p> required <code>greedy</code> <code>bool</code> <p>bool: Determine whether to use the greedy or non-greedy version of the generate function</p> <code>False</code> <code>max_new_tokens</code> <code>int</code> <p>int: Set the number of tokens to generate</p> <code>None</code> <code>**kwargs</code> <p>Pass any additional parameters to the process function</p> <code>{}</code> <p>Returns:</p> Type Description <p>A generator that yields the predicted text and the number of tokens generated</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def process(self,\n            string: str,\n            *,\n            greedy: bool = False,\n            max_new_tokens: int = None,\n            **kwargs\n            ):\n    \"\"\"\n    The process function is the main function of a model. It takes in an input string and returns a list of strings\n    that are generated from that input string. The process function can be called multiple times with different inputs,\n    and each time it will return a new set of outputs based on those inputs.\n\n    :param self: Access the class attributes\n    :param string: str: Pass the string that we want to generate\n    :param *: Pass a variable number of arguments to a function\n    :param greedy: bool: Determine whether to use the greedy or non-greedy version of the generate function\n    :param max_new_tokens: int: Set the number of tokens to generate\n    :param **kwargs: Pass any additional parameters to the process function\n    :return: A generator that yields the predicted text and the number of tokens generated\n\n    \"\"\"\n    tokens = self.prefix_tokenizer(\n        string,\n        max_length=self.config.max_length - self.config.max_stream_tokens,\n        padding='max_length',\n        return_tensors='jax'\n    ) \\\n        if self.config.use_prefix_tokenizer else \\\n        self.tokenizer(\n            string,\n            return_tensors='jax'\n        )\n\n    input_ids = tokens.input_ids\n    attention_mask = tokens.attention_mask\n    num_generated_tokens = 0\n    pad = self.config.max_length - self.config.max_stream_tokens\n\n    for _ in range((max_new_tokens or self.config.max_new_tokens) // self.config.max_stream_tokens):\n        predicted_token = self.greedy_generate(\n            params=self.params,\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        ) if greedy else self.generate(\n            params=self.params,\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n\n        num_generated_tokens += predicted_token.shape[-1]\n\n        input_ids = jnp.concatenate(\n            (input_ids, predicted_token), axis=-1\n        )[:, -pad:]\n        attention_mask = jnp.concatenate(\n            (attention_mask, jnp.ones((len(attention_mask), self.config.max_stream_tokens), dtype=jnp.int32)),\n            axis=-1\n        )[:, -pad:]\n\n        yield self.tokenizer.decode(input_ids[0][-num_generated_tokens:],\n                                    skip_special_tokens=True), num_generated_tokens\n        if predicted_token[0][-1] == self.tokenizer.eos_token_id or predicted_token[0][\n            -1] == self.prefix_tokenizer.eos_token_id:\n            break\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.process_gradio_chat","title":"<code>process_gradio_chat(prompt, history, max_new_tokens, system, greedy)</code>","text":"<p>The process_gradio_chat function is a wrapper for the process function. It takes in a prompt, history, max_new_tokens and system as arguments. The string variable is set to the output of format_chat with the given history and prompt. If stream tokens are not enabled then it will append an empty response to history and iterate through all responses from process until there are no more left (the last one). It will then return an empty string along with this new updated version of history. If stream tokens are enabled it appends an empty response to the end of our current list of histories (history) and iterates through</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>prompt</code> <p>Add the user's input to the history</p> required <code>history</code> <p>Keep track of the conversation</p> required <code>max_new_tokens</code> <p>Limit the number of tokens that can be generated by the model</p> required <code>system</code> <p>Determine whether the message is from the user or system</p> required <code>greedy</code> <p>Determine if the model should generate a response token by token or all at once</p> required <p>Returns:</p> Type Description <p>A tuple of two values:</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def process_gradio_chat(self, prompt, history, max_new_tokens, system, greedy):\n    \"\"\"\n    The process_gradio_chat function is a wrapper for the process function.\n    It takes in a prompt, history, max_new_tokens and system as arguments.\n    The string variable is set to the output of format_chat with the given history and prompt.\n    If stream tokens are not enabled then it will append an empty response to history and iterate through all responses from process until there are no more left (the last one). It will then return an empty string along with this new updated version of history. If stream tokens are enabled it appends an empty response to the end of our current list of histories (history) and iterates through\n\n    :param self: Refer to the object itself\n    :param prompt: Add the user's input to the history\n    :param history: Keep track of the conversation\n    :param max_new_tokens: Limit the number of tokens that can be generated by the model\n    :param system: Determine whether the message is from the user or system\n    :param greedy: Determine if the model should generate a response token by token or all at once\n    :return: A tuple of two values:\n\n    \"\"\"\n    string = self.format_chat(history=history, prompt=prompt, system=system)\n\n    if not self.config.stream_tokens_for_gradio:\n        response = ''\n        for response, _ in self.process(\n                string=string,\n                greedy=greedy,\n                max_new_tokens=max_new_tokens,\n        ):\n            pass\n        history.append([prompt, response])\n    else:\n        history.append([prompt, ''])\n        for response, _ in self.process(\n                string=string,\n                greedy=greedy,\n                max_new_tokens=max_new_tokens,\n        ):\n            history[-1][-1] = response\n            yield '', history\n    return '', history\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.process_gradio_instruct","title":"<code>process_gradio_instruct(instruction, system, max_new_tokens, greedy)</code>","text":"<p>The process_gradio_instruct function is a wrapper for the process function. It takes in an instruction and system, formats them into a string, and then passes that string to the process function. The response from this call to process is returned as output.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the instance of the class</p> required <code>instruction</code> <p>Pass in the instruction from the user</p> required <code>system</code> <p>Determine which system to use for the instruction</p> required <code>max_new_tokens</code> <p>Limit the number of new tokens that can be added to the vocabulary</p> required <code>greedy</code> <p>Determine whether the model should be greedy or not</p> required <p>Returns:</p> Type Description <p>A tuple of two strings:</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def process_gradio_instruct(self, instruction, system, max_new_tokens, greedy):\n    \"\"\"\n    The process_gradio_instruct function is a wrapper for the process function.\n    It takes in an instruction and system, formats them into a string, and then passes that string to the process function.\n    The response from this call to process is returned as output.\n\n    :param self: Refer to the instance of the class\n    :param instruction: Pass in the instruction from the user\n    :param system: Determine which system to use for the instruction\n    :param max_new_tokens: Limit the number of new tokens that can be added to the vocabulary\n    :param greedy: Determine whether the model should be greedy or not\n    :return: A tuple of two strings:\n\n    \"\"\"\n    string = self.format_instruct(instruction=instruction, system=system)\n    if not self.config.stream_tokens_for_gradio:\n        response = ''\n        for response, _ in self.process(\n                string=string,\n                greedy=greedy,\n                max_new_tokens=max_new_tokens,\n        ):\n            pass\n\n    else:\n        response = ''\n        for response, _ in self.process(\n                string=string,\n                greedy=greedy,\n                max_new_tokens=max_new_tokens,\n                stream=True\n        ):\n            yield '', response\n    return '', response\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.shard_params","title":"<code>shard_params(params, partition_rules)</code>","text":"<p>The shard_params function takes in a set of parameters and a partition rule. The partition rule is used to determine how the parameters should be sharded across devices. For example, if we have two devices, one with 4GB of memory and another with 8GB of memory, we may want to shard our model such that the device with more memory has more parameters on it. This function returns an updated version of params where each parameter is now stored on its own device.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Bind the instance of the class to a method</p> required <code>params</code> <p>Pass the parameters of the model to be sharded</p> required <code>partition_rules</code> <p>Specify how the parameters should be partitioned</p> required <p>Returns:</p> Type Description <p>The sharded parameters</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def shard_params(self, params, partition_rules):\n\n    \"\"\"\n    The shard_params function takes in a set of parameters and a partition rule.\n    The partition rule is used to determine how the parameters should be sharded across devices.\n    For example, if we have two devices, one with 4GB of memory and another with 8GB of memory,\n    we may want to shard our model such that the device with more memory has more parameters on it.\n    This function returns an updated version of params where each parameter is now stored on its own device.\n\n    :param self: Bind the instance of the class to a method\n    :param params: Pass the parameters of the model to be sharded\n    :param partition_rules: Specify how the parameters should be partitioned\n    :return: The sharded parameters\n\n    \"\"\"\n    logging.log(\n        logging.INFO,\n        'the parameters will be sharded and ba saved inside server you can access them by ``JAXServer.params``')\n    rules = match_partition_rules(params=params, rules=partition_rules)\n    self.rules = rules\n    shard_fns, _ = make_shard_and_gather_fns(rules, get_float_dtype_by_name(self.config.dtype))\n\n    with self.mesh:\n        self.params = jax.tree_map(\n            lambda f, p: f(p), shard_fns, params\n        )\n\n    return self.params\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JAXServer.status","title":"<code>status()</code>","text":"<p>The status function returns a dictionary with the following keys:     config: A dictionary containing all of the configuration parameters for this server.     devices: A string describing which devices are available to JAX.     number_of_backends: The number of backends available to JAX.  This is usually equal to the number of GPUs on your machine, but can be less if you have not installed CUDA or if you have disabled some GPUs in your system BIOS settings (e.g., because they are defective).  It can also be more than one if you have multiple machines connected via MPI and running under Horov</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A dictionary with the following keys:</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def status(self):\n    \"\"\"\n    The status function returns a dictionary with the following keys:\n        config: A dictionary containing all of the configuration parameters for this server.\n        devices: A string describing which devices are available to JAX.\n        number_of_backends: The number of backends available to JAX.  This is usually equal to the number of GPUs on your machine, but can be less if you have not installed CUDA or if you have disabled some GPUs in your system BIOS settings (e.g., because they are defective).  It can also be more than one if you have multiple machines connected via MPI and running under Horov\n\n    :param self: Represent the instance of the class\n    :return: A dictionary with the following keys:\n\n    \"\"\"\n    return {\n        'config': {k: v for k, v in self.config.__dict__.items()},\n        'devices': f\"{jax.devices()}\",\n        'number_of_backends': len(jax.devices()),\n        'status': 'Ready',\n        'number_of_served_request_until_last_up_time': f\"{self.number_of_served_request_until_last_up_time}\",\n        'memory': f\"{get_mem()}\"\n    }\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JaxServerConfig","title":"<code>JaxServerConfig</code>","text":"Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>class JaxServerConfig:\n    def __init__(\n            self,\n            host: str = \"0.0.0.0\",\n            port: int = 2059,\n            batch_size: int = 1,\n            contains_auto_format: bool = True,\n            max_length: int = 4096,\n            max_new_tokens: int = 4096,\n            max_stream_tokens: int = 64,\n            temperature: float = 0.1,\n            top_p: float = 0.95,\n            top_k: int = 50,\n            logging: bool = True,\n            mesh_axes_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"mp\"),\n            mesh_axes_shape: Sequence[int] = (1, -1, 1, 1),\n            dtype: str = 'fp16',\n            stream_tokens_for_gradio: bool = True,\n            use_prefix_tokenizer: bool = True,\n            pre_compile: bool = True,\n    ):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It sets up the attributes of an instance of this class, which are:\n            host: str = &amp;quot;0.0.0.0&amp;quot;\n                The IP address to listen on for incoming requests from clients\n\n        :param self: Represent the instance of the class\n        :param host: str: Set the host address of the server\n        :param port: int: Specify the port number that the server will run on\n        :param batch_size: int: Set the batch size of the model\n        :param contains_auto_format: bool: Determine whether the input text contains auto-formatting\n        :param max_length: int: Set the maximum length of the text that can be generated\n        :param max_new_tokens: int: Determine how many tokens can be added to the vocabulary\n        :param max_stream_tokens: int: Set the maximum number of tokens that can be streamed at a time\n        :param temperature: float: Control the randomness of the output\n        :param top_p: float: Control the diversity of the text generated\n        :param top_k: int: Limit the number of tokens that can be generated\n        :param logging: bool: Print out the progress of the server\n        :param mesh_axes_names: Sequence[str]: Specify the names of the axes in the mesh tensor\n        :param &amp;quot;fsdp&amp;quot;: Set the number of tokens to be generated\n        :param &amp;quot;tp&amp;quot;: Set the top_p value\n        :param &amp;quot;mp&amp;quot;): Define the mesh_axes_names\n        :param mesh_axes_shape: Sequence[int]: Specify the shape of the mesh\n        :param dtype: str: Specify the data type of the model\n        :param stream_tokens_for_gradio: bool: Determine whether the stream tokens\n        :param use_prefix_tokenizer: bool: Determine if the tokenizer should be used to generate tokens\n        :param pre_compile: bool: Pre-compile the model\n        :param : Set the host address\n        :return: Nothing\n\n        \"\"\"\n        self.host = host\n        self.port = port\n        self.batch_size = batch_size\n        self.contains_auto_format = contains_auto_format\n        self.max_length = max_length\n        self.max_new_tokens = max_new_tokens\n        self.max_stream_tokens = max_stream_tokens\n        self.temperature = temperature\n        self.top_p = top_p\n        self.top_k = top_k\n        self.logging = logging\n        self.mesh_axes_names = mesh_axes_names\n        self.mesh_axes_shape = mesh_axes_shape\n        self.dtype = dtype\n        self.stream_tokens_for_gradio = stream_tokens_for_gradio\n        self.use_prefix_tokenizer = use_prefix_tokenizer\n        self.pre_compile = pre_compile\n        assert max_new_tokens % max_stream_tokens == 0, \\\n            'max_new_tokens should be divisible by  max_new_tokens' \\\n            f'{max_new_tokens % max_stream_tokens}'\n\n    def __getitem__(self, item):\n        if hasattr(self, item):\n            return getattr(self, item)\n        else:\n            raise KeyError(f'{item} not found !')\n\n    def __setitem__(self, key, value):\n        setattr(self, key, value)\n</code></pre>"},{"location":"lib-python-EasyDel-serve-jax_serve/#lib.python.EasyDel.serve.jax_serve.JaxServerConfig.__init__","title":"<code>__init__(host='0.0.0.0', port=2059, batch_size=1, contains_auto_format=True, max_length=4096, max_new_tokens=4096, max_stream_tokens=64, temperature=0.1, top_p=0.95, top_k=50, logging=True, mesh_axes_names=('dp', 'fsdp', 'tp', 'mp'), mesh_axes_shape=(1, -1, 1, 1), dtype='fp16', stream_tokens_for_gradio=True, use_prefix_tokenizer=True, pre_compile=True)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the attributes of an instance of this class, which are:     host: str = \"0.0.0.0\"         The IP address to listen on for incoming requests from clients</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>host</code> <code>str</code> <p>str: Set the host address of the server</p> <code>'0.0.0.0'</code> <code>port</code> <code>int</code> <p>int: Specify the port number that the server will run on</p> <code>2059</code> <code>batch_size</code> <code>int</code> <p>int: Set the batch size of the model</p> <code>1</code> <code>contains_auto_format</code> <code>bool</code> <p>bool: Determine whether the input text contains auto-formatting</p> <code>True</code> <code>max_length</code> <code>int</code> <p>int: Set the maximum length of the text that can be generated</p> <code>4096</code> <code>max_new_tokens</code> <code>int</code> <p>int: Determine how many tokens can be added to the vocabulary</p> <code>4096</code> <code>max_stream_tokens</code> <code>int</code> <p>int: Set the maximum number of tokens that can be streamed at a time</p> <code>64</code> <code>temperature</code> <code>float</code> <p>float: Control the randomness of the output</p> <code>0.1</code> <code>top_p</code> <code>float</code> <p>float: Control the diversity of the text generated</p> <code>0.95</code> <code>top_k</code> <code>int</code> <p>int: Limit the number of tokens that can be generated</p> <code>50</code> <code>logging</code> <code>bool</code> <p>bool: Print out the progress of the server</p> <code>True</code> <code>mesh_axes_names</code> <code>Sequence[str]</code> <p>Sequence[str]: Specify the names of the axes in the mesh tensor</p> <code>('dp', 'fsdp', 'tp', 'mp')</code> <code>&amp;quot;fsdp&amp;quot;</code> <p>Set the number of tokens to be generated</p> required <code>&amp;quot;tp&amp;quot;</code> <p>Set the top_p value</p> required <code>&amp;quot;mp&amp;quot;)</code> <p>Define the mesh_axes_names</p> required <code>mesh_axes_shape</code> <code>Sequence[int]</code> <p>Sequence[int]: Specify the shape of the mesh</p> <code>(1, -1, 1, 1)</code> <code>dtype</code> <code>str</code> <p>str: Specify the data type of the model</p> <code>'fp16'</code> <code>stream_tokens_for_gradio</code> <code>bool</code> <p>bool: Determine whether the stream tokens</p> <code>True</code> <code>use_prefix_tokenizer</code> <code>bool</code> <p>bool: Determine if the tokenizer should be used to generate tokens</p> <code>True</code> <code>pre_compile</code> <code>bool</code> <p>bool: Pre-compile the model</p> <code>True</code> <code></code> <p>Set the host address</p> required <p>Returns:</p> Type Description <p>Nothing</p> Source code in <code>lib/python/EasyDel/serve/jax_serve.py</code> <pre><code>def __init__(\n        self,\n        host: str = \"0.0.0.0\",\n        port: int = 2059,\n        batch_size: int = 1,\n        contains_auto_format: bool = True,\n        max_length: int = 4096,\n        max_new_tokens: int = 4096,\n        max_stream_tokens: int = 64,\n        temperature: float = 0.1,\n        top_p: float = 0.95,\n        top_k: int = 50,\n        logging: bool = True,\n        mesh_axes_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"mp\"),\n        mesh_axes_shape: Sequence[int] = (1, -1, 1, 1),\n        dtype: str = 'fp16',\n        stream_tokens_for_gradio: bool = True,\n        use_prefix_tokenizer: bool = True,\n        pre_compile: bool = True,\n):\n    \"\"\"\n    The __init__ function is called when the class is instantiated.\n    It sets up the attributes of an instance of this class, which are:\n        host: str = &amp;quot;0.0.0.0&amp;quot;\n            The IP address to listen on for incoming requests from clients\n\n    :param self: Represent the instance of the class\n    :param host: str: Set the host address of the server\n    :param port: int: Specify the port number that the server will run on\n    :param batch_size: int: Set the batch size of the model\n    :param contains_auto_format: bool: Determine whether the input text contains auto-formatting\n    :param max_length: int: Set the maximum length of the text that can be generated\n    :param max_new_tokens: int: Determine how many tokens can be added to the vocabulary\n    :param max_stream_tokens: int: Set the maximum number of tokens that can be streamed at a time\n    :param temperature: float: Control the randomness of the output\n    :param top_p: float: Control the diversity of the text generated\n    :param top_k: int: Limit the number of tokens that can be generated\n    :param logging: bool: Print out the progress of the server\n    :param mesh_axes_names: Sequence[str]: Specify the names of the axes in the mesh tensor\n    :param &amp;quot;fsdp&amp;quot;: Set the number of tokens to be generated\n    :param &amp;quot;tp&amp;quot;: Set the top_p value\n    :param &amp;quot;mp&amp;quot;): Define the mesh_axes_names\n    :param mesh_axes_shape: Sequence[int]: Specify the shape of the mesh\n    :param dtype: str: Specify the data type of the model\n    :param stream_tokens_for_gradio: bool: Determine whether the stream tokens\n    :param use_prefix_tokenizer: bool: Determine if the tokenizer should be used to generate tokens\n    :param pre_compile: bool: Pre-compile the model\n    :param : Set the host address\n    :return: Nothing\n\n    \"\"\"\n    self.host = host\n    self.port = port\n    self.batch_size = batch_size\n    self.contains_auto_format = contains_auto_format\n    self.max_length = max_length\n    self.max_new_tokens = max_new_tokens\n    self.max_stream_tokens = max_stream_tokens\n    self.temperature = temperature\n    self.top_p = top_p\n    self.top_k = top_k\n    self.logging = logging\n    self.mesh_axes_names = mesh_axes_names\n    self.mesh_axes_shape = mesh_axes_shape\n    self.dtype = dtype\n    self.stream_tokens_for_gradio = stream_tokens_for_gradio\n    self.use_prefix_tokenizer = use_prefix_tokenizer\n    self.pre_compile = pre_compile\n    assert max_new_tokens % max_stream_tokens == 0, \\\n        'max_new_tokens should be divisible by  max_new_tokens' \\\n        f'{max_new_tokens % max_stream_tokens}'\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/","title":"serve.torch_serve","text":""},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PyTorchServer","title":"<code>PyTorchServer</code>","text":"<p>             Bases: <code>object</code></p> Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>class PyTorchServer(object):\n\n    def __init__(self, config: PytorchServerConfig):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It sets up the instance of the class, and defines all its attributes.\n        The __init__ function can accept arguments, which are passed at instantiation.\n\n        :param self: Represent the instance of the class\n        :param config: PytorchServerConfig: Pass the configuration parameters to the class\n        :return: The app, which is a fastapi object\n\n        \"\"\"\n        self.model, self.tokenizer = [None] * 2\n\n        self.config = config\n        self.process_uvicorn = None\n        self.app = FastAPI()\n        self.number_of_served_request_until_last_up_time = 0\n        self.device_rolling = self.get_gpu_memory(self.config.max_number_of_gpus)\n        self.dict_max_memory_sharding = {i: str(int(mem * self.config.max_gpu_perc_to_use)) + 'GiB' for i, mem in\n                                         enumerate(self.device_rolling)}\n        self.app.post('/chat')(self.forward_chat_fast_api)\n        self.app.post('/instruct')(self.forward_instruct_fast_api)\n        self.app.get('/status')(self.status)\n        self.app = gr.mount_gradio_app(self.app, self.create_gradio_ui_chat(), '/gradio_chat')\n        self.app = gr.mount_gradio_app(self.app, self.create_gradio_ui_instruct(), '/gradio_instruct')\n\n    @staticmethod\n    def get_gpu_memory(num_gpus_req=None):\n\n        \"\"\"\n        The get_gpu_memory function returns the amount of available GPU memory in GB.\n\n        :param num_gpus_req: Specify the number of gpus to be used\n        :return: The amount of free memory on each gpu\n\n        \"\"\"\n        gpu_m = []\n        dc = torch.cuda.device_count()\n        num_gpus = torch.cuda.device_count() if num_gpus_req is None else min(num_gpus_req, dc)\n\n        for gpu_id in range(num_gpus):\n            with torch.cuda.device(gpu_id):\n                gpu_properties = torch.cuda.get_device_properties(torch.cuda.current_device())\n                gpu_m.append(\n                    (gpu_properties.total_memory / (1024 ** 3)) - (torch.cuda.memory_allocated() / (1024 ** 3)))\n        return gpu_m\n\n    def get_model_load_kwargs(self):\n        \"\"\"\n        The get_model_load_kwargs function is used to set the torch_dtype, device_map and max_memory parameters for loading a model.\n\n        :param self: Bind the method to an object\n        :return: A dictionary with the following keys:\n\n        \"\"\"\n        if self.config.dtype == 'fp16':\n            dtype = torch.float16\n        elif self.config.dtype == 'fp32':\n            dtype = torch.float32\n        elif self.config.dtype == 'bf16':\n            dtype = torch.bfloat16\n        else:\n            raise ValueError('unknown type available types are [fp32 fp16 bf16]')\n        load_kwargs = {\n            'torch_dtype': dtype,\n            'device_map': 'auto',\n            'max_memory': self.dict_max_memory_sharding\n        }\n        return load_kwargs\n\n    def status(self):\n\n        \"\"\"\n        The status function returns a dictionary with the following keys:\n            config: A dictionary of configuration parameters.\n            devices: The number of GPUs available to the server.\n            device_sharding: Whether device sharding is enabled. If True, then each request will be served by\n            a different GPU (if multiple GPUs are available). If False, then all requests will be served by\n            the same GPU (or CPU if no GPUs are available). This parameter can also be set in your client's\n            initialization function via torch-serve's DeviceShardingStrategy\n            class. See https://pytorch-lightning.readthedoc\n\n        :param self: Represent the instance of the class\n        :return: A dictionary with the following keys:\n\n        \"\"\"\n        return {\n            'config': {k: v for k, v in self.config.__dict__.items()},\n            'devices': f\"{torch.cuda.device_count()}\",\n            'device_sharding': self.device_rolling,\n            'max_memory': self.dict_max_memory_sharding,\n            'status': 'Ready',\n            'number_of_served_request_until_last_up_time': f\"{self.number_of_served_request_until_last_up_time}\"\n        }\n\n    def forward_instruct_fast_api(self, data: InstructRequest):\n        \"\"\"\n        The forward_instruct_fast_api function is a ReST API endpoint that takes in an InstructRequest object and returns\n        a response. The InstructRequest object contains the following fields:\n            - system (str): A string representing the name of the system to be instructed. This should match one of the\n                systems defined in your config file, or else it will default to &amp;quot;default&amp;quot;. If you want to instruct multiple\n                systems at once, use forward_instruct_fast instead.\n\n        :param self: Refer to the object itself\n        :param data: InstructRequest: Pass in the data that is used to generate the response\n        :return: A dictionary with a single key, response\n\n        \"\"\"\n        string = self.format_instruct(\n            system=data.system,\n            instruction=data.instruction\n        )\n        response = self.process(\n            string=string,\n            max_length=self.config.max_length,\n            temperature=data.temperature,\n            stream=False,\n            top_k=self.config.top_k,\n            top_p=self.config.top_p,\n            max_new_tokens=self.config.max_new_tokens\n        )\n        return {\n            'response': response\n        }\n\n    def forward_chat_fast_api(self, data: ChatRequest):\n        \"\"\"\n        The forward_chat_fast_api function is a ReST API endpoint that takes in a ChatRequest object and returns the response from the model.\n\n        :param self: Refer to the object itself\n        :param data: ChatRequest: Pass the data from the api to the function\n        :return: A dictionary with a single key, response\n\n        \"\"\"\n        string = self.format_chat(\n            system=data.system,\n            history=data.history,\n            prompt=data.prompt,\n        )\n        response = self.process(\n            string=string,\n            max_length=self.config.max_length,\n            temperature=data.temperature,\n            stream=False,\n            top_k=self.config.top_k,\n            top_p=self.config.top_p,\n            max_new_tokens=self.config.max_new_tokens\n        )\n        return {\n            'response': response\n        }\n\n    @staticmethod\n    def format_instruct(system: str, instruction: str) -&gt; str:\n        \"\"\"\n        The format_instruct function is used to format the instruction string\n            for a particular system.  The function takes two arguments:\n\n        :param system: str: Determine which system the instruction is for\n        :param instruction: str: Store the instruction that is being passed in\n        :return: The instruction in the format of the system\n\n        \"\"\"\n        raise NotImplementedError()\n\n    @staticmethod\n    def format_chat(history: List[str], prompt: str, system: str = None) -&gt; str:\n        \"\"\"\n        The format_chat function takes a list of strings, representing the chat history,\n        and returns a string that is formatted in such a way that it can be printed to the screen.\n        The prompt argument is used to indicate which user's turn it currently is. The system argument\n        is used for messages from the system (e.g., &amp;quot;You are now connected!&amp;quot;). If no value for system\n        is provided, then this function should return None.\n\n        :param history: List[str]: Store the chat history\n        :param prompt: str: Display the prompt to the user\n        :param system: str: Add a system message to the chat history\n        :return: A string that contains the history of a chat\n\n        \"\"\"\n        raise NotImplementedError()\n\n    def process(self,\n                string: str,\n                max_new_tokens: int = None,\n                max_length: int = None,\n                temperature: float = 0.6,\n                top_k=50,\n                top_p=0.9,\n                stream: bool = True,\n                sample: bool = True\n\n                ):\n        \"\"\"\n        The process function is the main function of this class. It takes a string as input and returns a generator that yields strings.\n\n        :param self: Represent the instance of the class\n        :param string: str: Pass the string to be generated\n        :param max_new_tokens: int: Limit the number of new tokens that can be generated\n        :param max_length: int: Set the maximum length of the generated text\n        :param temperature: float: Control the randomness of the text generation\n        :param top_k: Filter out the top k tokens with the highest probability\n        :param top_p: Control the probability of sampling from the top n tokens\n        :param stream: bool: Determine whether to stream the output or not\n        :param sample: bool: Indicate whether to sample from the distribution or take the argmax\n        :return: A generator\n\n        \"\"\"\n        assert self.model is not None, 'you should first load model with ``load`` method'\n        tokens = self.tokenizer(\n            string,\n            return_tensors='pt'\n        )\n        input_ids = tokens.input_ids.to(self.model.device)\n        attention_mask = tokens.attention_mask.to(self.model.device)\n\n        iterator_streamer = TextIteratorStreamer(\n            tokenizer=self.tokenizer,\n            skip_prompt=True,\n            skip_special_tokens=True\n        )\n\n        if stream:\n            kwargs = dict(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                streamer=iterator_streamer,\n                generation_config=transformers.GenerationConfig(\n                    bos_token_id=self.tokenizer.bos_token_id,\n                    eos_token_id=self.tokenizer.eos_token_id,\n                    pad_token_id=self.tokenizer.pad_token_id,\n                    max_length=max_length or self.config.max_length,\n                    temperature=temperature,\n                    top_k=top_k,\n                    top_p=top_p,\n                    max_new_tokens=max_new_tokens or self.config.max_new_tokens,\n                    num_beams=1,\n                    do_sample=sample\n                )\n            )\n            thread_ = threading.Thread(\n                target=self.model.generate,\n                kwargs=kwargs\n            )\n            thread_.start()\n            for string in iterator_streamer:\n                yield string\n        else:\n            kwargs = dict(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                generation_config=transformers.GenerationConfig(\n                    bos_token_id=self.tokenizer.bos_token_id,\n                    eos_token_id=self.tokenizer.eos_token_id,\n                    pad_token_id=self.tokenizer.pad_token_id,\n                    max_length=max_length or self.config.max_length,\n                    temperature=temperature,\n                    top_k=top_k,\n                    top_p=top_p,\n                    max_new_tokens=max_new_tokens or self.config.max_new_tokens,\n                    num_beams=1\n                )\n            )\n            pred = self.tokenizer.decode(self.model.generate(\n                **kwargs\n            ).logits[0])\n            return pred\n\n    def load(self, repo_id: str, tokenizer_repo: str = None, auto_config: bool = True, **kwargs):\n        \"\"\"\n        The load function is used to load a model from the HuggingFace Model Hub.\n\n        :param self: Represent the instance of the class\n        :param repo_id: str: Specify the name of the model to be loaded\n        :param tokenizer_repo: str: Specify the repo id of the tokenizer\n        :param auto_config: bool: Determine whether the model should be loaded with a config file or not\n        :param **kwargs: Pass a variable number of keyword arguments to the function\n        :return: A tuple of model and tokenizer\n\n        \"\"\"\n        load_kwargs = kwargs if not auto_config else self.get_model_load_kwargs()\n        load_kwargs = load_kwargs | kwargs\n        model = transformers.AutoModelForCausalLM.from_pretrained(\n            repo_id,\n            trust_remote_code=True,\n            **load_kwargs\n        )\n        tokenizer = transformers.AutoTokenizer.from_pretrained(\n            tokenizer_repo or repo_id,\n            trust_remote_code=True\n        )\n\n        self.model = model\n        self.tokenizer = tokenizer\n\n    def process_gradio_chat(self,\n                            prompt: str,\n                            history: List[str],\n                            max_new_tokens: int,\n                            temperature: float,\n                            max_length: int,\n                            top_p: float,\n                            top_k: int\n                            ):\n        \"\"\"\n        The process_gradio_chat function is a wrapper for the process function.\n        It takes in the same arguments as process, but also takes in prompt and history.\n        The prompt is appended to the history list, which contains all of the previous messages sent by both parties.\n        The format_chat function formats this information into a string that can be fed into GPT-2's model.\n\n        :param self: Refer to the object itself\n        :param prompt: str: Set the prompt for the chatbot\n        :param history: List[str]: Store the history of the conversation\n        :param max_new_tokens: int: Limit the number of tokens that can be generated in a single response\n        :param temperature: float: Control the randomness of the text generation\n        :param max_length: int: Set the maximum length of the response\n        :param top_p: float: Control the randomness of the model\n        :param top_k: int: Control the number of tokens that are filtered from the top-k filtering\n        :return: A generator object, which is a type of iterator\n\n        \"\"\"\n        string = self.format_chat(prompt=prompt, history=history, system=None)\n        history.append([prompt, ''])\n        responses = ''\n        for response in self.process(\n                string=string,\n                max_new_tokens=max_new_tokens,\n                temperature=temperature,\n                max_length=max_length,\n                top_p=top_p,\n                top_k=top_k,\n                stream=True\n        ):\n            responses += response\n            history[-1][-1] = responses\n            yield '', history\n\n    def process_gradio_instruct(self,\n                                instruction: str,\n                                system: str,\n                                max_new_tokens: int,\n                                temperature: float,\n                                max_length: int,\n                                top_p: float,\n                                top_k: int\n                                ):\n        \"\"\"\n        The process_gradio_instruct function is a wrapper for the process function.\n        It takes in an instruction and system, formats them into a string, then passes that string to the process function.\n        The output of this function is formatted so that it can be used with gradio's stream_func decorator.\n\n        :param self: Refer to the object itself\n        :param instruction: str: Pass the instruction to the model\n        :param system: str: Specify the system that is being used for the chatbot\n        :param max_new_tokens: int: Specify the maximum number of tokens that can be generated\n        :param temperature: float: Control the randomness of the output\n        :param max_length: int: Set the maximum length of the response\n        :param top_p: float: Control the randomness of the model\n        :param top_k: int: Limit the number of tokens that are considered for each position\n        :return: A generator object\n\n        \"\"\"\n        string = self.format_instruct(system=system, instruction=instruction)\n        responses = ''\n        for response in self.process(\n                string=string,\n                max_new_tokens=max_new_tokens,\n                temperature=temperature,\n                max_length=max_length,\n                top_p=top_p,\n                top_k=top_k,\n                stream=True\n        ):\n            responses += response\n            yield '', response\n\n    def create_gradio_ui_chat(self):\n        \"\"\"\n        The create_gradio_ui_chat function creates a Gradio UI for the chatbot.\n\n        :param self: Refer to the object itself\n        :return: A block\n\n        \"\"\"\n        with gr.Blocks(\n                theme=seafoam) as block:\n            gr.Markdown(\"# &lt;h1&gt; &lt;center&gt;Powered by [EasyDeL](https://github.com/erfanzar/EasyDel) &lt;/center&gt; &lt;/h1&gt;\")\n            with gr.Row():\n                history = gr.Chatbot(elem_id=\"EasyDel\", label=\"EasyDel\", container=True, height=600)\n\n            with gr.Row():\n                with gr.Column():\n                    prompt = gr.Textbox(show_label=False, placeholder='Message Box', container=False)\n                with gr.Column():\n                    with gr.Row():\n                        submit = gr.Button(variant=\"primary\")\n                        stop = gr.Button(value='Stop ')\n                        clear = gr.Button(value='Clear Conversation')\n\n            with gr.Row():\n                with gr.Accordion('Advanced Options', open=False):\n                    max_new_tokens = gr.Slider(value=self.config.max_new_tokens, maximum=10000,\n                                               minimum=self.config.max_stream_tokens,\n                                               label='Max New Tokens', step=self.config.max_stream_tokens)\n                    max_length = gr.Slider(value=self.config.max_length, maximum=self.config.max_length, minimum=1,\n                                           label='Max Length', step=1)\n                    temperature = gr.Slider(value=0.2, maximum=1, minimum=0.1, label='Temperature', step=0.01)\n                    top_p = gr.Slider(value=0.9, maximum=1, minimum=0.1, label='Top P', step=0.01)\n                    top_k = gr.Slider(value=50, maximum=100, minimum=1, label='Top K', step=1)\n\n            inputs = [\n                prompt,\n                history,\n                max_new_tokens,\n                temperature,\n                max_length,\n                top_p,\n                top_k\n            ]\n            sub_event = submit.click(fn=self.process_gradio_chat, inputs=inputs, outputs=[prompt, history])\n\n            def clear_():\n                return []\n\n            clear.click(fn=clear_, outputs=[history])\n            txt_event = prompt.submit(fn=self.process_gradio_chat, inputs=inputs, outputs=[prompt, history])\n\n            stop.click(fn=None, inputs=None, outputs=None, cancels=[txt_event, sub_event])\n\n        block.queue()\n        return block\n\n    def create_gradio_ui_instruct(self):\n        \"\"\"\n        The create_gradio_ui_instruct function creates a Gradio UI for the EasyDel model.\n        The function takes in no arguments and returns a block of code that is used to create the UI.\n\n\n        :param self: Represent the instance of the class\n        :return: A block\n\n        \"\"\"\n        with gr.Blocks(\n                theme=seafoam) as block:\n            gr.Markdown(\"# &lt;h1&gt; &lt;center&gt;Powered by [EasyDeL](https://github.com/erfanzar/EasyDel) &lt;/center&gt; &lt;/h1&gt;\")\n            with gr.Row():\n                pred = gr.TextArea(elem_id=\"EasyDel\", label=\"EasyDel\", container=True)\n\n            with gr.Row():\n                submit = gr.Button(variant=\"primary\")\n                stop = gr.Button(value='Stop ')\n                clear = gr.Button(value='Clear Conversation')\n            with gr.Column():\n                prompt = gr.Textbox(show_label=False, placeholder='Instruct Message', container=False)\n                system = gr.Textbox(value='You Are an helpful AI Assistant, generate good and helpful answers',\n                                    show_label=False, placeholder='System Message', container=False)\n\n            with gr.Row():\n                with gr.Accordion('Advanced Options', open=False):\n                    max_new_tokens = gr.Slider(value=self.config.max_new_tokens, maximum=10000,\n                                               minimum=self.config.max_stream_tokens,\n                                               label='Max New Tokens', step=self.config.max_stream_tokens, )\n                    max_length = gr.Slider(value=self.config.max_length, maximum=self.config.max_length, minimum=1,\n                                           label='Max Length', step=1)\n                    temperature = gr.Slider(value=0.2, maximum=1, minimum=0.1, label='Temperature', step=0.01)\n                    top_p = gr.Slider(value=0.9, maximum=1, minimum=0.1, label='Top P', step=0.01)\n                    top_k = gr.Slider(value=50, maximum=100, minimum=1, label='Top K', step=1)\n\n            inputs = [\n                prompt,\n                system,\n                max_new_tokens,\n                temperature,\n                max_length,\n                top_p,\n                top_k\n            ]\n            sub_event = submit.click(fn=self.process_gradio_instruct, inputs=inputs, outputs=[prompt, pred])\n\n            def clear_():\n                return ''\n\n            clear.click(fn=clear_, outputs=[pred])\n            txt_event = prompt.submit(fn=self.process_gradio_instruct, inputs=inputs, outputs=[prompt, pred])\n\n            stop.click(fn=None, inputs=None, outputs=None, cancels=[txt_event, sub_event])\n\n        block.queue()\n        return block\n\n    def fire(self):\n        \"\"\"\n        The fire function starts the uvicorn server in a separate process.\n\n        :param self: Represent the instance of the class\n        :return: A process that runs the uvicorn server\n\n        \"\"\"\n        def run():\n            uvicorn.run(self.app, host=self.config.host, port=self.config.port)\n\n        self.process_uvicorn = mp.Process(target=run)\n        self.process_uvicorn.start()\n\n    def end(self):\n        \"\"\"\n        The end function is used to stop the server.\n            It will wait for the process to end before returning.\n\n        :param self: Represent the instance of the class\n        :return: A boolean value\n\n        \"\"\"\n        if self.process_uvicorn is not None:\n            self.process_uvicorn.join()\n        else:\n            logging.warning('you have to fire server before ending that this command will be ignored')\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PyTorchServer.__init__","title":"<code>__init__(config)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the instance of the class, and defines all its attributes. The init function can accept arguments, which are passed at instantiation.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>config</code> <code>PytorchServerConfig</code> <p>PytorchServerConfig: Pass the configuration parameters to the class</p> required <p>Returns:</p> Type Description <p>The app, which is a fastapi object</p> Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>def __init__(self, config: PytorchServerConfig):\n    \"\"\"\n    The __init__ function is called when the class is instantiated.\n    It sets up the instance of the class, and defines all its attributes.\n    The __init__ function can accept arguments, which are passed at instantiation.\n\n    :param self: Represent the instance of the class\n    :param config: PytorchServerConfig: Pass the configuration parameters to the class\n    :return: The app, which is a fastapi object\n\n    \"\"\"\n    self.model, self.tokenizer = [None] * 2\n\n    self.config = config\n    self.process_uvicorn = None\n    self.app = FastAPI()\n    self.number_of_served_request_until_last_up_time = 0\n    self.device_rolling = self.get_gpu_memory(self.config.max_number_of_gpus)\n    self.dict_max_memory_sharding = {i: str(int(mem * self.config.max_gpu_perc_to_use)) + 'GiB' for i, mem in\n                                     enumerate(self.device_rolling)}\n    self.app.post('/chat')(self.forward_chat_fast_api)\n    self.app.post('/instruct')(self.forward_instruct_fast_api)\n    self.app.get('/status')(self.status)\n    self.app = gr.mount_gradio_app(self.app, self.create_gradio_ui_chat(), '/gradio_chat')\n    self.app = gr.mount_gradio_app(self.app, self.create_gradio_ui_instruct(), '/gradio_instruct')\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PyTorchServer.create_gradio_ui_chat","title":"<code>create_gradio_ui_chat()</code>","text":"<p>The create_gradio_ui_chat function creates a Gradio UI for the chatbot.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <p>Returns:</p> Type Description <p>A block</p> Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>def create_gradio_ui_chat(self):\n    \"\"\"\n    The create_gradio_ui_chat function creates a Gradio UI for the chatbot.\n\n    :param self: Refer to the object itself\n    :return: A block\n\n    \"\"\"\n    with gr.Blocks(\n            theme=seafoam) as block:\n        gr.Markdown(\"# &lt;h1&gt; &lt;center&gt;Powered by [EasyDeL](https://github.com/erfanzar/EasyDel) &lt;/center&gt; &lt;/h1&gt;\")\n        with gr.Row():\n            history = gr.Chatbot(elem_id=\"EasyDel\", label=\"EasyDel\", container=True, height=600)\n\n        with gr.Row():\n            with gr.Column():\n                prompt = gr.Textbox(show_label=False, placeholder='Message Box', container=False)\n            with gr.Column():\n                with gr.Row():\n                    submit = gr.Button(variant=\"primary\")\n                    stop = gr.Button(value='Stop ')\n                    clear = gr.Button(value='Clear Conversation')\n\n        with gr.Row():\n            with gr.Accordion('Advanced Options', open=False):\n                max_new_tokens = gr.Slider(value=self.config.max_new_tokens, maximum=10000,\n                                           minimum=self.config.max_stream_tokens,\n                                           label='Max New Tokens', step=self.config.max_stream_tokens)\n                max_length = gr.Slider(value=self.config.max_length, maximum=self.config.max_length, minimum=1,\n                                       label='Max Length', step=1)\n                temperature = gr.Slider(value=0.2, maximum=1, minimum=0.1, label='Temperature', step=0.01)\n                top_p = gr.Slider(value=0.9, maximum=1, minimum=0.1, label='Top P', step=0.01)\n                top_k = gr.Slider(value=50, maximum=100, minimum=1, label='Top K', step=1)\n\n        inputs = [\n            prompt,\n            history,\n            max_new_tokens,\n            temperature,\n            max_length,\n            top_p,\n            top_k\n        ]\n        sub_event = submit.click(fn=self.process_gradio_chat, inputs=inputs, outputs=[prompt, history])\n\n        def clear_():\n            return []\n\n        clear.click(fn=clear_, outputs=[history])\n        txt_event = prompt.submit(fn=self.process_gradio_chat, inputs=inputs, outputs=[prompt, history])\n\n        stop.click(fn=None, inputs=None, outputs=None, cancels=[txt_event, sub_event])\n\n    block.queue()\n    return block\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PyTorchServer.create_gradio_ui_instruct","title":"<code>create_gradio_ui_instruct()</code>","text":"<p>The create_gradio_ui_instruct function creates a Gradio UI for the EasyDel model. The function takes in no arguments and returns a block of code that is used to create the UI.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A block</p> Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>def create_gradio_ui_instruct(self):\n    \"\"\"\n    The create_gradio_ui_instruct function creates a Gradio UI for the EasyDel model.\n    The function takes in no arguments and returns a block of code that is used to create the UI.\n\n\n    :param self: Represent the instance of the class\n    :return: A block\n\n    \"\"\"\n    with gr.Blocks(\n            theme=seafoam) as block:\n        gr.Markdown(\"# &lt;h1&gt; &lt;center&gt;Powered by [EasyDeL](https://github.com/erfanzar/EasyDel) &lt;/center&gt; &lt;/h1&gt;\")\n        with gr.Row():\n            pred = gr.TextArea(elem_id=\"EasyDel\", label=\"EasyDel\", container=True)\n\n        with gr.Row():\n            submit = gr.Button(variant=\"primary\")\n            stop = gr.Button(value='Stop ')\n            clear = gr.Button(value='Clear Conversation')\n        with gr.Column():\n            prompt = gr.Textbox(show_label=False, placeholder='Instruct Message', container=False)\n            system = gr.Textbox(value='You Are an helpful AI Assistant, generate good and helpful answers',\n                                show_label=False, placeholder='System Message', container=False)\n\n        with gr.Row():\n            with gr.Accordion('Advanced Options', open=False):\n                max_new_tokens = gr.Slider(value=self.config.max_new_tokens, maximum=10000,\n                                           minimum=self.config.max_stream_tokens,\n                                           label='Max New Tokens', step=self.config.max_stream_tokens, )\n                max_length = gr.Slider(value=self.config.max_length, maximum=self.config.max_length, minimum=1,\n                                       label='Max Length', step=1)\n                temperature = gr.Slider(value=0.2, maximum=1, minimum=0.1, label='Temperature', step=0.01)\n                top_p = gr.Slider(value=0.9, maximum=1, minimum=0.1, label='Top P', step=0.01)\n                top_k = gr.Slider(value=50, maximum=100, minimum=1, label='Top K', step=1)\n\n        inputs = [\n            prompt,\n            system,\n            max_new_tokens,\n            temperature,\n            max_length,\n            top_p,\n            top_k\n        ]\n        sub_event = submit.click(fn=self.process_gradio_instruct, inputs=inputs, outputs=[prompt, pred])\n\n        def clear_():\n            return ''\n\n        clear.click(fn=clear_, outputs=[pred])\n        txt_event = prompt.submit(fn=self.process_gradio_instruct, inputs=inputs, outputs=[prompt, pred])\n\n        stop.click(fn=None, inputs=None, outputs=None, cancels=[txt_event, sub_event])\n\n    block.queue()\n    return block\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PyTorchServer.end","title":"<code>end()</code>","text":"<p>The end function is used to stop the server.     It will wait for the process to end before returning.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A boolean value</p> Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>def end(self):\n    \"\"\"\n    The end function is used to stop the server.\n        It will wait for the process to end before returning.\n\n    :param self: Represent the instance of the class\n    :return: A boolean value\n\n    \"\"\"\n    if self.process_uvicorn is not None:\n        self.process_uvicorn.join()\n    else:\n        logging.warning('you have to fire server before ending that this command will be ignored')\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PyTorchServer.fire","title":"<code>fire()</code>","text":"<p>The fire function starts the uvicorn server in a separate process.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A process that runs the uvicorn server</p> Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>def fire(self):\n    \"\"\"\n    The fire function starts the uvicorn server in a separate process.\n\n    :param self: Represent the instance of the class\n    :return: A process that runs the uvicorn server\n\n    \"\"\"\n    def run():\n        uvicorn.run(self.app, host=self.config.host, port=self.config.port)\n\n    self.process_uvicorn = mp.Process(target=run)\n    self.process_uvicorn.start()\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PyTorchServer.format_chat","title":"<code>format_chat(history, prompt, system=None)</code>  <code>staticmethod</code>","text":"<p>The format_chat function takes a list of strings, representing the chat history, and returns a string that is formatted in such a way that it can be printed to the screen. The prompt argument is used to indicate which user's turn it currently is. The system argument is used for messages from the system (e.g., \"You are now connected!\"). If no value for system is provided, then this function should return None.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>List[str]</code> <p>List[str]: Store the chat history</p> required <code>prompt</code> <code>str</code> <p>str: Display the prompt to the user</p> required <code>system</code> <code>str</code> <p>str: Add a system message to the chat history</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>A string that contains the history of a chat</p> Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>@staticmethod\ndef format_chat(history: List[str], prompt: str, system: str = None) -&gt; str:\n    \"\"\"\n    The format_chat function takes a list of strings, representing the chat history,\n    and returns a string that is formatted in such a way that it can be printed to the screen.\n    The prompt argument is used to indicate which user's turn it currently is. The system argument\n    is used for messages from the system (e.g., &amp;quot;You are now connected!&amp;quot;). If no value for system\n    is provided, then this function should return None.\n\n    :param history: List[str]: Store the chat history\n    :param prompt: str: Display the prompt to the user\n    :param system: str: Add a system message to the chat history\n    :return: A string that contains the history of a chat\n\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PyTorchServer.format_instruct","title":"<code>format_instruct(system, instruction)</code>  <code>staticmethod</code>","text":"<p>The format_instruct function is used to format the instruction string     for a particular system.  The function takes two arguments:</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>str</code> <p>str: Determine which system the instruction is for</p> required <code>instruction</code> <code>str</code> <p>str: Store the instruction that is being passed in</p> required <p>Returns:</p> Type Description <code>str</code> <p>The instruction in the format of the system</p> Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>@staticmethod\ndef format_instruct(system: str, instruction: str) -&gt; str:\n    \"\"\"\n    The format_instruct function is used to format the instruction string\n        for a particular system.  The function takes two arguments:\n\n    :param system: str: Determine which system the instruction is for\n    :param instruction: str: Store the instruction that is being passed in\n    :return: The instruction in the format of the system\n\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PyTorchServer.forward_chat_fast_api","title":"<code>forward_chat_fast_api(data)</code>","text":"<p>The forward_chat_fast_api function is a ReST API endpoint that takes in a ChatRequest object and returns the response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>data</code> <code>ChatRequest</code> <p>ChatRequest: Pass the data from the api to the function</p> required <p>Returns:</p> Type Description <p>A dictionary with a single key, response</p> Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>def forward_chat_fast_api(self, data: ChatRequest):\n    \"\"\"\n    The forward_chat_fast_api function is a ReST API endpoint that takes in a ChatRequest object and returns the response from the model.\n\n    :param self: Refer to the object itself\n    :param data: ChatRequest: Pass the data from the api to the function\n    :return: A dictionary with a single key, response\n\n    \"\"\"\n    string = self.format_chat(\n        system=data.system,\n        history=data.history,\n        prompt=data.prompt,\n    )\n    response = self.process(\n        string=string,\n        max_length=self.config.max_length,\n        temperature=data.temperature,\n        stream=False,\n        top_k=self.config.top_k,\n        top_p=self.config.top_p,\n        max_new_tokens=self.config.max_new_tokens\n    )\n    return {\n        'response': response\n    }\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PyTorchServer.forward_instruct_fast_api","title":"<code>forward_instruct_fast_api(data)</code>","text":"<p>The forward_instruct_fast_api function is a ReST API endpoint that takes in an InstructRequest object and returns a response. The InstructRequest object contains the following fields:     - system (str): A string representing the name of the system to be instructed. This should match one of the         systems defined in your config file, or else it will default to \"default\". If you want to instruct multiple         systems at once, use forward_instruct_fast instead.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>data</code> <code>InstructRequest</code> <p>InstructRequest: Pass in the data that is used to generate the response</p> required <p>Returns:</p> Type Description <p>A dictionary with a single key, response</p> Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>def forward_instruct_fast_api(self, data: InstructRequest):\n    \"\"\"\n    The forward_instruct_fast_api function is a ReST API endpoint that takes in an InstructRequest object and returns\n    a response. The InstructRequest object contains the following fields:\n        - system (str): A string representing the name of the system to be instructed. This should match one of the\n            systems defined in your config file, or else it will default to &amp;quot;default&amp;quot;. If you want to instruct multiple\n            systems at once, use forward_instruct_fast instead.\n\n    :param self: Refer to the object itself\n    :param data: InstructRequest: Pass in the data that is used to generate the response\n    :return: A dictionary with a single key, response\n\n    \"\"\"\n    string = self.format_instruct(\n        system=data.system,\n        instruction=data.instruction\n    )\n    response = self.process(\n        string=string,\n        max_length=self.config.max_length,\n        temperature=data.temperature,\n        stream=False,\n        top_k=self.config.top_k,\n        top_p=self.config.top_p,\n        max_new_tokens=self.config.max_new_tokens\n    )\n    return {\n        'response': response\n    }\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PyTorchServer.get_gpu_memory","title":"<code>get_gpu_memory(num_gpus_req=None)</code>  <code>staticmethod</code>","text":"<p>The get_gpu_memory function returns the amount of available GPU memory in GB.</p> <p>Parameters:</p> Name Type Description Default <code>num_gpus_req</code> <p>Specify the number of gpus to be used</p> <code>None</code> <p>Returns:</p> Type Description <p>The amount of free memory on each gpu</p> Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>@staticmethod\ndef get_gpu_memory(num_gpus_req=None):\n\n    \"\"\"\n    The get_gpu_memory function returns the amount of available GPU memory in GB.\n\n    :param num_gpus_req: Specify the number of gpus to be used\n    :return: The amount of free memory on each gpu\n\n    \"\"\"\n    gpu_m = []\n    dc = torch.cuda.device_count()\n    num_gpus = torch.cuda.device_count() if num_gpus_req is None else min(num_gpus_req, dc)\n\n    for gpu_id in range(num_gpus):\n        with torch.cuda.device(gpu_id):\n            gpu_properties = torch.cuda.get_device_properties(torch.cuda.current_device())\n            gpu_m.append(\n                (gpu_properties.total_memory / (1024 ** 3)) - (torch.cuda.memory_allocated() / (1024 ** 3)))\n    return gpu_m\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PyTorchServer.get_model_load_kwargs","title":"<code>get_model_load_kwargs()</code>","text":"<p>The get_model_load_kwargs function is used to set the torch_dtype, device_map and max_memory parameters for loading a model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Bind the method to an object</p> required <p>Returns:</p> Type Description <p>A dictionary with the following keys:</p> Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>def get_model_load_kwargs(self):\n    \"\"\"\n    The get_model_load_kwargs function is used to set the torch_dtype, device_map and max_memory parameters for loading a model.\n\n    :param self: Bind the method to an object\n    :return: A dictionary with the following keys:\n\n    \"\"\"\n    if self.config.dtype == 'fp16':\n        dtype = torch.float16\n    elif self.config.dtype == 'fp32':\n        dtype = torch.float32\n    elif self.config.dtype == 'bf16':\n        dtype = torch.bfloat16\n    else:\n        raise ValueError('unknown type available types are [fp32 fp16 bf16]')\n    load_kwargs = {\n        'torch_dtype': dtype,\n        'device_map': 'auto',\n        'max_memory': self.dict_max_memory_sharding\n    }\n    return load_kwargs\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PyTorchServer.load","title":"<code>load(repo_id, tokenizer_repo=None, auto_config=True, **kwargs)</code>","text":"<p>The load function is used to load a model from the HuggingFace Model Hub.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>repo_id</code> <code>str</code> <p>str: Specify the name of the model to be loaded</p> required <code>tokenizer_repo</code> <code>str</code> <p>str: Specify the repo id of the tokenizer</p> <code>None</code> <code>auto_config</code> <code>bool</code> <p>bool: Determine whether the model should be loaded with a config file or not</p> <code>True</code> <code>**kwargs</code> <p>Pass a variable number of keyword arguments to the function</p> <code>{}</code> <p>Returns:</p> Type Description <p>A tuple of model and tokenizer</p> Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>def load(self, repo_id: str, tokenizer_repo: str = None, auto_config: bool = True, **kwargs):\n    \"\"\"\n    The load function is used to load a model from the HuggingFace Model Hub.\n\n    :param self: Represent the instance of the class\n    :param repo_id: str: Specify the name of the model to be loaded\n    :param tokenizer_repo: str: Specify the repo id of the tokenizer\n    :param auto_config: bool: Determine whether the model should be loaded with a config file or not\n    :param **kwargs: Pass a variable number of keyword arguments to the function\n    :return: A tuple of model and tokenizer\n\n    \"\"\"\n    load_kwargs = kwargs if not auto_config else self.get_model_load_kwargs()\n    load_kwargs = load_kwargs | kwargs\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        repo_id,\n        trust_remote_code=True,\n        **load_kwargs\n    )\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        tokenizer_repo or repo_id,\n        trust_remote_code=True\n    )\n\n    self.model = model\n    self.tokenizer = tokenizer\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PyTorchServer.process","title":"<code>process(string, max_new_tokens=None, max_length=None, temperature=0.6, top_k=50, top_p=0.9, stream=True, sample=True)</code>","text":"<p>The process function is the main function of this class. It takes a string as input and returns a generator that yields strings.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>string</code> <code>str</code> <p>str: Pass the string to be generated</p> required <code>max_new_tokens</code> <code>int</code> <p>int: Limit the number of new tokens that can be generated</p> <code>None</code> <code>max_length</code> <code>int</code> <p>int: Set the maximum length of the generated text</p> <code>None</code> <code>temperature</code> <code>float</code> <p>float: Control the randomness of the text generation</p> <code>0.6</code> <code>top_k</code> <p>Filter out the top k tokens with the highest probability</p> <code>50</code> <code>top_p</code> <p>Control the probability of sampling from the top n tokens</p> <code>0.9</code> <code>stream</code> <code>bool</code> <p>bool: Determine whether to stream the output or not</p> <code>True</code> <code>sample</code> <code>bool</code> <p>bool: Indicate whether to sample from the distribution or take the argmax</p> <code>True</code> <p>Returns:</p> Type Description <p>A generator</p> Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>def process(self,\n            string: str,\n            max_new_tokens: int = None,\n            max_length: int = None,\n            temperature: float = 0.6,\n            top_k=50,\n            top_p=0.9,\n            stream: bool = True,\n            sample: bool = True\n\n            ):\n    \"\"\"\n    The process function is the main function of this class. It takes a string as input and returns a generator that yields strings.\n\n    :param self: Represent the instance of the class\n    :param string: str: Pass the string to be generated\n    :param max_new_tokens: int: Limit the number of new tokens that can be generated\n    :param max_length: int: Set the maximum length of the generated text\n    :param temperature: float: Control the randomness of the text generation\n    :param top_k: Filter out the top k tokens with the highest probability\n    :param top_p: Control the probability of sampling from the top n tokens\n    :param stream: bool: Determine whether to stream the output or not\n    :param sample: bool: Indicate whether to sample from the distribution or take the argmax\n    :return: A generator\n\n    \"\"\"\n    assert self.model is not None, 'you should first load model with ``load`` method'\n    tokens = self.tokenizer(\n        string,\n        return_tensors='pt'\n    )\n    input_ids = tokens.input_ids.to(self.model.device)\n    attention_mask = tokens.attention_mask.to(self.model.device)\n\n    iterator_streamer = TextIteratorStreamer(\n        tokenizer=self.tokenizer,\n        skip_prompt=True,\n        skip_special_tokens=True\n    )\n\n    if stream:\n        kwargs = dict(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            streamer=iterator_streamer,\n            generation_config=transformers.GenerationConfig(\n                bos_token_id=self.tokenizer.bos_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n                pad_token_id=self.tokenizer.pad_token_id,\n                max_length=max_length or self.config.max_length,\n                temperature=temperature,\n                top_k=top_k,\n                top_p=top_p,\n                max_new_tokens=max_new_tokens or self.config.max_new_tokens,\n                num_beams=1,\n                do_sample=sample\n            )\n        )\n        thread_ = threading.Thread(\n            target=self.model.generate,\n            kwargs=kwargs\n        )\n        thread_.start()\n        for string in iterator_streamer:\n            yield string\n    else:\n        kwargs = dict(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            generation_config=transformers.GenerationConfig(\n                bos_token_id=self.tokenizer.bos_token_id,\n                eos_token_id=self.tokenizer.eos_token_id,\n                pad_token_id=self.tokenizer.pad_token_id,\n                max_length=max_length or self.config.max_length,\n                temperature=temperature,\n                top_k=top_k,\n                top_p=top_p,\n                max_new_tokens=max_new_tokens or self.config.max_new_tokens,\n                num_beams=1\n            )\n        )\n        pred = self.tokenizer.decode(self.model.generate(\n            **kwargs\n        ).logits[0])\n        return pred\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PyTorchServer.process_gradio_chat","title":"<code>process_gradio_chat(prompt, history, max_new_tokens, temperature, max_length, top_p, top_k)</code>","text":"<p>The process_gradio_chat function is a wrapper for the process function. It takes in the same arguments as process, but also takes in prompt and history. The prompt is appended to the history list, which contains all of the previous messages sent by both parties. The format_chat function formats this information into a string that can be fed into GPT-2's model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>prompt</code> <code>str</code> <p>str: Set the prompt for the chatbot</p> required <code>history</code> <code>List[str]</code> <p>List[str]: Store the history of the conversation</p> required <code>max_new_tokens</code> <code>int</code> <p>int: Limit the number of tokens that can be generated in a single response</p> required <code>temperature</code> <code>float</code> <p>float: Control the randomness of the text generation</p> required <code>max_length</code> <code>int</code> <p>int: Set the maximum length of the response</p> required <code>top_p</code> <code>float</code> <p>float: Control the randomness of the model</p> required <code>top_k</code> <code>int</code> <p>int: Control the number of tokens that are filtered from the top-k filtering</p> required <p>Returns:</p> Type Description <p>A generator object, which is a type of iterator</p> Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>def process_gradio_chat(self,\n                        prompt: str,\n                        history: List[str],\n                        max_new_tokens: int,\n                        temperature: float,\n                        max_length: int,\n                        top_p: float,\n                        top_k: int\n                        ):\n    \"\"\"\n    The process_gradio_chat function is a wrapper for the process function.\n    It takes in the same arguments as process, but also takes in prompt and history.\n    The prompt is appended to the history list, which contains all of the previous messages sent by both parties.\n    The format_chat function formats this information into a string that can be fed into GPT-2's model.\n\n    :param self: Refer to the object itself\n    :param prompt: str: Set the prompt for the chatbot\n    :param history: List[str]: Store the history of the conversation\n    :param max_new_tokens: int: Limit the number of tokens that can be generated in a single response\n    :param temperature: float: Control the randomness of the text generation\n    :param max_length: int: Set the maximum length of the response\n    :param top_p: float: Control the randomness of the model\n    :param top_k: int: Control the number of tokens that are filtered from the top-k filtering\n    :return: A generator object, which is a type of iterator\n\n    \"\"\"\n    string = self.format_chat(prompt=prompt, history=history, system=None)\n    history.append([prompt, ''])\n    responses = ''\n    for response in self.process(\n            string=string,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            max_length=max_length,\n            top_p=top_p,\n            top_k=top_k,\n            stream=True\n    ):\n        responses += response\n        history[-1][-1] = responses\n        yield '', history\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PyTorchServer.process_gradio_instruct","title":"<code>process_gradio_instruct(instruction, system, max_new_tokens, temperature, max_length, top_p, top_k)</code>","text":"<p>The process_gradio_instruct function is a wrapper for the process function. It takes in an instruction and system, formats them into a string, then passes that string to the process function. The output of this function is formatted so that it can be used with gradio's stream_func decorator.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <code>instruction</code> <code>str</code> <p>str: Pass the instruction to the model</p> required <code>system</code> <code>str</code> <p>str: Specify the system that is being used for the chatbot</p> required <code>max_new_tokens</code> <code>int</code> <p>int: Specify the maximum number of tokens that can be generated</p> required <code>temperature</code> <code>float</code> <p>float: Control the randomness of the output</p> required <code>max_length</code> <code>int</code> <p>int: Set the maximum length of the response</p> required <code>top_p</code> <code>float</code> <p>float: Control the randomness of the model</p> required <code>top_k</code> <code>int</code> <p>int: Limit the number of tokens that are considered for each position</p> required <p>Returns:</p> Type Description <p>A generator object</p> Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>def process_gradio_instruct(self,\n                            instruction: str,\n                            system: str,\n                            max_new_tokens: int,\n                            temperature: float,\n                            max_length: int,\n                            top_p: float,\n                            top_k: int\n                            ):\n    \"\"\"\n    The process_gradio_instruct function is a wrapper for the process function.\n    It takes in an instruction and system, formats them into a string, then passes that string to the process function.\n    The output of this function is formatted so that it can be used with gradio's stream_func decorator.\n\n    :param self: Refer to the object itself\n    :param instruction: str: Pass the instruction to the model\n    :param system: str: Specify the system that is being used for the chatbot\n    :param max_new_tokens: int: Specify the maximum number of tokens that can be generated\n    :param temperature: float: Control the randomness of the output\n    :param max_length: int: Set the maximum length of the response\n    :param top_p: float: Control the randomness of the model\n    :param top_k: int: Limit the number of tokens that are considered for each position\n    :return: A generator object\n\n    \"\"\"\n    string = self.format_instruct(system=system, instruction=instruction)\n    responses = ''\n    for response in self.process(\n            string=string,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            max_length=max_length,\n            top_p=top_p,\n            top_k=top_k,\n            stream=True\n    ):\n        responses += response\n        yield '', response\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PyTorchServer.status","title":"<code>status()</code>","text":"<p>The status function returns a dictionary with the following keys:     config: A dictionary of configuration parameters.     devices: The number of GPUs available to the server.     device_sharding: Whether device sharding is enabled. If True, then each request will be served by     a different GPU (if multiple GPUs are available). If False, then all requests will be served by     the same GPU (or CPU if no GPUs are available). This parameter can also be set in your client's     initialization function via torch-serve's DeviceShardingStrategy     class. See https://pytorch-lightning.readthedoc</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A dictionary with the following keys:</p> Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>def status(self):\n\n    \"\"\"\n    The status function returns a dictionary with the following keys:\n        config: A dictionary of configuration parameters.\n        devices: The number of GPUs available to the server.\n        device_sharding: Whether device sharding is enabled. If True, then each request will be served by\n        a different GPU (if multiple GPUs are available). If False, then all requests will be served by\n        the same GPU (or CPU if no GPUs are available). This parameter can also be set in your client's\n        initialization function via torch-serve's DeviceShardingStrategy\n        class. See https://pytorch-lightning.readthedoc\n\n    :param self: Represent the instance of the class\n    :return: A dictionary with the following keys:\n\n    \"\"\"\n    return {\n        'config': {k: v for k, v in self.config.__dict__.items()},\n        'devices': f\"{torch.cuda.device_count()}\",\n        'device_sharding': self.device_rolling,\n        'max_memory': self.dict_max_memory_sharding,\n        'status': 'Ready',\n        'number_of_served_request_until_last_up_time': f\"{self.number_of_served_request_until_last_up_time}\"\n    }\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PytorchServerConfig","title":"<code>PytorchServerConfig</code>","text":"Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>class PytorchServerConfig:\n    def __init__(self,\n                 host='0.0.0.0',\n                 port=2059,\n                 batch_size=1,\n                 contains_auto_format=True,\n                 max_length=2048,\n                 max_new_tokens=2048,\n                 temperature=0.8,\n                 top_p=0.95,\n                 top_k=50,\n                 logging=True,\n                 dtype='fp16',\n                 max_number_of_gpus=None,\n                 max_gpu_perc_to_use=0.95,\n                 max_stream_tokens: int = 1\n                 ):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It sets up the instance of the class, and defines all its attributes.\n\n\n        :param self: Represent the instance of the class\n        :param host: Specify the ip address of the server\n        :param port: Specify the port number that will be used by the server\n        :param batch_size: Determine the number of samples to be generated in a single batch\n        :param contains_auto_format: Determine whether the input text contains auto_formatting\n        :param max_length: Set the maximum length of a sentence\n        :param max_new_tokens: Limit the number of new tokens that can be generated in a single batch\n        :param temperature: Control the randomness of the generated text\n        :param top_p: Control the probability of sampling from the top candidates\n        :param top_k: Limit the number of tokens that are considered for each token\n        :param logging: Control whether the server will print out\n        :param dtype: Specify the data type of the tensors\n        :param max_number_of_gpus: Limit the number of gpus used by the server\n        :param max_gpu_perc_to_use: Specify the maximum percentage of gpu memory that can be used by the server\n        :param max_stream_tokens: int: Limit the number of tokens that can be streamed to a single client\n        :return: Nothing\n\n        \"\"\"\n        self.host = host\n        self.port = port\n        self.batch_size = batch_size\n        self.contains_auto_format = contains_auto_format\n        self.max_length = max_length\n        self.max_new_tokens = max_new_tokens\n        self.temperature = temperature\n        self.top_p = top_p\n        self.top_k = top_k\n        self.logging = logging\n        self.dtype = dtype\n        self.max_number_of_gpus = max_number_of_gpus\n        self.max_gpu_perc_to_use = max_gpu_perc_to_use\n        self.max_stream_tokens = max_stream_tokens\n</code></pre>"},{"location":"lib-python-EasyDel-serve-torch_serve/#lib.python.EasyDel.serve.torch_serve.PytorchServerConfig.__init__","title":"<code>__init__(host='0.0.0.0', port=2059, batch_size=1, contains_auto_format=True, max_length=2048, max_new_tokens=2048, temperature=0.8, top_p=0.95, top_k=50, logging=True, dtype='fp16', max_number_of_gpus=None, max_gpu_perc_to_use=0.95, max_stream_tokens=1)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the instance of the class, and defines all its attributes.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>host</code> <p>Specify the ip address of the server</p> <code>'0.0.0.0'</code> <code>port</code> <p>Specify the port number that will be used by the server</p> <code>2059</code> <code>batch_size</code> <p>Determine the number of samples to be generated in a single batch</p> <code>1</code> <code>contains_auto_format</code> <p>Determine whether the input text contains auto_formatting</p> <code>True</code> <code>max_length</code> <p>Set the maximum length of a sentence</p> <code>2048</code> <code>max_new_tokens</code> <p>Limit the number of new tokens that can be generated in a single batch</p> <code>2048</code> <code>temperature</code> <p>Control the randomness of the generated text</p> <code>0.8</code> <code>top_p</code> <p>Control the probability of sampling from the top candidates</p> <code>0.95</code> <code>top_k</code> <p>Limit the number of tokens that are considered for each token</p> <code>50</code> <code>logging</code> <p>Control whether the server will print out</p> <code>True</code> <code>dtype</code> <p>Specify the data type of the tensors</p> <code>'fp16'</code> <code>max_number_of_gpus</code> <p>Limit the number of gpus used by the server</p> <code>None</code> <code>max_gpu_perc_to_use</code> <p>Specify the maximum percentage of gpu memory that can be used by the server</p> <code>0.95</code> <code>max_stream_tokens</code> <code>int</code> <p>int: Limit the number of tokens that can be streamed to a single client</p> <code>1</code> <p>Returns:</p> Type Description <p>Nothing</p> Source code in <code>lib/python/EasyDel/serve/torch_serve.py</code> <pre><code>def __init__(self,\n             host='0.0.0.0',\n             port=2059,\n             batch_size=1,\n             contains_auto_format=True,\n             max_length=2048,\n             max_new_tokens=2048,\n             temperature=0.8,\n             top_p=0.95,\n             top_k=50,\n             logging=True,\n             dtype='fp16',\n             max_number_of_gpus=None,\n             max_gpu_perc_to_use=0.95,\n             max_stream_tokens: int = 1\n             ):\n    \"\"\"\n    The __init__ function is called when the class is instantiated.\n    It sets up the instance of the class, and defines all its attributes.\n\n\n    :param self: Represent the instance of the class\n    :param host: Specify the ip address of the server\n    :param port: Specify the port number that will be used by the server\n    :param batch_size: Determine the number of samples to be generated in a single batch\n    :param contains_auto_format: Determine whether the input text contains auto_formatting\n    :param max_length: Set the maximum length of a sentence\n    :param max_new_tokens: Limit the number of new tokens that can be generated in a single batch\n    :param temperature: Control the randomness of the generated text\n    :param top_p: Control the probability of sampling from the top candidates\n    :param top_k: Limit the number of tokens that are considered for each token\n    :param logging: Control whether the server will print out\n    :param dtype: Specify the data type of the tensors\n    :param max_number_of_gpus: Limit the number of gpus used by the server\n    :param max_gpu_perc_to_use: Specify the maximum percentage of gpu memory that can be used by the server\n    :param max_stream_tokens: int: Limit the number of tokens that can be streamed to a single client\n    :return: Nothing\n\n    \"\"\"\n    self.host = host\n    self.port = port\n    self.batch_size = batch_size\n    self.contains_auto_format = contains_auto_format\n    self.max_length = max_length\n    self.max_new_tokens = max_new_tokens\n    self.temperature = temperature\n    self.top_p = top_p\n    self.top_k = top_k\n    self.logging = logging\n    self.dtype = dtype\n    self.max_number_of_gpus = max_number_of_gpus\n    self.max_gpu_perc_to_use = max_gpu_perc_to_use\n    self.max_stream_tokens = max_stream_tokens\n</code></pre>"},{"location":"lib-python-EasyDel-serve-utils/","title":"serve.utils","text":""},{"location":"lib-python-EasyDel-serve-utils/#lib.python.EasyDel.serve.utils.Seafoam","title":"<code>Seafoam</code>","text":"<p>             Bases: <code>Base</code></p> Source code in <code>lib/python/EasyDel/serve/utils.py</code> <pre><code>class Seafoam(Base):\n    def __init__(\n            self,\n            *,\n            primary_hue: Union[colors.Color, str] = colors.emerald,\n            secondary_hue: Union[colors.Color, str] = colors.blue,\n            neutral_hue: Union[colors.Color, str] = colors.gray,\n            spacing_size: Union[sizes.Size, str] = sizes.spacing_md,\n            radius_size: Union[sizes.Size, str] = sizes.radius_md,\n            text_size: Union[sizes.Size, str] = sizes.text_lg,\n            font: Union[fonts.Font, str]\n            = (\n                    fonts.GoogleFont(\"Quicksand\"),\n                    \"ui-sans-serif\",\n                    \"sans-serif\",\n            ),\n            font_mono: Union[fonts.Font, str]\n            = (\n                    fonts.GoogleFont(\"IBM Plex Mono\"),\n                    \"ui-monospace\",\n                    \"monospace\",\n            ),\n    ):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It sets up the object with all of its instance variables and other things it needs to function properly.\n\n\n        :param self: Represent the instance of the object\n        :param *: Unpack the list of parameters into a tuple\n        :param primary_hue: Union[colors.Color,str]: Set the primary color of the theme\n        :param secondary_hue: Union[colors.Color,str]: Set the secondary color of the theme\n        :param neutral_hue: Union[colors.Color,str]: Set the neutral color of the theme\n        :param spacing_size: Union[sizes.Size,str]: Set the spacing size of the theme\n        :param radius_size: Union[sizes.Size,str]: Set the radius of the buttons and other elements\n        :param text_size: Union[sizes.Size,str]: Set the size of the text in the app\n\n        :return: The class object\n\n        \"\"\"\n\n        super().__init__(\n            primary_hue=primary_hue,\n            secondary_hue=secondary_hue,\n            neutral_hue=neutral_hue,\n            spacing_size=spacing_size,\n            radius_size=radius_size,\n            text_size=text_size,\n            font=font,\n            font_mono=font_mono,\n\n        )\n        super().set(\n            body_background_fill=\"linear-gradient(90deg, *secondary_800, *neutral_900)\",\n            body_background_fill_dark=\"linear-gradient(90deg, *secondary_800, *neutral_900)\",\n            button_primary_background_fill=\"linear-gradient(90deg, *primary_300, *secondary_400)\",\n            button_primary_background_fill_hover=\"linear-gradient(90deg, *primary_200, *secondary_300)\",\n            button_primary_text_color=\"white\",\n            button_primary_background_fill_dark=\"linear-gradient(90deg, *primary_600, *secondary_800)\",\n            slider_color=\"*secondary_300\",\n            slider_color_dark=\"*secondary_400\",\n            block_title_text_weight=\"600\",\n            block_border_width=\"0px\",\n            block_shadow=\"*shadow_drop_lg\",\n            button_shadow=\"*shadow_drop_lg\",\n            button_large_padding=\"4px\",\n        )\n</code></pre>"},{"location":"lib-python-EasyDel-serve-utils/#lib.python.EasyDel.serve.utils.Seafoam.__init__","title":"<code>__init__(*, primary_hue=colors.emerald, secondary_hue=colors.blue, neutral_hue=colors.gray, spacing_size=sizes.spacing_md, radius_size=sizes.radius_md, text_size=sizes.text_lg, font=(fonts.GoogleFont('Quicksand'), 'ui-sans-serif', 'sans-serif'), font_mono=(fonts.GoogleFont('IBM Plex Mono'), 'ui-monospace', 'monospace'))</code>","text":"<p>The init function is called when the class is instantiated. It sets up the object with all of its instance variables and other things it needs to function properly.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the object</p> required <code>*</code> <p>Unpack the list of parameters into a tuple</p> required <code>primary_hue</code> <code>Union[Color, str]</code> <p>Union[colors.Color,str]: Set the primary color of the theme</p> <code>emerald</code> <code>secondary_hue</code> <code>Union[Color, str]</code> <p>Union[colors.Color,str]: Set the secondary color of the theme</p> <code>blue</code> <code>neutral_hue</code> <code>Union[Color, str]</code> <p>Union[colors.Color,str]: Set the neutral color of the theme</p> <code>gray</code> <code>spacing_size</code> <code>Union[Size, str]</code> <p>Union[sizes.Size,str]: Set the spacing size of the theme</p> <code>spacing_md</code> <code>radius_size</code> <code>Union[Size, str]</code> <p>Union[sizes.Size,str]: Set the radius of the buttons and other elements</p> <code>radius_md</code> <code>text_size</code> <code>Union[Size, str]</code> <p>Union[sizes.Size,str]: Set the size of the text in the app</p> <code>text_lg</code> <p>Returns:</p> Type Description <p>The class object</p> Source code in <code>lib/python/EasyDel/serve/utils.py</code> <pre><code>def __init__(\n        self,\n        *,\n        primary_hue: Union[colors.Color, str] = colors.emerald,\n        secondary_hue: Union[colors.Color, str] = colors.blue,\n        neutral_hue: Union[colors.Color, str] = colors.gray,\n        spacing_size: Union[sizes.Size, str] = sizes.spacing_md,\n        radius_size: Union[sizes.Size, str] = sizes.radius_md,\n        text_size: Union[sizes.Size, str] = sizes.text_lg,\n        font: Union[fonts.Font, str]\n        = (\n                fonts.GoogleFont(\"Quicksand\"),\n                \"ui-sans-serif\",\n                \"sans-serif\",\n        ),\n        font_mono: Union[fonts.Font, str]\n        = (\n                fonts.GoogleFont(\"IBM Plex Mono\"),\n                \"ui-monospace\",\n                \"monospace\",\n        ),\n):\n    \"\"\"\n    The __init__ function is called when the class is instantiated.\n    It sets up the object with all of its instance variables and other things it needs to function properly.\n\n\n    :param self: Represent the instance of the object\n    :param *: Unpack the list of parameters into a tuple\n    :param primary_hue: Union[colors.Color,str]: Set the primary color of the theme\n    :param secondary_hue: Union[colors.Color,str]: Set the secondary color of the theme\n    :param neutral_hue: Union[colors.Color,str]: Set the neutral color of the theme\n    :param spacing_size: Union[sizes.Size,str]: Set the spacing size of the theme\n    :param radius_size: Union[sizes.Size,str]: Set the radius of the buttons and other elements\n    :param text_size: Union[sizes.Size,str]: Set the size of the text in the app\n\n    :return: The class object\n\n    \"\"\"\n\n    super().__init__(\n        primary_hue=primary_hue,\n        secondary_hue=secondary_hue,\n        neutral_hue=neutral_hue,\n        spacing_size=spacing_size,\n        radius_size=radius_size,\n        text_size=text_size,\n        font=font,\n        font_mono=font_mono,\n\n    )\n    super().set(\n        body_background_fill=\"linear-gradient(90deg, *secondary_800, *neutral_900)\",\n        body_background_fill_dark=\"linear-gradient(90deg, *secondary_800, *neutral_900)\",\n        button_primary_background_fill=\"linear-gradient(90deg, *primary_300, *secondary_400)\",\n        button_primary_background_fill_hover=\"linear-gradient(90deg, *primary_200, *secondary_300)\",\n        button_primary_text_color=\"white\",\n        button_primary_background_fill_dark=\"linear-gradient(90deg, *primary_600, *secondary_800)\",\n        slider_color=\"*secondary_300\",\n        slider_color_dark=\"*secondary_400\",\n        block_title_text_weight=\"600\",\n        block_border_width=\"0px\",\n        block_shadow=\"*shadow_drop_lg\",\n        button_shadow=\"*shadow_drop_lg\",\n        button_large_padding=\"4px\",\n    )\n</code></pre>"},{"location":"lib-python-EasyDel-smi-smi/","title":"smi.smi","text":""},{"location":"lib-python-EasyDel-smi-smi/#lib.python.EasyDel.smi.smi.get_mem","title":"<code>get_mem(dir_prefix='/dev/shm')</code>","text":"<p>The get_mem function is a wrapper around the go tool pprof command. It takes in an optional argument, dir_prefix, which defaults to /dev/shm. The function then runs the go tool pprof command with arguments -tags and dir_prefix/memory.prof, and returns its stdout as a string.</p> <p>Parameters:</p> Name Type Description Default <code>dir_prefix</code> <code>str</code> <p>str: Specify the directory where</p> <code>'/dev/shm'</code> <p>Returns:</p> Type Description <p>A string of the memory profile</p> Source code in <code>lib/python/EasyDel/smi/smi.py</code> <pre><code>def get_mem(dir_prefix: str = '/dev/shm'):\n    \"\"\"\n    The get_mem function is a wrapper around the go tool pprof command.\n    It takes in an optional argument, dir_prefix, which defaults to /dev/shm.\n    The function then runs the go tool pprof command with arguments -tags and dir_prefix/memory.prof,\n    and returns its stdout as a string.\n\n    :param dir_prefix: str: Specify the directory where\n    :return: A string of the memory profile\n\n    \"\"\"\n    return subprocess.run(\n        args=['go', 'tool', 'pprof', '-tags', f'{dir_prefix}/memory.prof'],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.DEVNULL,\n    ).stdout.decode('utf-8')\n</code></pre>"},{"location":"lib-python-EasyDel-smi-smi/#lib.python.EasyDel.smi.smi.initialise_tracking","title":"<code>initialise_tracking(interval=1.0, dir_prefix='/dev/shm')</code>","text":"<p>The initialise_tracking function starts a daemon thread that periodically saves the current memory profile to disk.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>float</code> <p>float: Specify the time interval between each memory profile</p> <code>1.0</code> <code>dir_prefix</code> <code>str</code> <p>str: Specify the directory where the memory profile will be saved</p> <code>'/dev/shm'</code> <p>Returns:</p> Type Description <code>None</code> <p>Nothing, but it starts a thread that</p> Source code in <code>lib/python/EasyDel/smi/smi.py</code> <pre><code>def initialise_tracking(interval: float = 1., dir_prefix: str = '/dev/shm') -&gt; None:\n    \"\"\"\n    The initialise_tracking function starts a daemon thread that periodically saves the current memory profile to disk.\n\n    :param interval: float: Specify the time interval between each memory profile\n    :param dir_prefix: str: Specify the directory where the memory profile will be saved\n    :return: Nothing, but it starts a thread that\n\n    \"\"\"\n\n    def inner():\n        while True:\n            jax.profiler.save_device_memory_profile(f'{dir_prefix}/memory.prof.new')\n            os.rename(f'{dir_prefix}/memory.prof.new', f'{dir_prefix}/memory.prof')\n            time.sleep(interval)\n\n    thread = threading.Thread(target=inner, daemon=True)\n    thread.start()\n</code></pre>"},{"location":"lib-python-EasyDel-smi-smi/#lib.python.EasyDel.smi.smi.run","title":"<code>run(note_book=None, interval=1, dir_prefix='/dev/shm', dpr=True)</code>","text":"<p>The run function is a simple wrapper around the go tool pprof command. It runs the command every interval seconds and prints out its output to stdout. If you are running this in a notebook, it will print to IPython's display instead of stdout.</p> <p>Parameters:</p> Name Type Description Default <code>note_book</code> <p>Determine whether the program is running in a notebook or not</p> <code>None</code> <code>interval</code> <code>float</code> <p>float: Specify the time interval between each refresh</p> <code>1</code> <code>dir_prefix</code> <code>str</code> <p>str: Specify the directory where the memory</p> <code>'/dev/shm'</code> <code>dpr</code> <p>Control whether the output is displayed in a notebook or not</p> <code>True</code> <p>Returns:</p> Type Description <p>The output of the pprof command</p> Source code in <code>lib/python/EasyDel/smi/smi.py</code> <pre><code>def run(note_book=None, interval: float = 1, dir_prefix: str = '/dev/shm', dpr=True):\n    \"\"\"\n    The run function is a simple wrapper around the go tool pprof command.\n    It runs the command every interval seconds and prints out its output to stdout.\n    If you are running this in a notebook, it will print to IPython's display instead of stdout.\n\n\n    :param note_book: Determine whether the program is running in a notebook or not\n    :param interval: float: Specify the time interval between each refresh\n    :param dir_prefix: str: Specify the directory where the memory\n    :param dpr: Control whether the output is displayed in a notebook or not\n    :return: The output of the pprof command\n\n    \"\"\"\n    if note_book is None:\n        import os\n\n        def is_notebook():\n            \"\"\"Returns True if the code is being run in a notebook, False otherwise.\"\"\"\n            return os.environ.get(\"IPYTHON\") is not None\n\n        note_book = is_notebook()\n    std = curses.initscr() if not note_book else None\n    try:\n        while True:\n            if not note_book and dpr:\n                std.clear()\n            output = subprocess.run(\n                args=['go', 'tool', 'pprof', '-tags', f'{dir_prefix}/memory.prof'],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.DEVNULL,\n            ).stdout.decode('utf-8')\n            if not note_book and dpr:\n                std.addstr(output)\n                std.refresh()\n            if note_book and dpr:\n                IPython.display.clear_output(True)\n                print(output)\n\n            with open(f'{dir_prefix}/memory.json', 'w') as fin:\n                json.dump({\n                    'log': output\n                }, fin)\n            time.sleep(interval)\n    except KeyboardInterrupt:\n        curses.endwin()\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-config/","title":"trainer.config","text":""},{"location":"lib-python-EasyDel-trainer-config/#lib.python.EasyDel.trainer.config.TrainArguments","title":"<code>TrainArguments</code>","text":"<p>             Bases: <code>OrderedDict</code></p> Source code in <code>lib/python/EasyDel/trainer/config.py</code> <pre><code>class TrainArguments(\n    OrderedDict\n):\n    def __init__(\n            self,\n            model_name: str,\n            num_train_epochs: int,\n            model_id: str = None,\n            model_class=None,\n            total_batch_size: int = 32,\n            max_steps: Union[int, None] = None,\n            optimizer: str = 'lion',\n            scheduler: str = 'linear',\n            learning_rate: Union[int, float] = 5e-5,\n            learning_rate_end: Union[None, float] = 5e-6,\n            gradient_accumulation_steps: int = 1,\n            weight_decay: float = 0.01,\n            gradient_checkpointing: str = 'nothing_saveable',\n            max_length: Union[int, None] = 4096,\n            sharding_array: Union[tuple, int] = (1, -1, 1, 1),\n            is_fine_tuning: bool = True,\n            do_train: bool = True,\n            do_eval: bool = False,\n            do_test: Union[bool, None] = False,\n            backend: Union[str, None] = None,\n            extra_optimizer_kwargs: dict = None,\n            save_steps: Union[int, None] = None,\n            save_dir: str = 'easydel_ckpt',\n            use_pjit_attention_force: bool = False,\n            dtype=jnp.bfloat16,\n            param_dtype=jnp.bfloat16,\n            fully_fsdp=True,\n            use_wandb: bool = True,\n            custom_rule=None,\n            extra_configs=None,\n            ids_to_pop_from_dataset: list = None,\n            remove_ckpt_after_load: bool = False,\n            configs_to_init_model_class=None,\n            do_last_save: bool = True,\n            model_parameters=None,\n            do_shard_fns: bool = True,\n            track_memory: bool = True,\n            loss_remat: str = '',\n            loss_chunk: int = 1024,\n            is_left_padded: bool = False,\n            warmup_steps: int = 500,\n            init_input_shape: typing.Tuple[int, int] = (1, 1),\n            step_partition_spec: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp'),\n            **kwargs\n    ):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It sets up the instance of the class, and makes sure that it has all of\n        the attributes necessary for proper functioning. It also allows you to set\n        default values for those attributes if they are not provided as arguments by\n        the person creating an instance.\n\n        :param self: Refer to the class instance itself\n        :param model_name: str: Specify the model name\n        :param num_train_epochs: int: Set the number of epochs for training\n        :param model_id: str: Load a model from the save_dir\n        :param model_class: Initialize the model, and the configs_to_init_model_class parameter is used to\n        :param total_batch_size: int: Set the batch size of the model\n        :param max_steps: Union[int,None]: Determine the maximum number of steps to train for\n        :param optimizer: str: Specify which optimizer to use\n        :param scheduler: str: Set the learning rate scheduler\n        :param learning_rate: Union[int,float]: Set the learning rate , Set the dtype of the model parameters\n        :param learning_rate_end: Union[None,float]: Set the end learning rate, Set the dtype of the model parameters\n        :param gradient_accumulation_steps: int: Accumulate gradients over multiple batches\n        :param weight_decay: float: Control the weight decay\n        :param gradient_checkpointing: str: Control the gradient checkpointing method\n        :param max_length: Union[int, None]: Set the maximum length of a sequence, Pass the model_class to the trainer class\n        :param sharding_array: Union[tuple: Shard the model across multiple devices\n        :param is_fine_tuning: bool: Determine whether the model is being trained from scratch or not\n        :param do_train: bool: Determine whether the model should be trained or not\n        :param do_eval: bool: Determine whether to run the eval loop or not\n        :param do_test: Union[bool,None]: Determine whether to run the test or not, Pass the model_class to the trainer\n        :param backend: Union[str, None]:: Specify the device that will be used for training, Define the default value of a parameter\n        :param extra_optimizer_kwargs: dict: Pass extra arguments to the optimizer\n        :param save_steps: Union[int,None]: Save the model after a number of steps,  Set the default value of do_test to none\n        :param save_dir: str: Specify the directory where the model checkpoints will be saved\n        :param use_pjit_attention_force: bool: Determine whether to use the jax\n        :param dtype: Set the data type of the model parameters and inputs\n        :param param_dtype: Specify the data type of the model parameters\n        :param fully_fsdp: Control the use of fully fused sdp\n        :param use_wandb: bool: Determine whether to use wandb or not\n        :param custom_rule: Pass a custom rule to the optimizer,\n        :param extra_configs: Pass extra configurations to the model class\n        :param ids_to_pop_from_dataset: list: Pop some keys from the dataset,\n        :param remove_ckpt_after_load: bool: Remove the checkpoint after loading it\n        :param configs_to_init_model_class: Pass the configs to the model class\n        :param do_last_save: bool: Save the model at the end of training\n        :param model_parameters: Pass the model parameters to the trainer\n        :param do_shard_fns: bool: Shard the model across multiple devices\n        :param track_memory: bool: Track the memory usage of the model\n        :param loss_remat: str: Specify how to rematerialize the loss function\n        :param loss_chunk: int: Chunk the loss function\n        :param is_left_padded: bool: Indicate whether the input is left padded or not\n        :param warmup_steps: int: Warm up the learning rate\n        :param init_input_shape: typing.Tuple[int]: Initialize the input shape of the model\n        :param step_partition_spec: jax.sharding.PartitionSpec: PartitionSpec Custom to be used in training and eval or test loop\n        :param **kwargs: Pass a variable number of keyword arguments to a function\n        :return: Nothing\n\n        \"\"\"\n        super().__init__()\n        if ids_to_pop_from_dataset is None:\n            ids_to_pop_from_dataset = []\n        if extra_optimizer_kwargs is None:\n            extra_optimizer_kwargs = {}\n        assert model_class is not None or model_id is not None, 'you cant pass model_class and model_id both None ' \\\n                                                                'you should at least pass one of them to build ' \\\n                                                                'model with'\n        assert backend in AVAILABLE_BACKENDS, f'{backend} is not recognized, ' \\\n                                              f'available backends are {AVAILABLE_BACKENDS}'\n        assert gradient_checkpointing in AVAILABLE_GRADIENT_CHECK_POINTING, f'{gradient_checkpointing} is not ' \\\n                                                                            f'recognized, available gradient ' \\\n                                                                            f'checkpointing methods are ' \\\n                                                                            f'{AVAILABLE_GRADIENT_CHECK_POINTING}'\n        assert scheduler in AVAILABLE_SCHEDULERS, f'{scheduler} is not recognized, ' \\\n                                                  f'available schedulers are {AVAILABLE_SCHEDULERS}'\n        assert optimizer in AVAILABLE_OPTIMIZERS, f'{optimizer} is not recognized, ' \\\n                                                  f'available optimizers are {AVAILABLE_OPTIMIZERS}'\n        self.available_backends = len(jax.devices(backend))\n        total_batch_size *= gradient_accumulation_steps\n        array_devices = jnp.ones((self.available_backends, 1)).reshape(sharding_array)\n        self.array_devices_shape = array_devices.shape\n\n        self.model_id = model_id\n        self.num_train_epochs = num_train_epochs\n        self.total_batch_size = total_batch_size\n        self.max_steps = max_steps\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.extra_optimizer_kwargs = extra_optimizer_kwargs\n        self.learning_rate = learning_rate\n        self.learning_rate_end = learning_rate_end\n        self.weight_decay = weight_decay\n        self.model_name = model_name\n        self.gradient_checkpointing = gradient_checkpointing\n        self.max_length = max_length\n        self.sharding_array = sharding_array\n        self.is_fine_tuning = is_fine_tuning\n        self.do_train = do_train\n        self.do_eval = do_eval\n        self.do_test = do_test\n        self.save_steps = save_steps\n        self.save_dir = save_dir\n        self.use_pjit_attention_force = use_pjit_attention_force\n        self.dtype = dtype\n        self.warmup_steps = warmup_steps\n        self.param_dtype = param_dtype\n        self.fully_fsdp = fully_fsdp\n        self.use_wandb = use_wandb\n        self.custom_rule = custom_rule\n        self.extra_configs = extra_configs\n        self.ids_to_pop_from_dataset = ids_to_pop_from_dataset\n        self.remove_ckpt_after_load = remove_ckpt_after_load\n        self.model_class = model_class\n        self.configs_to_init_model_class = configs_to_init_model_class\n        self.do_last_save = do_last_save\n        self.model_parameters = model_parameters\n        self.do_shard_fns = do_shard_fns\n        self.gradient_accumulation_steps = gradient_accumulation_steps\n        self.track_memory = track_memory\n\n        self.loss_chunk = loss_chunk\n        self.loss_remat = loss_remat\n        self.init_input_shape = init_input_shape\n        self.is_left_padded = is_left_padded\n        self.step_partition_spec = step_partition_spec\n        torch.set_default_device('cpu')\n        self.__dict__.update(**kwargs)\n\n    def __call__(self):\n        return {k: v for k, v in self.__dict__.items()}\n\n    def get_meter_dict(self):\n        \"\"\"\n        The get_meter_dict function is used to return a dictionary of the hyperparameters.\n        The function iterates through all the attributes in the class and returns a dictionary with\n        the key as &amp;quot;hyperparameters/{k}&amp;quot; and value as v for each attribute k,v in self.__dict__ if it is an instance of int, float, str, bool or torch.Tensor.\n\n        :param self: Represent the instance of the class\n        :return: A dictionary of hyperparameters\n\n        \"\"\"\n        return {f\"hyperparameters/{k}\": v for k, v in self.__dict__.items() if\n                isinstance(v, (int, float, str, bool, torch.Tensor))}\n\n    def get_wandb_init(self):\n        \"\"\"\n        The get_wandb_init function is a helper function that returns the wandb.init() call with\n        the project name, config object, and tags set to appropriate values for this model.\n\n        :param self: Pass the class instance to the function\n        :return: A wandb\n\n        \"\"\"\n        return wandb.init(\n            project=f'easydel-{self.model_name}',\n            config=self(),\n            tags=[\n                'Easy Del',\n                'OST-OpenSourceTransformers',\n                'Jax/Flax'\n            ]\n        )\n\n    def __str__(self):\n        string = f'TrainingArguments(\\n'\n        for k, v in self.__call__().items():\n            if isinstance(v, typing.Callable):\n                def string_func(it_self):\n                    string_ = f'{it_self.__class__.__name__}(\\n'\n                    for k_, v_ in it_self.__dict__.items():\n                        string_ += f'\\t\\t{k_} : {v_}\\n'\n                    string_ += '\\t)'\n                    return string_\n\n                v.__str__ = string_func\n                v = v.__str__(v)\n            string += f'\\t{k} : {v}\\n'\n        string += ')'\n        return string\n\n    def get_path(self):\n        \"\"\"\n        The get_path function returns a pathlib.Path object, which is a class that\n        represents file paths and provides methods for interacting with the files at\n        those paths. The get_path function takes no arguments and returns an instance of\n        the Path class initialized with two arguments: self.save_dir (a string) and\n        self.model_name (also a string). The save directory is the directory where we'll\n        store our model checkpoints, while the model name will be used to create unique\n        filenames for each checkpoint.\n\n        :param self: Represent the instance of the class\n        :return: A pathlib\n\n        \"\"\"\n        return pathlib.Path(\n            self.save_dir, self.model_name\n        )\n\n    def ckpt_path_exists(self):\n        \"\"\"\n        The ckpt_path_exists function checks to see if the path exists. If it does not, then it creates a new directory.\n\n        :param self: Represent the instance of the class\n        :return: A path\n\n        \"\"\"\n        path = self.get_path()\n        if not path.exists():\n            path.mkdir(parents=True)\n\n    def get_mesh(self):\n        \"\"\"\n        The get_mesh function is used to create a mesh object that can be used\n        to define the geometry of the device. The mesh object contains two arrays:\n        a list of vertices and a list of faces. Each face is defined by three indices,\n        which correspond to three vertices in the vertex array. The get_mesh function\n        is called when creating an instance of DeviceGeometry, which is then passed\n        into an instance of DeviceSimulation.\n\n        :param self: Refer to the object itself\n        :return: A mesh object with the device array shape and the mesh names\n\n        \"\"\"\n        return Mesh(\n            create_device_mesh(\n                self.array_devices_shape\n            ),\n            self.get_mesh_names()\n        )\n\n    def __repr__(self):\n        return self.__str__()\n\n    @staticmethod\n    def get_mesh_names():\n        return \"dp\", \"fsdp\", \"tp\", \"mp\"\n\n    def get_optimizer_and_scheduler(self, steps=None):\n        \"\"\"\n        The get_optimizer_and_scheduler function is a helper function that returns the optimizer and scheduler\n            based on the parameters passed to it.\n\n        :param self: Represent the instance of the class\n        :param steps: Calculate the number of steps to train\n        :return: A tuple of two objects:\n\n        \"\"\"\n        steps = self.max_steps or steps\n        assert steps is not None, 'if you haven\\'t pass max steps to init you should pass init in func'\n\n        if self.optimizer == 'adafactor':\n            if self.scheduler == 'linear':\n                tx, sc = fjformer.optimizers.get_adafactor_with_linear_scheduler(\n                    learning_rate_start=self.learning_rate,\n                    learning_rate_end=self.learning_rate_end,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    steps=steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'cosine':\n                tx, sc = fjformer.optimizers.get_adafactor_with_cosine_scheduler(\n                    learning_rate=self.learning_rate,\n                    steps=steps,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'none':\n                tx, sc = fjformer.optimizers.get_adafactor_with_linear_scheduler(\n                    learning_rate_start=self.learning_rate,\n                    learning_rate_end=self.learning_rate,\n                    steps=steps,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'warm_up_cosine':\n                tx, sc = fjformer.optimizers.get_adafactor_with_warm_up_cosine_scheduler(\n                    learning_rate=self.learning_rate,\n                    steps=steps,\n                    weight_decay=self.weight_decay,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'warm_up_linear':\n                tx, sc = fjformer.optimizers.get_adafactor_with_warmup_linear_scheduler(\n                    learning_rate_start=self.learning_rate,\n                    steps=steps,\n                    learning_rate_end=self.learning_rate_end,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    warmup_steps=self.warmup_steps,\n                    **self.extra_optimizer_kwargs\n\n                )\n\n            else:\n                raise ValueError('seems like you have choose wrong type or unavailable scheduler')\n        elif self.optimizer == 'lion':\n            if self.scheduler == 'linear':\n                tx, sc = fjformer.optimizers.get_lion_with_linear_scheduler(\n                    learning_rate_start=self.learning_rate,\n                    learning_rate_end=self.learning_rate_end,\n                    steps=steps,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'cosine':\n                tx, sc = fjformer.optimizers.get_lion_with_cosine_scheduler(\n                    learning_rate=self.learning_rate,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    steps=steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'none':\n                tx, sc = fjformer.optimizers.get_lion_with_linear_scheduler(\n                    learning_rate_start=self.learning_rate,\n                    learning_rate_end=self.learning_rate,\n                    steps=steps,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'warm_up_cosine':\n                tx, sc = fjformer.optimizers.get_lion_with_warm_up_cosine_scheduler(\n                    learning_rate=self.learning_rate,\n                    steps=steps,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    **self.extra_optimizer_kwargs\n                )\n\n            elif self.scheduler == 'warm_up_linear':\n                tx, sc = fjformer.optimizers.get_lion_with_with_warmup_linear_scheduler(\n                    learning_rate_start=self.learning_rate,\n                    steps=steps,\n                    learning_rate_end=self.learning_rate_end,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    warmup_steps=self.warmup_steps,\n                    **self.extra_optimizer_kwargs\n                )\n            else:\n                raise ValueError('seems like you have choose wrong type or unavailable scheduler')\n        elif self.optimizer == 'adamw':\n            if self.scheduler == 'linear':\n                tx, sc = fjformer.optimizers.get_adamw_with_linear_scheduler(\n                    learning_rate_start=self.learning_rate,\n                    learning_rate_end=self.learning_rate_end,\n                    steps=steps,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'cosine':\n                tx, sc = fjformer.optimizers.get_adamw_with_cosine_scheduler(\n                    learning_rate=self.learning_rate,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    steps=steps,\n                    weight_decay=self.weight_decay,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'none':\n                tx, sc = fjformer.optimizers.get_adamw_with_linear_scheduler(\n                    learning_rate_start=self.learning_rate,\n                    learning_rate_end=self.learning_rate,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    steps=steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'warm_up_cosine':\n                tx, sc = fjformer.optimizers.get_adamw_with_warm_up_cosine_scheduler(\n                    learning_rate=self.learning_rate,\n                    steps=steps,\n                    weight_decay=self.weight_decay,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    **self.extra_optimizer_kwargs\n                )\n            elif self.scheduler == 'warm_up_linear':\n                tx, sc = fjformer.optimizers.get_adamw_with_warmup_linear_scheduler(\n                    learning_rate_start=self.learning_rate,\n                    steps=steps,\n                    weight_decay=self.weight_decay,\n                    learning_rate_end=self.learning_rate_end,\n                    gradient_accumulation_steps=self.gradient_accumulation_steps,\n                    warmup_steps=self.warmup_steps,\n                    **self.extra_optimizer_kwargs\n                )\n            else:\n                raise ValueError('seems like you have choose wrong type or unavailable scheduler')\n        else:\n            raise ValueError('seems like you have choose wrong type or unavailable optimizer')\n        return tx, sc\n\n    def get_streaming_checkpointer(self):\n        \"\"\"\n        The get_streaming_checkpointer function is used to save the model's weights.\n        The streaming checkpointer saves the model's weights in a file called &amp;quot;checkpoint&amp;quot; and then\n        saves a copy of that file with an incrementing number appended to it (e.g., checkpoint_001,\n        checkpoint_002, etc.). This allows you to keep multiple versions of your trained models.\n\n        :param self: Represent the instance of the class\n        :return: A streamingcheckpointer object\n\n        \"\"\"\n        return StreamingCheckpointer(StreamingCheckpointer.get_default_config(),\n                                     os.path.join(self.save_dir, self.model_name))\n\n    def get_board(self):\n        \"\"\"\n        The get_board function is a helper function that returns a TensorBoard object.\n        The TensorBoard object is used to log the training and validation loss, as well as\n        the accuracy of the model during training. The get_board function takes no arguments,\n        and returns an instance of torch.utils.tensorboard SummaryWriter class.\n\n        :param self: Represent the instance of the class\n        :return: A summary-writer object\n\n        \"\"\"\n        return torch.utils.tensorboard.SummaryWriter(\n            log_dir=str(self.get_path()),\n            comment=f'{self.model_name}',\n            filename_suffix='easydel'\n        )\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-config/#lib.python.EasyDel.trainer.config.TrainArguments.__init__","title":"<code>__init__(model_name, num_train_epochs, model_id=None, model_class=None, total_batch_size=32, max_steps=None, optimizer='lion', scheduler='linear', learning_rate=5e-05, learning_rate_end=5e-06, gradient_accumulation_steps=1, weight_decay=0.01, gradient_checkpointing='nothing_saveable', max_length=4096, sharding_array=(1, -1, 1, 1), is_fine_tuning=True, do_train=True, do_eval=False, do_test=False, backend=None, extra_optimizer_kwargs=None, save_steps=None, save_dir='easydel_ckpt', use_pjit_attention_force=False, dtype=jnp.bfloat16, param_dtype=jnp.bfloat16, fully_fsdp=True, use_wandb=True, custom_rule=None, extra_configs=None, ids_to_pop_from_dataset=None, remove_ckpt_after_load=False, configs_to_init_model_class=None, do_last_save=True, model_parameters=None, do_shard_fns=True, track_memory=True, loss_remat='', loss_chunk=1024, is_left_padded=False, warmup_steps=500, init_input_shape=(1, 1), step_partition_spec=jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp'), **kwargs)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the instance of the class, and makes sure that it has all of the attributes necessary for proper functioning. It also allows you to set default values for those attributes if they are not provided as arguments by the person creating an instance.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the class instance itself</p> required <code>model_name</code> <code>str</code> <p>str: Specify the model name</p> required <code>num_train_epochs</code> <code>int</code> <p>int: Set the number of epochs for training</p> required <code>model_id</code> <code>str</code> <p>str: Load a model from the save_dir</p> <code>None</code> <code>model_class</code> <p>Initialize the model, and the configs_to_init_model_class parameter is used to</p> <code>None</code> <code>total_batch_size</code> <code>int</code> <p>int: Set the batch size of the model</p> <code>32</code> <code>max_steps</code> <code>Union[int, None]</code> <p>Union[int,None]: Determine the maximum number of steps to train for</p> <code>None</code> <code>optimizer</code> <code>str</code> <p>str: Specify which optimizer to use</p> <code>'lion'</code> <code>scheduler</code> <code>str</code> <p>str: Set the learning rate scheduler</p> <code>'linear'</code> <code>learning_rate</code> <code>Union[int, float]</code> <p>Union[int,float]: Set the learning rate , Set the dtype of the model parameters</p> <code>5e-05</code> <code>learning_rate_end</code> <code>Union[None, float]</code> <p>Union[None,float]: Set the end learning rate, Set the dtype of the model parameters</p> <code>5e-06</code> <code>gradient_accumulation_steps</code> <code>int</code> <p>int: Accumulate gradients over multiple batches</p> <code>1</code> <code>weight_decay</code> <code>float</code> <p>float: Control the weight decay</p> <code>0.01</code> <code>gradient_checkpointing</code> <code>str</code> <p>str: Control the gradient checkpointing method</p> <code>'nothing_saveable'</code> <code>max_length</code> <code>Union[int, None]</code> <p>Union[int, None]: Set the maximum length of a sequence, Pass the model_class to the trainer class</p> <code>4096</code> <code>sharding_array</code> <code>Union[tuple, int]</code> <p>Union[tuple: Shard the model across multiple devices</p> <code>(1, -1, 1, 1)</code> <code>is_fine_tuning</code> <code>bool</code> <p>bool: Determine whether the model is being trained from scratch or not</p> <code>True</code> <code>do_train</code> <code>bool</code> <p>bool: Determine whether the model should be trained or not</p> <code>True</code> <code>do_eval</code> <code>bool</code> <p>bool: Determine whether to run the eval loop or not</p> <code>False</code> <code>do_test</code> <code>Union[bool, None]</code> <p>Union[bool,None]: Determine whether to run the test or not, Pass the model_class to the trainer</p> <code>False</code> <code>backend</code> <code>Union[str, None]</code> <p>Union[str, None]:: Specify the device that will be used for training, Define the default value of a parameter</p> <code>None</code> <code>extra_optimizer_kwargs</code> <code>dict</code> <p>dict: Pass extra arguments to the optimizer</p> <code>None</code> <code>save_steps</code> <code>Union[int, None]</code> <p>Union[int,None]: Save the model after a number of steps,  Set the default value of do_test to none</p> <code>None</code> <code>save_dir</code> <code>str</code> <p>str: Specify the directory where the model checkpoints will be saved</p> <code>'easydel_ckpt'</code> <code>use_pjit_attention_force</code> <code>bool</code> <p>bool: Determine whether to use the jax</p> <code>False</code> <code>dtype</code> <p>Set the data type of the model parameters and inputs</p> <code>bfloat16</code> <code>param_dtype</code> <p>Specify the data type of the model parameters</p> <code>bfloat16</code> <code>fully_fsdp</code> <p>Control the use of fully fused sdp</p> <code>True</code> <code>use_wandb</code> <code>bool</code> <p>bool: Determine whether to use wandb or not</p> <code>True</code> <code>custom_rule</code> <p>Pass a custom rule to the optimizer,</p> <code>None</code> <code>extra_configs</code> <p>Pass extra configurations to the model class</p> <code>None</code> <code>ids_to_pop_from_dataset</code> <code>list</code> <p>list: Pop some keys from the dataset,</p> <code>None</code> <code>remove_ckpt_after_load</code> <code>bool</code> <p>bool: Remove the checkpoint after loading it</p> <code>False</code> <code>configs_to_init_model_class</code> <p>Pass the configs to the model class</p> <code>None</code> <code>do_last_save</code> <code>bool</code> <p>bool: Save the model at the end of training</p> <code>True</code> <code>model_parameters</code> <p>Pass the model parameters to the trainer</p> <code>None</code> <code>do_shard_fns</code> <code>bool</code> <p>bool: Shard the model across multiple devices</p> <code>True</code> <code>track_memory</code> <code>bool</code> <p>bool: Track the memory usage of the model</p> <code>True</code> <code>loss_remat</code> <code>str</code> <p>str: Specify how to rematerialize the loss function</p> <code>''</code> <code>loss_chunk</code> <code>int</code> <p>int: Chunk the loss function</p> <code>1024</code> <code>is_left_padded</code> <code>bool</code> <p>bool: Indicate whether the input is left padded or not</p> <code>False</code> <code>warmup_steps</code> <code>int</code> <p>int: Warm up the learning rate</p> <code>500</code> <code>init_input_shape</code> <code>Tuple[int, int]</code> <p>typing.Tuple[int]: Initialize the input shape of the model</p> <code>(1, 1)</code> <code>step_partition_spec</code> <code>PartitionSpec</code> <p>jax.sharding.PartitionSpec: PartitionSpec Custom to be used in training and eval or test loop</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp')</code> <code>**kwargs</code> <p>Pass a variable number of keyword arguments to a function</p> <code>{}</code> <p>Returns:</p> Type Description <p>Nothing</p> Source code in <code>lib/python/EasyDel/trainer/config.py</code> <pre><code>def __init__(\n        self,\n        model_name: str,\n        num_train_epochs: int,\n        model_id: str = None,\n        model_class=None,\n        total_batch_size: int = 32,\n        max_steps: Union[int, None] = None,\n        optimizer: str = 'lion',\n        scheduler: str = 'linear',\n        learning_rate: Union[int, float] = 5e-5,\n        learning_rate_end: Union[None, float] = 5e-6,\n        gradient_accumulation_steps: int = 1,\n        weight_decay: float = 0.01,\n        gradient_checkpointing: str = 'nothing_saveable',\n        max_length: Union[int, None] = 4096,\n        sharding_array: Union[tuple, int] = (1, -1, 1, 1),\n        is_fine_tuning: bool = True,\n        do_train: bool = True,\n        do_eval: bool = False,\n        do_test: Union[bool, None] = False,\n        backend: Union[str, None] = None,\n        extra_optimizer_kwargs: dict = None,\n        save_steps: Union[int, None] = None,\n        save_dir: str = 'easydel_ckpt',\n        use_pjit_attention_force: bool = False,\n        dtype=jnp.bfloat16,\n        param_dtype=jnp.bfloat16,\n        fully_fsdp=True,\n        use_wandb: bool = True,\n        custom_rule=None,\n        extra_configs=None,\n        ids_to_pop_from_dataset: list = None,\n        remove_ckpt_after_load: bool = False,\n        configs_to_init_model_class=None,\n        do_last_save: bool = True,\n        model_parameters=None,\n        do_shard_fns: bool = True,\n        track_memory: bool = True,\n        loss_remat: str = '',\n        loss_chunk: int = 1024,\n        is_left_padded: bool = False,\n        warmup_steps: int = 500,\n        init_input_shape: typing.Tuple[int, int] = (1, 1),\n        step_partition_spec: jax.sharding.PartitionSpec = jax.sharding.PartitionSpec(('dp', 'fsdp'), 'mp'),\n        **kwargs\n):\n    \"\"\"\n    The __init__ function is called when the class is instantiated.\n    It sets up the instance of the class, and makes sure that it has all of\n    the attributes necessary for proper functioning. It also allows you to set\n    default values for those attributes if they are not provided as arguments by\n    the person creating an instance.\n\n    :param self: Refer to the class instance itself\n    :param model_name: str: Specify the model name\n    :param num_train_epochs: int: Set the number of epochs for training\n    :param model_id: str: Load a model from the save_dir\n    :param model_class: Initialize the model, and the configs_to_init_model_class parameter is used to\n    :param total_batch_size: int: Set the batch size of the model\n    :param max_steps: Union[int,None]: Determine the maximum number of steps to train for\n    :param optimizer: str: Specify which optimizer to use\n    :param scheduler: str: Set the learning rate scheduler\n    :param learning_rate: Union[int,float]: Set the learning rate , Set the dtype of the model parameters\n    :param learning_rate_end: Union[None,float]: Set the end learning rate, Set the dtype of the model parameters\n    :param gradient_accumulation_steps: int: Accumulate gradients over multiple batches\n    :param weight_decay: float: Control the weight decay\n    :param gradient_checkpointing: str: Control the gradient checkpointing method\n    :param max_length: Union[int, None]: Set the maximum length of a sequence, Pass the model_class to the trainer class\n    :param sharding_array: Union[tuple: Shard the model across multiple devices\n    :param is_fine_tuning: bool: Determine whether the model is being trained from scratch or not\n    :param do_train: bool: Determine whether the model should be trained or not\n    :param do_eval: bool: Determine whether to run the eval loop or not\n    :param do_test: Union[bool,None]: Determine whether to run the test or not, Pass the model_class to the trainer\n    :param backend: Union[str, None]:: Specify the device that will be used for training, Define the default value of a parameter\n    :param extra_optimizer_kwargs: dict: Pass extra arguments to the optimizer\n    :param save_steps: Union[int,None]: Save the model after a number of steps,  Set the default value of do_test to none\n    :param save_dir: str: Specify the directory where the model checkpoints will be saved\n    :param use_pjit_attention_force: bool: Determine whether to use the jax\n    :param dtype: Set the data type of the model parameters and inputs\n    :param param_dtype: Specify the data type of the model parameters\n    :param fully_fsdp: Control the use of fully fused sdp\n    :param use_wandb: bool: Determine whether to use wandb or not\n    :param custom_rule: Pass a custom rule to the optimizer,\n    :param extra_configs: Pass extra configurations to the model class\n    :param ids_to_pop_from_dataset: list: Pop some keys from the dataset,\n    :param remove_ckpt_after_load: bool: Remove the checkpoint after loading it\n    :param configs_to_init_model_class: Pass the configs to the model class\n    :param do_last_save: bool: Save the model at the end of training\n    :param model_parameters: Pass the model parameters to the trainer\n    :param do_shard_fns: bool: Shard the model across multiple devices\n    :param track_memory: bool: Track the memory usage of the model\n    :param loss_remat: str: Specify how to rematerialize the loss function\n    :param loss_chunk: int: Chunk the loss function\n    :param is_left_padded: bool: Indicate whether the input is left padded or not\n    :param warmup_steps: int: Warm up the learning rate\n    :param init_input_shape: typing.Tuple[int]: Initialize the input shape of the model\n    :param step_partition_spec: jax.sharding.PartitionSpec: PartitionSpec Custom to be used in training and eval or test loop\n    :param **kwargs: Pass a variable number of keyword arguments to a function\n    :return: Nothing\n\n    \"\"\"\n    super().__init__()\n    if ids_to_pop_from_dataset is None:\n        ids_to_pop_from_dataset = []\n    if extra_optimizer_kwargs is None:\n        extra_optimizer_kwargs = {}\n    assert model_class is not None or model_id is not None, 'you cant pass model_class and model_id both None ' \\\n                                                            'you should at least pass one of them to build ' \\\n                                                            'model with'\n    assert backend in AVAILABLE_BACKENDS, f'{backend} is not recognized, ' \\\n                                          f'available backends are {AVAILABLE_BACKENDS}'\n    assert gradient_checkpointing in AVAILABLE_GRADIENT_CHECK_POINTING, f'{gradient_checkpointing} is not ' \\\n                                                                        f'recognized, available gradient ' \\\n                                                                        f'checkpointing methods are ' \\\n                                                                        f'{AVAILABLE_GRADIENT_CHECK_POINTING}'\n    assert scheduler in AVAILABLE_SCHEDULERS, f'{scheduler} is not recognized, ' \\\n                                              f'available schedulers are {AVAILABLE_SCHEDULERS}'\n    assert optimizer in AVAILABLE_OPTIMIZERS, f'{optimizer} is not recognized, ' \\\n                                              f'available optimizers are {AVAILABLE_OPTIMIZERS}'\n    self.available_backends = len(jax.devices(backend))\n    total_batch_size *= gradient_accumulation_steps\n    array_devices = jnp.ones((self.available_backends, 1)).reshape(sharding_array)\n    self.array_devices_shape = array_devices.shape\n\n    self.model_id = model_id\n    self.num_train_epochs = num_train_epochs\n    self.total_batch_size = total_batch_size\n    self.max_steps = max_steps\n    self.optimizer = optimizer\n    self.scheduler = scheduler\n    self.extra_optimizer_kwargs = extra_optimizer_kwargs\n    self.learning_rate = learning_rate\n    self.learning_rate_end = learning_rate_end\n    self.weight_decay = weight_decay\n    self.model_name = model_name\n    self.gradient_checkpointing = gradient_checkpointing\n    self.max_length = max_length\n    self.sharding_array = sharding_array\n    self.is_fine_tuning = is_fine_tuning\n    self.do_train = do_train\n    self.do_eval = do_eval\n    self.do_test = do_test\n    self.save_steps = save_steps\n    self.save_dir = save_dir\n    self.use_pjit_attention_force = use_pjit_attention_force\n    self.dtype = dtype\n    self.warmup_steps = warmup_steps\n    self.param_dtype = param_dtype\n    self.fully_fsdp = fully_fsdp\n    self.use_wandb = use_wandb\n    self.custom_rule = custom_rule\n    self.extra_configs = extra_configs\n    self.ids_to_pop_from_dataset = ids_to_pop_from_dataset\n    self.remove_ckpt_after_load = remove_ckpt_after_load\n    self.model_class = model_class\n    self.configs_to_init_model_class = configs_to_init_model_class\n    self.do_last_save = do_last_save\n    self.model_parameters = model_parameters\n    self.do_shard_fns = do_shard_fns\n    self.gradient_accumulation_steps = gradient_accumulation_steps\n    self.track_memory = track_memory\n\n    self.loss_chunk = loss_chunk\n    self.loss_remat = loss_remat\n    self.init_input_shape = init_input_shape\n    self.is_left_padded = is_left_padded\n    self.step_partition_spec = step_partition_spec\n    torch.set_default_device('cpu')\n    self.__dict__.update(**kwargs)\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-config/#lib.python.EasyDel.trainer.config.TrainArguments.ckpt_path_exists","title":"<code>ckpt_path_exists()</code>","text":"<p>The ckpt_path_exists function checks to see if the path exists. If it does not, then it creates a new directory.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A path</p> Source code in <code>lib/python/EasyDel/trainer/config.py</code> <pre><code>def ckpt_path_exists(self):\n    \"\"\"\n    The ckpt_path_exists function checks to see if the path exists. If it does not, then it creates a new directory.\n\n    :param self: Represent the instance of the class\n    :return: A path\n\n    \"\"\"\n    path = self.get_path()\n    if not path.exists():\n        path.mkdir(parents=True)\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-config/#lib.python.EasyDel.trainer.config.TrainArguments.get_board","title":"<code>get_board()</code>","text":"<p>The get_board function is a helper function that returns a TensorBoard object. The TensorBoard object is used to log the training and validation loss, as well as the accuracy of the model during training. The get_board function takes no arguments, and returns an instance of torch.utils.tensorboard SummaryWriter class.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A summary-writer object</p> Source code in <code>lib/python/EasyDel/trainer/config.py</code> <pre><code>def get_board(self):\n    \"\"\"\n    The get_board function is a helper function that returns a TensorBoard object.\n    The TensorBoard object is used to log the training and validation loss, as well as\n    the accuracy of the model during training. The get_board function takes no arguments,\n    and returns an instance of torch.utils.tensorboard SummaryWriter class.\n\n    :param self: Represent the instance of the class\n    :return: A summary-writer object\n\n    \"\"\"\n    return torch.utils.tensorboard.SummaryWriter(\n        log_dir=str(self.get_path()),\n        comment=f'{self.model_name}',\n        filename_suffix='easydel'\n    )\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-config/#lib.python.EasyDel.trainer.config.TrainArguments.get_mesh","title":"<code>get_mesh()</code>","text":"<p>The get_mesh function is used to create a mesh object that can be used to define the geometry of the device. The mesh object contains two arrays: a list of vertices and a list of faces. Each face is defined by three indices, which correspond to three vertices in the vertex array. The get_mesh function is called when creating an instance of DeviceGeometry, which is then passed into an instance of DeviceSimulation.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the object itself</p> required <p>Returns:</p> Type Description <p>A mesh object with the device array shape and the mesh names</p> Source code in <code>lib/python/EasyDel/trainer/config.py</code> <pre><code>def get_mesh(self):\n    \"\"\"\n    The get_mesh function is used to create a mesh object that can be used\n    to define the geometry of the device. The mesh object contains two arrays:\n    a list of vertices and a list of faces. Each face is defined by three indices,\n    which correspond to three vertices in the vertex array. The get_mesh function\n    is called when creating an instance of DeviceGeometry, which is then passed\n    into an instance of DeviceSimulation.\n\n    :param self: Refer to the object itself\n    :return: A mesh object with the device array shape and the mesh names\n\n    \"\"\"\n    return Mesh(\n        create_device_mesh(\n            self.array_devices_shape\n        ),\n        self.get_mesh_names()\n    )\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-config/#lib.python.EasyDel.trainer.config.TrainArguments.get_meter_dict","title":"<code>get_meter_dict()</code>","text":"<p>The get_meter_dict function is used to return a dictionary of the hyperparameters. The function iterates through all the attributes in the class and returns a dictionary with the key as \"hyperparameters/{k}\" and value as v for each attribute k,v in self.dict if it is an instance of int, float, str, bool or torch.Tensor.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A dictionary of hyperparameters</p> Source code in <code>lib/python/EasyDel/trainer/config.py</code> <pre><code>def get_meter_dict(self):\n    \"\"\"\n    The get_meter_dict function is used to return a dictionary of the hyperparameters.\n    The function iterates through all the attributes in the class and returns a dictionary with\n    the key as &amp;quot;hyperparameters/{k}&amp;quot; and value as v for each attribute k,v in self.__dict__ if it is an instance of int, float, str, bool or torch.Tensor.\n\n    :param self: Represent the instance of the class\n    :return: A dictionary of hyperparameters\n\n    \"\"\"\n    return {f\"hyperparameters/{k}\": v for k, v in self.__dict__.items() if\n            isinstance(v, (int, float, str, bool, torch.Tensor))}\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-config/#lib.python.EasyDel.trainer.config.TrainArguments.get_optimizer_and_scheduler","title":"<code>get_optimizer_and_scheduler(steps=None)</code>","text":"<p>The get_optimizer_and_scheduler function is a helper function that returns the optimizer and scheduler     based on the parameters passed to it.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>steps</code> <p>Calculate the number of steps to train</p> <code>None</code> <p>Returns:</p> Type Description <p>A tuple of two objects:</p> Source code in <code>lib/python/EasyDel/trainer/config.py</code> <pre><code>def get_optimizer_and_scheduler(self, steps=None):\n    \"\"\"\n    The get_optimizer_and_scheduler function is a helper function that returns the optimizer and scheduler\n        based on the parameters passed to it.\n\n    :param self: Represent the instance of the class\n    :param steps: Calculate the number of steps to train\n    :return: A tuple of two objects:\n\n    \"\"\"\n    steps = self.max_steps or steps\n    assert steps is not None, 'if you haven\\'t pass max steps to init you should pass init in func'\n\n    if self.optimizer == 'adafactor':\n        if self.scheduler == 'linear':\n            tx, sc = fjformer.optimizers.get_adafactor_with_linear_scheduler(\n                learning_rate_start=self.learning_rate,\n                learning_rate_end=self.learning_rate_end,\n                gradient_accumulation_steps=self.gradient_accumulation_steps,\n                steps=steps,\n                **self.extra_optimizer_kwargs\n            )\n        elif self.scheduler == 'cosine':\n            tx, sc = fjformer.optimizers.get_adafactor_with_cosine_scheduler(\n                learning_rate=self.learning_rate,\n                steps=steps,\n                gradient_accumulation_steps=self.gradient_accumulation_steps,\n                **self.extra_optimizer_kwargs\n            )\n        elif self.scheduler == 'none':\n            tx, sc = fjformer.optimizers.get_adafactor_with_linear_scheduler(\n                learning_rate_start=self.learning_rate,\n                learning_rate_end=self.learning_rate,\n                steps=steps,\n                gradient_accumulation_steps=self.gradient_accumulation_steps,\n                **self.extra_optimizer_kwargs\n            )\n        elif self.scheduler == 'warm_up_cosine':\n            tx, sc = fjformer.optimizers.get_adafactor_with_warm_up_cosine_scheduler(\n                learning_rate=self.learning_rate,\n                steps=steps,\n                weight_decay=self.weight_decay,\n                gradient_accumulation_steps=self.gradient_accumulation_steps,\n                **self.extra_optimizer_kwargs\n            )\n        elif self.scheduler == 'warm_up_linear':\n            tx, sc = fjformer.optimizers.get_adafactor_with_warmup_linear_scheduler(\n                learning_rate_start=self.learning_rate,\n                steps=steps,\n                learning_rate_end=self.learning_rate_end,\n                gradient_accumulation_steps=self.gradient_accumulation_steps,\n                warmup_steps=self.warmup_steps,\n                **self.extra_optimizer_kwargs\n\n            )\n\n        else:\n            raise ValueError('seems like you have choose wrong type or unavailable scheduler')\n    elif self.optimizer == 'lion':\n        if self.scheduler == 'linear':\n            tx, sc = fjformer.optimizers.get_lion_with_linear_scheduler(\n                learning_rate_start=self.learning_rate,\n                learning_rate_end=self.learning_rate_end,\n                steps=steps,\n                gradient_accumulation_steps=self.gradient_accumulation_steps,\n                **self.extra_optimizer_kwargs\n            )\n        elif self.scheduler == 'cosine':\n            tx, sc = fjformer.optimizers.get_lion_with_cosine_scheduler(\n                learning_rate=self.learning_rate,\n                gradient_accumulation_steps=self.gradient_accumulation_steps,\n                steps=steps,\n                **self.extra_optimizer_kwargs\n            )\n        elif self.scheduler == 'none':\n            tx, sc = fjformer.optimizers.get_lion_with_linear_scheduler(\n                learning_rate_start=self.learning_rate,\n                learning_rate_end=self.learning_rate,\n                steps=steps,\n                gradient_accumulation_steps=self.gradient_accumulation_steps,\n                **self.extra_optimizer_kwargs\n            )\n        elif self.scheduler == 'warm_up_cosine':\n            tx, sc = fjformer.optimizers.get_lion_with_warm_up_cosine_scheduler(\n                learning_rate=self.learning_rate,\n                steps=steps,\n                gradient_accumulation_steps=self.gradient_accumulation_steps,\n                **self.extra_optimizer_kwargs\n            )\n\n        elif self.scheduler == 'warm_up_linear':\n            tx, sc = fjformer.optimizers.get_lion_with_with_warmup_linear_scheduler(\n                learning_rate_start=self.learning_rate,\n                steps=steps,\n                learning_rate_end=self.learning_rate_end,\n                gradient_accumulation_steps=self.gradient_accumulation_steps,\n                warmup_steps=self.warmup_steps,\n                **self.extra_optimizer_kwargs\n            )\n        else:\n            raise ValueError('seems like you have choose wrong type or unavailable scheduler')\n    elif self.optimizer == 'adamw':\n        if self.scheduler == 'linear':\n            tx, sc = fjformer.optimizers.get_adamw_with_linear_scheduler(\n                learning_rate_start=self.learning_rate,\n                learning_rate_end=self.learning_rate_end,\n                steps=steps,\n                gradient_accumulation_steps=self.gradient_accumulation_steps,\n                **self.extra_optimizer_kwargs\n            )\n        elif self.scheduler == 'cosine':\n            tx, sc = fjformer.optimizers.get_adamw_with_cosine_scheduler(\n                learning_rate=self.learning_rate,\n                gradient_accumulation_steps=self.gradient_accumulation_steps,\n                steps=steps,\n                weight_decay=self.weight_decay,\n                **self.extra_optimizer_kwargs\n            )\n        elif self.scheduler == 'none':\n            tx, sc = fjformer.optimizers.get_adamw_with_linear_scheduler(\n                learning_rate_start=self.learning_rate,\n                learning_rate_end=self.learning_rate,\n                gradient_accumulation_steps=self.gradient_accumulation_steps,\n                steps=steps,\n                **self.extra_optimizer_kwargs\n            )\n        elif self.scheduler == 'warm_up_cosine':\n            tx, sc = fjformer.optimizers.get_adamw_with_warm_up_cosine_scheduler(\n                learning_rate=self.learning_rate,\n                steps=steps,\n                weight_decay=self.weight_decay,\n                gradient_accumulation_steps=self.gradient_accumulation_steps,\n                **self.extra_optimizer_kwargs\n            )\n        elif self.scheduler == 'warm_up_linear':\n            tx, sc = fjformer.optimizers.get_adamw_with_warmup_linear_scheduler(\n                learning_rate_start=self.learning_rate,\n                steps=steps,\n                weight_decay=self.weight_decay,\n                learning_rate_end=self.learning_rate_end,\n                gradient_accumulation_steps=self.gradient_accumulation_steps,\n                warmup_steps=self.warmup_steps,\n                **self.extra_optimizer_kwargs\n            )\n        else:\n            raise ValueError('seems like you have choose wrong type or unavailable scheduler')\n    else:\n        raise ValueError('seems like you have choose wrong type or unavailable optimizer')\n    return tx, sc\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-config/#lib.python.EasyDel.trainer.config.TrainArguments.get_path","title":"<code>get_path()</code>","text":"<p>The get_path function returns a pathlib.Path object, which is a class that represents file paths and provides methods for interacting with the files at those paths. The get_path function takes no arguments and returns an instance of the Path class initialized with two arguments: self.save_dir (a string) and self.model_name (also a string). The save directory is the directory where we'll store our model checkpoints, while the model name will be used to create unique filenames for each checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A pathlib</p> Source code in <code>lib/python/EasyDel/trainer/config.py</code> <pre><code>def get_path(self):\n    \"\"\"\n    The get_path function returns a pathlib.Path object, which is a class that\n    represents file paths and provides methods for interacting with the files at\n    those paths. The get_path function takes no arguments and returns an instance of\n    the Path class initialized with two arguments: self.save_dir (a string) and\n    self.model_name (also a string). The save directory is the directory where we'll\n    store our model checkpoints, while the model name will be used to create unique\n    filenames for each checkpoint.\n\n    :param self: Represent the instance of the class\n    :return: A pathlib\n\n    \"\"\"\n    return pathlib.Path(\n        self.save_dir, self.model_name\n    )\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-config/#lib.python.EasyDel.trainer.config.TrainArguments.get_streaming_checkpointer","title":"<code>get_streaming_checkpointer()</code>","text":"<p>The get_streaming_checkpointer function is used to save the model's weights. The streaming checkpointer saves the model's weights in a file called \"checkpoint\" and then saves a copy of that file with an incrementing number appended to it (e.g., checkpoint_001, checkpoint_002, etc.). This allows you to keep multiple versions of your trained models.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A streamingcheckpointer object</p> Source code in <code>lib/python/EasyDel/trainer/config.py</code> <pre><code>def get_streaming_checkpointer(self):\n    \"\"\"\n    The get_streaming_checkpointer function is used to save the model's weights.\n    The streaming checkpointer saves the model's weights in a file called &amp;quot;checkpoint&amp;quot; and then\n    saves a copy of that file with an incrementing number appended to it (e.g., checkpoint_001,\n    checkpoint_002, etc.). This allows you to keep multiple versions of your trained models.\n\n    :param self: Represent the instance of the class\n    :return: A streamingcheckpointer object\n\n    \"\"\"\n    return StreamingCheckpointer(StreamingCheckpointer.get_default_config(),\n                                 os.path.join(self.save_dir, self.model_name))\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-config/#lib.python.EasyDel.trainer.config.TrainArguments.get_wandb_init","title":"<code>get_wandb_init()</code>","text":"<p>The get_wandb_init function is a helper function that returns the wandb.init() call with the project name, config object, and tags set to appropriate values for this model.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Pass the class instance to the function</p> required <p>Returns:</p> Type Description <p>A wandb</p> Source code in <code>lib/python/EasyDel/trainer/config.py</code> <pre><code>def get_wandb_init(self):\n    \"\"\"\n    The get_wandb_init function is a helper function that returns the wandb.init() call with\n    the project name, config object, and tags set to appropriate values for this model.\n\n    :param self: Pass the class instance to the function\n    :return: A wandb\n\n    \"\"\"\n    return wandb.init(\n        project=f'easydel-{self.model_name}',\n        config=self(),\n        tags=[\n            'Easy Del',\n            'OST-OpenSourceTransformers',\n            'Jax/Flax'\n        ]\n    )\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-fsdp_train/","title":"trainer.fsdp_train","text":""},{"location":"lib-python-EasyDel-trainer-fsdp_train/#lib.python.EasyDel.trainer.fsdp_train.CausalLMTrainer","title":"<code>CausalLMTrainer</code>","text":"Source code in <code>lib/python/EasyDel/trainer/fsdp_train.py</code> <pre><code>class CausalLMTrainer:\n    def __init__(self,\n                 arguments: TrainArguments,\n                 dataset_train: Dataset,\n                 dataset_eval: Dataset = None,\n                 finetune: bool = True,\n                 ckpt_path: typing.Union[str, os.PathLike] = None,\n                 _do_init_fns: bool = True\n                 ):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It sets up all the variables that are needed for training, including:\n        - The timer to keep track of how long each epoch takes.\n        - The dataloaders for both training and evaluation (if provided).\n        - The model itself, which will be created from a checkpoint if one was provided.  Otherwise,\n         it will be created from scratch using the arguments passed in by the user.\n         Note that this function also handles creating a mesh if one was not already specified in arguments\n         or loaded from a checkpoint file (see below).   This means that you can pass in either\n\n        :param self: Represent the instance of the class\n        :param arguments: TrainArguments: Pass the arguments to the trainer\n        :param dataset_train: Dataset: Pass the training dataset to the trainer\n        :param dataset_eval: Dataset: Pass the validation dataset\n        :param finetune: bool: Load the model from a checkpoint\n        :param ckpt_path: typing.Union[str,os.PathLike] : Load the checkpoint path\n        :param _do_init_fns: bool: Initialize the functions\n        :return: Nothing, it just initializes the class\n\n        \"\"\"\n        self.timer = None\n        self.dataloader_train = None\n        self.dataloader_eval = None\n        self.model = None\n        self.wandb_runtime = None\n        self.max_steps_train = None\n        self.max_steps_eval = None\n        self.config = None\n        self.scheduler = None\n        self.tx = None\n        self.sharded_create_from_params_fn = None\n        self.sharded_train_step_fn = None\n        self.sharded_predict = None\n        self.mesh = None\n        self.ckpt_streamer = None\n        self.init_fn = None\n        self.train_state_shape = None\n        self.train_state_partition_spec = None\n        self.arguments = arguments\n        self.dataset_train = dataset_train\n        self.dataset_eval = dataset_eval\n        self.finetune = finetune\n        self.ckpt_path = ckpt_path\n        self.dtype = arguments.dtype\n        self.param_dtype = arguments.param_dtype\n        if finetune:\n            if ckpt_path is None:\n                prefix_print(\n                    'Warning',\n                    'In case of using finetune = True and Passing ckpt_path = None you should pass parameters'\n                    'in train function'\n                )\n        if _do_init_fns:\n            self.init_functions()\n        else:\n            prefix_print('Warning', 'you have set _do_init_fns to False so function will not me initialized you have '\n                                    f'to do in manually (simply with  trainer.init_functions() )')\n\n    def __str__(self):\n        string = f'CausalLMTrainer('\n        for k, v in self.__dict__.items():\n            if isinstance(v, typing.Callable):\n                def string_func(it_self):\n\n                    string_ = f'{it_self.__class__.__name__}(\\n'\n                    for k_, v_ in it_self.__dict__.items():\n                        string_ += f'\\t\\t{k_} : {v_}\\n'\n                    string_ += '\\t)'\n                    return string_\n\n                try:\n                    v.__str__ = string_func\n                    v = v.__str__(v)\n                except RuntimeError:\n                    pass\n\n            string += f'\\n\\t{k} : {v}'\n        string += ')'\n        return string\n\n    def __repr__(self):\n        return self.__str__()\n\n    @staticmethod\n    def finish():\n        \"\"\"\n        The finish function is called when the experiment ends.\n        It can be used to save data, upload files, or do any other cleanup tasks.\n\n        :return: A dictionary of the run's metadata\n\n        \"\"\"\n        wandb.finish()\n\n    def init_functions(self):\n        \"\"\"\n        The init_functions function is responsible for initializing the following:\n            - wandb_runtime (if you use_wandb is True)\n            - timer object (for logging time taken by various functions)\n            - dataloader objects for training and evaluation data, along with max steps per epoch.\n              The configure_dataloader function accomplishes this task.\n\n        :param self: Represent the instance of the class\n        :return: A tuple of functions\n\n        \"\"\"\n        self.wandb_runtime = self.arguments.get_wandb_init() if self.arguments.use_wandb else None\n        self.timer = Timers(\n            use_wandb=False,\n            tensorboard_writer=self.arguments.get_board()\n        )\n        self.timer(\n            'configure dataloaders'\n        ).start()\n        self.dataloader_train, self.max_steps_train, \\\n            self.dataloader_eval, self.max_steps_eval = self.configure_dataloader()\n        self.timer(\n            'configure dataloaders'\n        ).stop()\n\n        self.timer.log(['configure dataloaders'])\n\n        self.timer(\n            'configure Model ,Optimizer ,Scheduler and Config'\n        ).start()\n        self.model, self.tx, self.scheduler, self.config = self.configure_model()\n        self.timer(\n            'configure Model ,Optimizer ,Scheduler and Config'\n        ).stop()\n        self.timer.log(['configure Model ,Optimizer ,Scheduler and Config'])\n\n        self.timer(\n            'configure functions and sharding them'\n        ).start()\n        funcs = self.configure_functions()\n        self.sharded_create_from_params_fn = funcs[0]\n        self.sharded_train_step_fn = funcs[1]\n        self.sharded_predict = funcs[2]\n        self.mesh = funcs[3]\n        self.ckpt_streamer = funcs[4]\n        self.init_fn = funcs[5]\n        self.timer(\n            'configure functions and sharding them'\n        ).stop()\n        self.timer.log(['configure functions and sharding them'])\n\n    def configure_dataloader(self):\n\n        \"\"\"\n        The configure_dataloader function is used to configure the dataloader for training and evaluation.\n\n        :param self: Refer to the class instance itself\n        :return: A dataloader_train, max_steps_train, dataloader_eval and max steps eval\n\n        \"\"\"\n\n        def collate_fn(batch):\n            rs = {}\n            for key in batch[0].keys():\n                if self.arguments.is_left_padded:\n                    ssp = [jnp.array(f[key])[..., -self.arguments.max_length:] for f in batch]\n                else:\n                    ssp = [jnp.array(f[key])[..., :self.arguments.max_length] for f in batch]\n                rs[key] = jnp.stack(ssp).reshape(-1, ssp[0].shape[-1])\n            return rs\n\n        dataloader_train = DataLoader(self.dataset_train, collate_fn=collate_fn,\n                                      batch_size=self.arguments.total_batch_size, drop_last=True)\n        max_steps_train = self.arguments.num_train_epochs * len(\n            dataloader_train) if self.arguments.max_steps is None else self.arguments.max_steps\n        if self.dataset_eval is not None and self.arguments.do_eval:\n            dataloader_eval = DataLoader(self.dataset_eval, collate_fn=collate_fn,\n                                         batch_size=self.arguments.total_batch_size, drop_last=True)\n            max_steps_eval = len(\n                dataloader_eval) if self.arguments.max_steps is None else self.arguments.max_steps\n        else:\n            dataloader_eval, max_steps_eval = None, 0\n        return dataloader_train, max_steps_train, dataloader_eval, max_steps_eval\n\n    def configure_model(self):\n        \"\"\"\n        The configure_model function is responsible for creating the model, optimizer and scheduler.\n\n        :param self: Represent the instance of the class\n        :return: A model, optimizer, scheduler and config\n\n        \"\"\"\n        extra_configs = {} if self.arguments.extra_configs is None else self.arguments.extra_configs\n        if self.arguments.model_class is None:\n            config = AutoConfig.from_pretrained(self.arguments.model_id, trust_remote_code=True\n                                                , gradient_checkpointing=self.arguments.gradient_checkpointing,\n                                                use_pjit_attention_force=self.arguments.use_pjit_attention_force,\n                                                **extra_configs\n                                                )\n\n            assert hasattr(config, 'get_partition_rules')\n            model = FlaxAutoModelForCausalLM.from_config(config, trust_remote_code=True, dtype=self.arguments.dtype,\n                                                         param_dtype=self.arguments.param_dtype,\n                                                         _do_init=False)\n\n        else:\n            if not hasattr(self.arguments.configs_to_init_model_class['config'], 'get_partition_rules'):\n                assert self.arguments.custom_rule is not None, 'if you are using custom model to init you must' \\\n                                                               ' pass custom_rule for partition rules '\n\n            self.arguments.configs_to_init_model_class[\n                'config'\n            ].use_pjit_attention_force = self.arguments.use_pjit_attention_force\n\n            self.arguments.configs_to_init_model_class['config'].axis_dims = self.arguments.sharding_array\n\n            model = self.arguments.model_class(\n                **self.arguments.configs_to_init_model_class,\n                _do_init=False\n            )\n\n            config = self.arguments.configs_to_init_model_class['config']\n\n        tx, scheduler = self.arguments.get_optimizer_and_scheduler(self.max_steps_train)\n        return model, tx, scheduler, config\n\n    def configure_functions(self):\n        \"\"\"\n        The configure_functions function is responsible for configuring the functions that will be used in training.\n        It does this by first defining a function called init_fn, which initializes the model parameters and returns them as a\n        TrainState object. The TrainState object contains all of the information needed to train or evaluate on a batch of data,\n        including:\n\n        :param self: Access the class attributes\n        :return: A tuple of functions\n\n        \"\"\"\n\n        def init_fn():\n            params__ = self.model.init_weights(\n                jax.random.PRNGKey(0), self.arguments.init_input_shape\n            )\n            if self.arguments.dtype == jnp.bfloat16:\n                params__ = self.model.to_bf16(params__)\n            elif self.arguments.dtype == jnp.float16:\n                params__ = self.model.to_fp16(params__)\n            return train_state.TrainState.create(\n                tx=self.tx,\n                params=flax.core.freeze({'params': params__}),\n                apply_fn=self.model.__call__\n            )\n\n        def create_train_state_from_params(params_):\n            return train_state.TrainState.create(\n                tx=self.tx,\n                apply_fn=self.model.__call__,\n                params=params_\n            )\n\n        if self.arguments.loss_remat == 'OHA':\n            loss_fn = fjformer.func.loss_func.cross_entropy_with_logits\n        elif self.arguments.loss_remat != '':\n            loss_fn = fused_cross_entropy_loss_and_accuracy\n        else:\n            loss_fn = cross_entropy_loss_and_accuracy\n\n        def fsdp_train_step_(state, batch):\n            batch = with_sharding_constraint(batch, self.arguments.step_partition_spec)\n\n            def calculate_loss(params):\n                labels = batch.pop('labels')\n                logits = state.apply_fn(params=params, **batch,\n                                        return_dict=True).logits[:, :-1, :]\n\n                loss, accuracy = loss_fn(\n                    logits, labels, batch['attention_mask'].astype(jnp.float32)[:, 1:]\n                )\n                return loss, accuracy\n\n            grad_fn = jax.value_and_grad(calculate_loss, has_aux=True)\n            (loss__, accuracy__), grad = grad_fn(state.params)\n            state = state.apply_gradients(grads=grad)\n            return state, loss__, accuracy__\n\n        train_state_shape = jax.eval_shape(init_fn)\n        train_state_partition_spec = match_partition_rules(\n            self.config.get_partition_rules(\n                fully_fsdp=self.arguments.fully_fsdp\n            ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n            train_state_shape\n        )\n        sharded_create_from_params_fn = pjit(\n            create_train_state_from_params,\n            in_shardings=(train_state_partition_spec.params,),\n            out_shardings=train_state_partition_spec,\n            donate_argnums=(0,)\n        )\n        sharded_train_step_fn = pjit(\n            fsdp_train_step_,\n            in_shardings=(train_state_partition_spec, PartitionSpec()),\n            out_shardings=(train_state_partition_spec, PartitionSpec(), PartitionSpec()),\n            donate_argnums=(0, 0),\n        )\n        sharded_predict = pjit(predict, out_shardings=PartitionSpec(),\n                               in_shardings=(train_state_partition_spec, PartitionSpec()))\n        mesh = self.arguments.get_mesh()\n        self.arguments.ckpt_path_exists()\n        ckpt_streamer = self.arguments.get_streaming_checkpointer()\n        self.train_state_partition_spec = train_state_partition_spec\n        self.train_state_shape = train_state_shape\n        return sharded_create_from_params_fn, sharded_train_step_fn, sharded_predict, mesh, ckpt_streamer, init_fn\n\n    def train(self, model_parameters: flax.core.FrozenDict = None) -&gt; OutputFineTuner:\n        \"\"\"\n        The train function is the main function of this module.\n        It takes a model_parameters argument which can be used to load a pretrained model and finetune it.\n        The train function returns an OutputFineTuner object that contains the last saved file name, predict func,\n        train state, mesh and checkpoint streamer.\n\n\n        :param self: Make the class methods aware of other methods and attributes within the class\n        :param model_parameters: flax.core.FrozenDict: Load a pre-trained model\n        :return: An object of type \"OutputFineTuner\"\n\n        \"\"\"\n\n        def count_params(_p):\n            print('\\033[1;31mModel Contain : ',\n                  sum(i.size for i in jax.tree_util.tree_flatten(flax.core.unfreeze(_p))[0]) / 1e9,\n                  ' Billion Parameters')\n\n        dir_prefix: str = '/dev/shm'\n        if self.arguments.track_memory:\n            initialise_tracking(dir_prefix=dir_prefix)\n\n        with self.mesh:\n            if self.finetune:\n                shard_fns, gather_fns = make_shard_and_gather_fns(self.train_state_partition_spec,\n                                                                  dtype_specs=self.dtype)\n\n                if model_parameters is None:\n                    prefix_print(\n                        'Action', f'Loading Model From {self.ckpt_path}'\n                    )\n                    _, params = StreamingCheckpointer.load_trainstate_checkpoint(\n                        f'params::{self.ckpt_path}', self.train_state_shape, shard_fns\n                    )\n\n                    if self.arguments.remove_ckpt_after_load:\n                        os.remove(self.ckpt_path)\n                else:\n                    prefix_print(\n                        'Action', f'Sharding Passed Parameters'\n                    )\n                    from flax.core import unfreeze\n                    if not isinstance(model_parameters, flax.core.FrozenDict):\n                        prefix_print(\n                            'Warning', 'Model Parameters should be like FrozenDict({\"params\" : params}) make sure to '\n                                       'pass as type FrozenDict in case of not getting UnExcepted Errors '\n                        )\n                    params = model_parameters if not self.arguments.do_shard_fns else jax.tree_util.tree_map(\n                        lambda f, x: f(x), shard_fns.params,\n                        model_parameters)\n\n                sharded_train_state_ = self.sharded_create_from_params_fn(params)\n\n                count_params(sharded_train_state_.params)\n            else:\n                sharded_train_state_ = self.init_fn()\n\n                count_params(sharded_train_state_.params)\n\n            pbar = tqdm(total=self.max_steps_train)\n            i = sharded_train_state_.step.tolist()\n            losses = []\n            accuracies = []\n            pbar.update(sharded_train_state_.step.tolist())\n            learning_rates = []\n            if self.arguments.use_wandb:\n                self.wandb_runtime.log(\n                    {\n                        'model billion parameters': sum(\n                            i.size for i in\n                            jax.tree_util.tree_flatten(flax.core.unfreeze(sharded_train_state_.params))[0]) / 1e9\n                    }\n                )\n            try:\n                for ep in range(self.arguments.num_train_epochs):\n                    for batch in self.dataloader_train:\n                        i += 1\n                        if i &lt; self.max_steps_train:\n\n                            batch['labels'] = batch['input_ids'][..., 1:]\n\n                            for ssb in self.arguments.ids_to_pop_from_dataset:\n                                _ = batch.pop(ssb, None)\n                            time_s = time.time()\n                            sharded_train_state_, loss, accuracy = self.sharded_train_step_fn(sharded_train_state_,\n                                                                                              batch\n                                                                                              )\n                            ttl_time = time.time() - time_s\n                            losses.append(loss)\n                            learning_rates.append(self.scheduler(i).tolist())\n                            accuracies.append(accuracy)\n                            if self.arguments.track_memory:\n                                mem_res = get_mem(dir_prefix=dir_prefix)\n                            else:\n                                mem_res = 'Tracking Option is OFF'\n                            pbar.update(1)\n\n                            if self.arguments.use_wandb:\n                                with jax.spmd_mode(\"allow_all\"):\n                                    self.wandb_runtime.log(\n                                        {\n                                            \"loss\": loss.tolist(),\n                                            \"learning_rate\": self.scheduler(\n                                                sharded_train_state_.step.tolist()\n                                            ).tolist(),\n                                            \"step\": sharded_train_state_.step.tolist(),\n                                            \"step time\": ttl_time,\n                                            \"perplexity\": jnp.exp(loss).tolist(),\n                                            \"accuracy\": accuracy.tolist(),\n                                            \"avg_accuracy\": (sum(accuracies) / len(accuracies)).tolist(),\n                                            \"mem_res\": mem_res,\n                                        }\n                                    )\n                            if self.arguments.track_memory:\n                                IPython.display.clear_output(True)\n                                pbar.display(mem_res)\n                            pbar.set_postfix(loss=loss,\n                                             learning_rate=self.scheduler(sharded_train_state_.step.tolist()).tolist(),\n                                             step=sharded_train_state_.step.tolist(),\n                                             perplexity=jnp.exp(loss).tolist(),\n                                             accuracy=accuracy,\n                                             )\n                        else:\n                            break\n                        if self.arguments.save_steps is not None and i % self.arguments.save_steps == 0:\n                            filename = f'{self.arguments.model_name}-{sum(losses) / len(losses)}-{i}'\n                            print(f'Saving Model to \\033[1;30m{filename}\\033[1;0m')\n                            self.ckpt_streamer.save_checkpoint(sharded_train_state_.params['params'],\n                                                               filename,\n                                                               gather_fns=gather_fns.params['params'])\n            except KeyboardInterrupt:\n                print(\n                    '\\033[1;30m KeyboardInterrupt At training model Will return current state of the model * \\033[1;0m')\n            if self.arguments.do_eval:\n                if self.dataset_eval is not None:\n                    pbar_eval = tqdm(total=self.max_steps_eval)\n                    for i_eval, batch_eval in enumerate(self.dataloader_eval):\n                        _ = batch_eval.pop('token_type_ids', None)\n                        batch_eval['labels'] = batch_eval['input_ids'][..., 1:]\n                        for i in self.arguments.ids_to_pop_from_dataset:\n                            _ = batch_eval.pop(i, None)\n                        loss_eval, accuracy = create_fsdp_eval_step(self.arguments.step_partition_spec)(\n                            sharded_train_state_, batch_eval)\n                        pbar_eval.update(1)\n                        if self.arguments.use_wandb:\n                            self.wandb_runtime.log(\n                                {'loss_eval': loss_eval.tolist(),\n                                 'accuracy': accuracy.tolist()}\n                            )\n                        pbar_eval.set_postfix(loss_eval=loss_eval.tolist())\n            if self.arguments.save_steps is None and self.arguments.do_last_save:\n                filename = f'{self.arguments.model_name}-{sum(losses) / len(losses)}-{i}'\n                print(f'Saving Model to \\033[1;30m{filename}\\033[1;0m')\n                self.ckpt_streamer.save_checkpoint(sharded_train_state_.params['params'],\n                                                   filename,\n                                                   gather_fns=gather_fns.params['params'])\n            else:\n                filename = 'not_saved | None'\n        output = OutputFineTuner(\n            last_save_file_name=filename,\n            predict_fun=self.sharded_predict,\n            train_state=sharded_train_state_,\n            mesh=self.mesh,\n            shard_fns=shard_fns,\n            gather_fns=gather_fns,\n            ckpt_stream=self.ckpt_streamer\n        )\n        wandb.finish()\n\n        return output\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-fsdp_train/#lib.python.EasyDel.trainer.fsdp_train.CausalLMTrainer.__init__","title":"<code>__init__(arguments, dataset_train, dataset_eval=None, finetune=True, ckpt_path=None, _do_init_fns=True)</code>","text":"<p>The init function is called when the class is instantiated. It sets up all the variables that are needed for training, including: - The timer to keep track of how long each epoch takes. - The dataloaders for both training and evaluation (if provided). - The model itself, which will be created from a checkpoint if one was provided.  Otherwise,  it will be created from scratch using the arguments passed in by the user.  Note that this function also handles creating a mesh if one was not already specified in arguments  or loaded from a checkpoint file (see below).   This means that you can pass in either</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>arguments</code> <code>TrainArguments</code> <p>TrainArguments: Pass the arguments to the trainer</p> required <code>dataset_train</code> <code>Dataset</code> <p>Dataset: Pass the training dataset to the trainer</p> required <code>dataset_eval</code> <code>Dataset</code> <p>Dataset: Pass the validation dataset</p> <code>None</code> <code>finetune</code> <code>bool</code> <p>bool: Load the model from a checkpoint</p> <code>True</code> <code>ckpt_path</code> <code>Union[str, PathLike]</code> <p>typing.Union[str,os.PathLike] : Load the checkpoint path</p> <code>None</code> <code>_do_init_fns</code> <code>bool</code> <p>bool: Initialize the functions</p> <code>True</code> <p>Returns:</p> Type Description <p>Nothing, it just initializes the class</p> Source code in <code>lib/python/EasyDel/trainer/fsdp_train.py</code> <pre><code>def __init__(self,\n             arguments: TrainArguments,\n             dataset_train: Dataset,\n             dataset_eval: Dataset = None,\n             finetune: bool = True,\n             ckpt_path: typing.Union[str, os.PathLike] = None,\n             _do_init_fns: bool = True\n             ):\n    \"\"\"\n    The __init__ function is called when the class is instantiated.\n    It sets up all the variables that are needed for training, including:\n    - The timer to keep track of how long each epoch takes.\n    - The dataloaders for both training and evaluation (if provided).\n    - The model itself, which will be created from a checkpoint if one was provided.  Otherwise,\n     it will be created from scratch using the arguments passed in by the user.\n     Note that this function also handles creating a mesh if one was not already specified in arguments\n     or loaded from a checkpoint file (see below).   This means that you can pass in either\n\n    :param self: Represent the instance of the class\n    :param arguments: TrainArguments: Pass the arguments to the trainer\n    :param dataset_train: Dataset: Pass the training dataset to the trainer\n    :param dataset_eval: Dataset: Pass the validation dataset\n    :param finetune: bool: Load the model from a checkpoint\n    :param ckpt_path: typing.Union[str,os.PathLike] : Load the checkpoint path\n    :param _do_init_fns: bool: Initialize the functions\n    :return: Nothing, it just initializes the class\n\n    \"\"\"\n    self.timer = None\n    self.dataloader_train = None\n    self.dataloader_eval = None\n    self.model = None\n    self.wandb_runtime = None\n    self.max_steps_train = None\n    self.max_steps_eval = None\n    self.config = None\n    self.scheduler = None\n    self.tx = None\n    self.sharded_create_from_params_fn = None\n    self.sharded_train_step_fn = None\n    self.sharded_predict = None\n    self.mesh = None\n    self.ckpt_streamer = None\n    self.init_fn = None\n    self.train_state_shape = None\n    self.train_state_partition_spec = None\n    self.arguments = arguments\n    self.dataset_train = dataset_train\n    self.dataset_eval = dataset_eval\n    self.finetune = finetune\n    self.ckpt_path = ckpt_path\n    self.dtype = arguments.dtype\n    self.param_dtype = arguments.param_dtype\n    if finetune:\n        if ckpt_path is None:\n            prefix_print(\n                'Warning',\n                'In case of using finetune = True and Passing ckpt_path = None you should pass parameters'\n                'in train function'\n            )\n    if _do_init_fns:\n        self.init_functions()\n    else:\n        prefix_print('Warning', 'you have set _do_init_fns to False so function will not me initialized you have '\n                                f'to do in manually (simply with  trainer.init_functions() )')\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-fsdp_train/#lib.python.EasyDel.trainer.fsdp_train.CausalLMTrainer.configure_dataloader","title":"<code>configure_dataloader()</code>","text":"<p>The configure_dataloader function is used to configure the dataloader for training and evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Refer to the class instance itself</p> required <p>Returns:</p> Type Description <p>A dataloader_train, max_steps_train, dataloader_eval and max steps eval</p> Source code in <code>lib/python/EasyDel/trainer/fsdp_train.py</code> <pre><code>def configure_dataloader(self):\n\n    \"\"\"\n    The configure_dataloader function is used to configure the dataloader for training and evaluation.\n\n    :param self: Refer to the class instance itself\n    :return: A dataloader_train, max_steps_train, dataloader_eval and max steps eval\n\n    \"\"\"\n\n    def collate_fn(batch):\n        rs = {}\n        for key in batch[0].keys():\n            if self.arguments.is_left_padded:\n                ssp = [jnp.array(f[key])[..., -self.arguments.max_length:] for f in batch]\n            else:\n                ssp = [jnp.array(f[key])[..., :self.arguments.max_length] for f in batch]\n            rs[key] = jnp.stack(ssp).reshape(-1, ssp[0].shape[-1])\n        return rs\n\n    dataloader_train = DataLoader(self.dataset_train, collate_fn=collate_fn,\n                                  batch_size=self.arguments.total_batch_size, drop_last=True)\n    max_steps_train = self.arguments.num_train_epochs * len(\n        dataloader_train) if self.arguments.max_steps is None else self.arguments.max_steps\n    if self.dataset_eval is not None and self.arguments.do_eval:\n        dataloader_eval = DataLoader(self.dataset_eval, collate_fn=collate_fn,\n                                     batch_size=self.arguments.total_batch_size, drop_last=True)\n        max_steps_eval = len(\n            dataloader_eval) if self.arguments.max_steps is None else self.arguments.max_steps\n    else:\n        dataloader_eval, max_steps_eval = None, 0\n    return dataloader_train, max_steps_train, dataloader_eval, max_steps_eval\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-fsdp_train/#lib.python.EasyDel.trainer.fsdp_train.CausalLMTrainer.configure_functions","title":"<code>configure_functions()</code>","text":"<p>The configure_functions function is responsible for configuring the functions that will be used in training. It does this by first defining a function called init_fn, which initializes the model parameters and returns them as a TrainState object. The TrainState object contains all of the information needed to train or evaluate on a batch of data, including:</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the class attributes</p> required <p>Returns:</p> Type Description <p>A tuple of functions</p> Source code in <code>lib/python/EasyDel/trainer/fsdp_train.py</code> <pre><code>def configure_functions(self):\n    \"\"\"\n    The configure_functions function is responsible for configuring the functions that will be used in training.\n    It does this by first defining a function called init_fn, which initializes the model parameters and returns them as a\n    TrainState object. The TrainState object contains all of the information needed to train or evaluate on a batch of data,\n    including:\n\n    :param self: Access the class attributes\n    :return: A tuple of functions\n\n    \"\"\"\n\n    def init_fn():\n        params__ = self.model.init_weights(\n            jax.random.PRNGKey(0), self.arguments.init_input_shape\n        )\n        if self.arguments.dtype == jnp.bfloat16:\n            params__ = self.model.to_bf16(params__)\n        elif self.arguments.dtype == jnp.float16:\n            params__ = self.model.to_fp16(params__)\n        return train_state.TrainState.create(\n            tx=self.tx,\n            params=flax.core.freeze({'params': params__}),\n            apply_fn=self.model.__call__\n        )\n\n    def create_train_state_from_params(params_):\n        return train_state.TrainState.create(\n            tx=self.tx,\n            apply_fn=self.model.__call__,\n            params=params_\n        )\n\n    if self.arguments.loss_remat == 'OHA':\n        loss_fn = fjformer.func.loss_func.cross_entropy_with_logits\n    elif self.arguments.loss_remat != '':\n        loss_fn = fused_cross_entropy_loss_and_accuracy\n    else:\n        loss_fn = cross_entropy_loss_and_accuracy\n\n    def fsdp_train_step_(state, batch):\n        batch = with_sharding_constraint(batch, self.arguments.step_partition_spec)\n\n        def calculate_loss(params):\n            labels = batch.pop('labels')\n            logits = state.apply_fn(params=params, **batch,\n                                    return_dict=True).logits[:, :-1, :]\n\n            loss, accuracy = loss_fn(\n                logits, labels, batch['attention_mask'].astype(jnp.float32)[:, 1:]\n            )\n            return loss, accuracy\n\n        grad_fn = jax.value_and_grad(calculate_loss, has_aux=True)\n        (loss__, accuracy__), grad = grad_fn(state.params)\n        state = state.apply_gradients(grads=grad)\n        return state, loss__, accuracy__\n\n    train_state_shape = jax.eval_shape(init_fn)\n    train_state_partition_spec = match_partition_rules(\n        self.config.get_partition_rules(\n            fully_fsdp=self.arguments.fully_fsdp\n        ) if self.arguments.custom_rule is None else self.arguments.custom_rule,\n        train_state_shape\n    )\n    sharded_create_from_params_fn = pjit(\n        create_train_state_from_params,\n        in_shardings=(train_state_partition_spec.params,),\n        out_shardings=train_state_partition_spec,\n        donate_argnums=(0,)\n    )\n    sharded_train_step_fn = pjit(\n        fsdp_train_step_,\n        in_shardings=(train_state_partition_spec, PartitionSpec()),\n        out_shardings=(train_state_partition_spec, PartitionSpec(), PartitionSpec()),\n        donate_argnums=(0, 0),\n    )\n    sharded_predict = pjit(predict, out_shardings=PartitionSpec(),\n                           in_shardings=(train_state_partition_spec, PartitionSpec()))\n    mesh = self.arguments.get_mesh()\n    self.arguments.ckpt_path_exists()\n    ckpt_streamer = self.arguments.get_streaming_checkpointer()\n    self.train_state_partition_spec = train_state_partition_spec\n    self.train_state_shape = train_state_shape\n    return sharded_create_from_params_fn, sharded_train_step_fn, sharded_predict, mesh, ckpt_streamer, init_fn\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-fsdp_train/#lib.python.EasyDel.trainer.fsdp_train.CausalLMTrainer.configure_model","title":"<code>configure_model()</code>","text":"<p>The configure_model function is responsible for creating the model, optimizer and scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A model, optimizer, scheduler and config</p> Source code in <code>lib/python/EasyDel/trainer/fsdp_train.py</code> <pre><code>def configure_model(self):\n    \"\"\"\n    The configure_model function is responsible for creating the model, optimizer and scheduler.\n\n    :param self: Represent the instance of the class\n    :return: A model, optimizer, scheduler and config\n\n    \"\"\"\n    extra_configs = {} if self.arguments.extra_configs is None else self.arguments.extra_configs\n    if self.arguments.model_class is None:\n        config = AutoConfig.from_pretrained(self.arguments.model_id, trust_remote_code=True\n                                            , gradient_checkpointing=self.arguments.gradient_checkpointing,\n                                            use_pjit_attention_force=self.arguments.use_pjit_attention_force,\n                                            **extra_configs\n                                            )\n\n        assert hasattr(config, 'get_partition_rules')\n        model = FlaxAutoModelForCausalLM.from_config(config, trust_remote_code=True, dtype=self.arguments.dtype,\n                                                     param_dtype=self.arguments.param_dtype,\n                                                     _do_init=False)\n\n    else:\n        if not hasattr(self.arguments.configs_to_init_model_class['config'], 'get_partition_rules'):\n            assert self.arguments.custom_rule is not None, 'if you are using custom model to init you must' \\\n                                                           ' pass custom_rule for partition rules '\n\n        self.arguments.configs_to_init_model_class[\n            'config'\n        ].use_pjit_attention_force = self.arguments.use_pjit_attention_force\n\n        self.arguments.configs_to_init_model_class['config'].axis_dims = self.arguments.sharding_array\n\n        model = self.arguments.model_class(\n            **self.arguments.configs_to_init_model_class,\n            _do_init=False\n        )\n\n        config = self.arguments.configs_to_init_model_class['config']\n\n    tx, scheduler = self.arguments.get_optimizer_and_scheduler(self.max_steps_train)\n    return model, tx, scheduler, config\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-fsdp_train/#lib.python.EasyDel.trainer.fsdp_train.CausalLMTrainer.finish","title":"<code>finish()</code>  <code>staticmethod</code>","text":"<p>The finish function is called when the experiment ends. It can be used to save data, upload files, or do any other cleanup tasks.</p> <p>Returns:</p> Type Description <p>A dictionary of the run's metadata</p> Source code in <code>lib/python/EasyDel/trainer/fsdp_train.py</code> <pre><code>@staticmethod\ndef finish():\n    \"\"\"\n    The finish function is called when the experiment ends.\n    It can be used to save data, upload files, or do any other cleanup tasks.\n\n    :return: A dictionary of the run's metadata\n\n    \"\"\"\n    wandb.finish()\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-fsdp_train/#lib.python.EasyDel.trainer.fsdp_train.CausalLMTrainer.init_functions","title":"<code>init_functions()</code>","text":"<p>The init_functions function is responsible for initializing the following:     - wandb_runtime (if you use_wandb is True)     - timer object (for logging time taken by various functions)     - dataloader objects for training and evaluation data, along with max steps per epoch.       The configure_dataloader function accomplishes this task.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>A tuple of functions</p> Source code in <code>lib/python/EasyDel/trainer/fsdp_train.py</code> <pre><code>def init_functions(self):\n    \"\"\"\n    The init_functions function is responsible for initializing the following:\n        - wandb_runtime (if you use_wandb is True)\n        - timer object (for logging time taken by various functions)\n        - dataloader objects for training and evaluation data, along with max steps per epoch.\n          The configure_dataloader function accomplishes this task.\n\n    :param self: Represent the instance of the class\n    :return: A tuple of functions\n\n    \"\"\"\n    self.wandb_runtime = self.arguments.get_wandb_init() if self.arguments.use_wandb else None\n    self.timer = Timers(\n        use_wandb=False,\n        tensorboard_writer=self.arguments.get_board()\n    )\n    self.timer(\n        'configure dataloaders'\n    ).start()\n    self.dataloader_train, self.max_steps_train, \\\n        self.dataloader_eval, self.max_steps_eval = self.configure_dataloader()\n    self.timer(\n        'configure dataloaders'\n    ).stop()\n\n    self.timer.log(['configure dataloaders'])\n\n    self.timer(\n        'configure Model ,Optimizer ,Scheduler and Config'\n    ).start()\n    self.model, self.tx, self.scheduler, self.config = self.configure_model()\n    self.timer(\n        'configure Model ,Optimizer ,Scheduler and Config'\n    ).stop()\n    self.timer.log(['configure Model ,Optimizer ,Scheduler and Config'])\n\n    self.timer(\n        'configure functions and sharding them'\n    ).start()\n    funcs = self.configure_functions()\n    self.sharded_create_from_params_fn = funcs[0]\n    self.sharded_train_step_fn = funcs[1]\n    self.sharded_predict = funcs[2]\n    self.mesh = funcs[3]\n    self.ckpt_streamer = funcs[4]\n    self.init_fn = funcs[5]\n    self.timer(\n        'configure functions and sharding them'\n    ).stop()\n    self.timer.log(['configure functions and sharding them'])\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-fsdp_train/#lib.python.EasyDel.trainer.fsdp_train.CausalLMTrainer.train","title":"<code>train(model_parameters=None)</code>","text":"<p>The train function is the main function of this module. It takes a model_parameters argument which can be used to load a pretrained model and finetune it. The train function returns an OutputFineTuner object that contains the last saved file name, predict func, train state, mesh and checkpoint streamer.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Make the class methods aware of other methods and attributes within the class</p> required <code>model_parameters</code> <code>FrozenDict</code> <p>flax.core.FrozenDict: Load a pre-trained model</p> <code>None</code> <p>Returns:</p> Type Description <code>OutputFineTuner</code> <p>An object of type \"OutputFineTuner\"</p> Source code in <code>lib/python/EasyDel/trainer/fsdp_train.py</code> <pre><code>def train(self, model_parameters: flax.core.FrozenDict = None) -&gt; OutputFineTuner:\n    \"\"\"\n    The train function is the main function of this module.\n    It takes a model_parameters argument which can be used to load a pretrained model and finetune it.\n    The train function returns an OutputFineTuner object that contains the last saved file name, predict func,\n    train state, mesh and checkpoint streamer.\n\n\n    :param self: Make the class methods aware of other methods and attributes within the class\n    :param model_parameters: flax.core.FrozenDict: Load a pre-trained model\n    :return: An object of type \"OutputFineTuner\"\n\n    \"\"\"\n\n    def count_params(_p):\n        print('\\033[1;31mModel Contain : ',\n              sum(i.size for i in jax.tree_util.tree_flatten(flax.core.unfreeze(_p))[0]) / 1e9,\n              ' Billion Parameters')\n\n    dir_prefix: str = '/dev/shm'\n    if self.arguments.track_memory:\n        initialise_tracking(dir_prefix=dir_prefix)\n\n    with self.mesh:\n        if self.finetune:\n            shard_fns, gather_fns = make_shard_and_gather_fns(self.train_state_partition_spec,\n                                                              dtype_specs=self.dtype)\n\n            if model_parameters is None:\n                prefix_print(\n                    'Action', f'Loading Model From {self.ckpt_path}'\n                )\n                _, params = StreamingCheckpointer.load_trainstate_checkpoint(\n                    f'params::{self.ckpt_path}', self.train_state_shape, shard_fns\n                )\n\n                if self.arguments.remove_ckpt_after_load:\n                    os.remove(self.ckpt_path)\n            else:\n                prefix_print(\n                    'Action', f'Sharding Passed Parameters'\n                )\n                from flax.core import unfreeze\n                if not isinstance(model_parameters, flax.core.FrozenDict):\n                    prefix_print(\n                        'Warning', 'Model Parameters should be like FrozenDict({\"params\" : params}) make sure to '\n                                   'pass as type FrozenDict in case of not getting UnExcepted Errors '\n                    )\n                params = model_parameters if not self.arguments.do_shard_fns else jax.tree_util.tree_map(\n                    lambda f, x: f(x), shard_fns.params,\n                    model_parameters)\n\n            sharded_train_state_ = self.sharded_create_from_params_fn(params)\n\n            count_params(sharded_train_state_.params)\n        else:\n            sharded_train_state_ = self.init_fn()\n\n            count_params(sharded_train_state_.params)\n\n        pbar = tqdm(total=self.max_steps_train)\n        i = sharded_train_state_.step.tolist()\n        losses = []\n        accuracies = []\n        pbar.update(sharded_train_state_.step.tolist())\n        learning_rates = []\n        if self.arguments.use_wandb:\n            self.wandb_runtime.log(\n                {\n                    'model billion parameters': sum(\n                        i.size for i in\n                        jax.tree_util.tree_flatten(flax.core.unfreeze(sharded_train_state_.params))[0]) / 1e9\n                }\n            )\n        try:\n            for ep in range(self.arguments.num_train_epochs):\n                for batch in self.dataloader_train:\n                    i += 1\n                    if i &lt; self.max_steps_train:\n\n                        batch['labels'] = batch['input_ids'][..., 1:]\n\n                        for ssb in self.arguments.ids_to_pop_from_dataset:\n                            _ = batch.pop(ssb, None)\n                        time_s = time.time()\n                        sharded_train_state_, loss, accuracy = self.sharded_train_step_fn(sharded_train_state_,\n                                                                                          batch\n                                                                                          )\n                        ttl_time = time.time() - time_s\n                        losses.append(loss)\n                        learning_rates.append(self.scheduler(i).tolist())\n                        accuracies.append(accuracy)\n                        if self.arguments.track_memory:\n                            mem_res = get_mem(dir_prefix=dir_prefix)\n                        else:\n                            mem_res = 'Tracking Option is OFF'\n                        pbar.update(1)\n\n                        if self.arguments.use_wandb:\n                            with jax.spmd_mode(\"allow_all\"):\n                                self.wandb_runtime.log(\n                                    {\n                                        \"loss\": loss.tolist(),\n                                        \"learning_rate\": self.scheduler(\n                                            sharded_train_state_.step.tolist()\n                                        ).tolist(),\n                                        \"step\": sharded_train_state_.step.tolist(),\n                                        \"step time\": ttl_time,\n                                        \"perplexity\": jnp.exp(loss).tolist(),\n                                        \"accuracy\": accuracy.tolist(),\n                                        \"avg_accuracy\": (sum(accuracies) / len(accuracies)).tolist(),\n                                        \"mem_res\": mem_res,\n                                    }\n                                )\n                        if self.arguments.track_memory:\n                            IPython.display.clear_output(True)\n                            pbar.display(mem_res)\n                        pbar.set_postfix(loss=loss,\n                                         learning_rate=self.scheduler(sharded_train_state_.step.tolist()).tolist(),\n                                         step=sharded_train_state_.step.tolist(),\n                                         perplexity=jnp.exp(loss).tolist(),\n                                         accuracy=accuracy,\n                                         )\n                    else:\n                        break\n                    if self.arguments.save_steps is not None and i % self.arguments.save_steps == 0:\n                        filename = f'{self.arguments.model_name}-{sum(losses) / len(losses)}-{i}'\n                        print(f'Saving Model to \\033[1;30m{filename}\\033[1;0m')\n                        self.ckpt_streamer.save_checkpoint(sharded_train_state_.params['params'],\n                                                           filename,\n                                                           gather_fns=gather_fns.params['params'])\n        except KeyboardInterrupt:\n            print(\n                '\\033[1;30m KeyboardInterrupt At training model Will return current state of the model * \\033[1;0m')\n        if self.arguments.do_eval:\n            if self.dataset_eval is not None:\n                pbar_eval = tqdm(total=self.max_steps_eval)\n                for i_eval, batch_eval in enumerate(self.dataloader_eval):\n                    _ = batch_eval.pop('token_type_ids', None)\n                    batch_eval['labels'] = batch_eval['input_ids'][..., 1:]\n                    for i in self.arguments.ids_to_pop_from_dataset:\n                        _ = batch_eval.pop(i, None)\n                    loss_eval, accuracy = create_fsdp_eval_step(self.arguments.step_partition_spec)(\n                        sharded_train_state_, batch_eval)\n                    pbar_eval.update(1)\n                    if self.arguments.use_wandb:\n                        self.wandb_runtime.log(\n                            {'loss_eval': loss_eval.tolist(),\n                             'accuracy': accuracy.tolist()}\n                        )\n                    pbar_eval.set_postfix(loss_eval=loss_eval.tolist())\n        if self.arguments.save_steps is None and self.arguments.do_last_save:\n            filename = f'{self.arguments.model_name}-{sum(losses) / len(losses)}-{i}'\n            print(f'Saving Model to \\033[1;30m{filename}\\033[1;0m')\n            self.ckpt_streamer.save_checkpoint(sharded_train_state_.params['params'],\n                                               filename,\n                                               gather_fns=gather_fns.params['params'])\n        else:\n            filename = 'not_saved | None'\n    output = OutputFineTuner(\n        last_save_file_name=filename,\n        predict_fun=self.sharded_predict,\n        train_state=sharded_train_state_,\n        mesh=self.mesh,\n        shard_fns=shard_fns,\n        gather_fns=gather_fns,\n        ckpt_stream=self.ckpt_streamer\n    )\n    wandb.finish()\n\n    return output\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-fsdp_train/#lib.python.EasyDel.trainer.fsdp_train.calculate_accuracy","title":"<code>calculate_accuracy(predictions, targets)</code>","text":"<p>The calculate_accuracy function takes in two arrays, predictions and targets. The function then calculates the accuracy of the model by comparing the predicted classes to the target classes. The predicted class is determined by taking argmax along axis - 1 of predictions. The correct_predictions variable is an array containing True or False values depending on whether or not the prediction was correct for each example in a batch. The total number of examples that were correctly predicted are summed up and divided by the total number of examples to get an accuracy value between 0 and 1.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Array</code> <p>chex.Array: Pass in the predictions from the model</p> required <code>targets</code> <code>Array</code> <p>chex.Array: Calculate the accuracy of the model</p> required <p>Returns:</p> Type Description <p>A single value, the accuracy</p> Source code in <code>lib/python/EasyDel/trainer/fsdp_train.py</code> <pre><code>def calculate_accuracy(predictions: chex.Array, targets: chex.Array):\n    \"\"\"\n    The calculate_accuracy function takes in two arrays, predictions and targets.\n    The function then calculates the accuracy of the model by comparing the predicted classes to\n    the target classes. The predicted class is determined by taking argmax along axis - 1 of predictions.\n    The correct_predictions variable is an array containing True or False values depending on whether or not\n    the prediction was correct for each example in a batch. The total number of examples that were correctly\n    predicted are summed up and divided by the total number of examples to get an accuracy value between 0 and 1.\n\n    :param predictions: chex.Array: Pass in the predictions from the model\n    :param targets: chex.Array: Calculate the accuracy of the model\n    :return: A single value, the accuracy\n\n    \"\"\"\n    predicted_classes = jnp.argmax(predictions, axis=-1)\n    correct_predictions = (predicted_classes == targets).sum()\n    total_predictions = targets.shape[0]\n    accuracy = correct_predictions / total_predictions\n    return accuracy\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-fsdp_train/#lib.python.EasyDel.trainer.fsdp_train.create_fsdp_eval_step","title":"<code>create_fsdp_eval_step(partition_spec=PartitionSpec(('dp', 'fsdp'), 'mp'))</code>","text":"<p>The create_fsdp_eval_step function is used to create a function that calculates the loss and accuracy of a model. It takes in a set of parameters, which are then passed into the state.apply_fn function to generate logits for each token in the batch. The cross entropy loss and accuracy are then calculated from these logits.</p> <p>Parameters:</p> Name Type Description Default <code>partition_spec</code> <p>Specify the partitioning of the model parameters</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp')</code> <p>Returns:</p> Type Description <p>A function that can be used to calculate the loss and accuracy of a model</p> Source code in <code>lib/python/EasyDel/trainer/fsdp_train.py</code> <pre><code>def create_fsdp_eval_step(partition_spec=PartitionSpec(('dp', 'fsdp'), 'mp')):\n    \"\"\"\n    The create_fsdp_eval_step function is used to create a function that calculates the loss and accuracy of a model.\n    It takes in a set of parameters, which are then passed into the state.apply_fn function\n    to generate logits for each token in the batch. The cross entropy loss and accuracy are then calculated from these logits.\n\n    :param partition_spec: Specify the partitioning of the model parameters\n    :return: A function that can be used to calculate the loss and accuracy of a model\n\n    \"\"\"\n\n    def fsdp_eval_step(state, batch_eval):\n        \"\"\"\n        The fsdp_eval_step function is used to calculate the loss and accuracy of a model.\n        It takes in a set of parameters, which are then passed into the state.apply_fn function\n        to generate logits for each token in the batch. The cross entropy loss and accuracy are then calculated from these logits.\n\n        :param state: Store the model parameters and other information about the training process\n        :param batch_eval: Pass the batch of data to the function\n        :return: The loss and accuracy of the model\n\n        \"\"\"\n        batch_eval = with_sharding_constraint(\n            batch_eval, partition_spec\n        )\n\n        def calculate_loss(params):\n            \"\"\"\n            The calculate_loss function is used to calculate the loss and accuracy of a model.\n            It takes in a set of parameters, which are then passed into the state.apply_fn function\n            to generate logits for each token in the batch. The cross entropy loss and accuracy are then calculated from these logits.\n\n            :param params: Pass the model parameters to the function\n            :return: The loss and the accuracy\n\n            \"\"\"\n            labels = batch_eval.pop('labels')\n            logits = state.apply_fn(params=params, **batch_eval,\n                                    return_dict=True).logits\n\n            loss, accuracy = cross_entropy_loss_and_accuracy(\n                logits[:, :-1, :], labels, batch_eval['attention_mask'].astype(jnp.float32)[:, 1:]\n            )\n            return loss, accuracy\n\n        loss__, accuracy__ = calculate_loss(state.params)\n        return loss__, accuracy__\n\n    return fsdp_eval_step\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-fsdp_train/#lib.python.EasyDel.trainer.fsdp_train.create_fsdp_train_step","title":"<code>create_fsdp_train_step(partition_spec=PartitionSpec(('dp', 'fsdp'), 'mp'))</code>","text":"<p>The create_fsdp_train_step function is a training step function that takes in the current state of the model, and a batch of data. It then calculates the loss and accuracy for this batch, and returns an updated state with new parameters based on these gradients.</p> <p>Parameters:</p> Name Type Description Default <code>partition_spec</code> <p>Specify which devices the model will be split across</p> <code>PartitionSpec(('dp', 'fsdp'), 'mp')</code> <p>Returns:</p> Type Description <p>A fsdp_train_step function that takes in the current state of the model,</p> Source code in <code>lib/python/EasyDel/trainer/fsdp_train.py</code> <pre><code>def create_fsdp_train_step(partition_spec=PartitionSpec(('dp', 'fsdp'), 'mp')):\n    \"\"\"\n    The create_fsdp_train_step function is a training step function that takes in the current state of the model,\n    and a batch of data. It then calculates the loss and accuracy for this batch, and returns an updated state\n    with new parameters based on these gradients.\n\n    :param partition_spec: Specify which devices the model will be split across\n    :return: A fsdp_train_step function that takes in the current state of the model,\n\n    \"\"\"\n\n    def fsdp_train_step(state, batch):\n        \"\"\"\n        The fsdp_train_step function is a training step function that takes in the current state of the model,\n        and a batch of data. It then calculates the loss and accuracy for this batch, and returns an updated state\n        with new parameters based on these gradients.\n\n        :param state: Store the model parameters\n        :param batch: Pass the data to the model\n        :return: A tuple of (state, loss, accuracy)\n\n        \"\"\"\n        batch = with_sharding_constraint(batch, partition_spec)\n\n        def calculate_loss(params):\n            labels = batch.pop('labels')\n            logits = state.apply_fn(params=params, **batch,\n                                    return_dict=True).logits\n\n            loss, accuracy = cross_entropy_loss_and_accuracy(\n                logits[:, :-1, :], labels, batch['attention_mask'].astype(jnp.float32)[:, 1:]\n            )\n            return loss, accuracy\n\n        grad_fn = jax.value_and_grad(calculate_loss, has_aux=True)\n        (loss__, accuracy__), grad = grad_fn(state.params)\n        state = state.apply_gradients(grads=grad)\n        return state, loss__, accuracy__\n\n    return fsdp_train_step\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-fsdp_train/#lib.python.EasyDel.trainer.fsdp_train.predict","title":"<code>predict(state, input_ids)</code>","text":"<p>The predict function takes in a state and input_ids, and returns the next token.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <p>Store the model parameters and the input_ids parameter is used to pass in a batch of token ids</p> required <code>input_ids</code> <p>Pass the input to the model</p> required <p>Returns:</p> Type Description <p>The next input_ids</p> Source code in <code>lib/python/EasyDel/trainer/fsdp_train.py</code> <pre><code>def predict(state, input_ids):\n    \"\"\"\n    The predict function takes in a state and input_ids, and returns the next token.\n\n    :param state: Store the model parameters and the input_ids parameter is used to pass in a batch of token ids\n    :param input_ids: Pass the input to the model\n    :return: The next input_ids\n\n    \"\"\"\n    input_ids = with_sharding_constraint(input_ids, PartitionSpec(('dp', 'fsdp')))\n    pred = state.apply_fn(params=state.params, input_ids=input_ids, return_dict=True)\n    token = jnp.argmax(jax.nn.softmax(pred.logits)[:, -1, :])\n    input_ids = jnp.concatenate([input_ids, token.reshape(1, -1)], axis=-1)\n    return input_ids\n</code></pre>"},{"location":"lib-python-EasyDel-trainer-tf_dataset/","title":"trainer.tf_dataset","text":""},{"location":"lib-python-EasyDel-trainer-training_utils/","title":"trainer.training_utils","text":""},{"location":"lib-python-EasyDel-transform-easydel_transform/","title":"transform.easydel_transform","text":""},{"location":"lib-python-EasyDel-transform-easydel_transform/#lib.python.EasyDel.transform.easydel_transform.float_tensor_to_dtype","title":"<code>float_tensor_to_dtype(tensor, dtype)</code>","text":"<p>The float_tensor_to_dtype function is used to convert a tensor's dtype to the specified dtype.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <p>Convert the tensor to a float dtype</p> required <code>dtype</code> <p>Convert the tensor to a specific dtype</p> required <p>Returns:</p> Type Description <p>A tensor with the specified dtype</p> Source code in <code>lib/python/EasyDel/transform/easydel_transform.py</code> <pre><code>def float_tensor_to_dtype(tensor, dtype):\n    \"\"\"\n    The float_tensor_to_dtype function is used to convert a tensor's dtype to the specified dtype.\n\n    :param tensor: Convert the tensor to a float dtype\n    :param dtype: Convert the tensor to a specific dtype\n    :return: A tensor with the specified dtype\n\n    \"\"\"\n    if dtype is None or dtype == '':\n        return tensor\n    if isinstance(dtype, str):\n        dtype = get_float_dtype_by_name(dtype)\n    float_dtypes = (jax.numpy.bfloat16, jax.numpy.float16, jax.numpy.float32, jax.numpy.float64)\n    if getattr(tensor, 'dtype', None) in float_dtypes:\n        tensor = tensor.astype(dtype)\n    return tensor\n</code></pre>"},{"location":"lib-python-EasyDel-transform-easydel_transform/#lib.python.EasyDel.transform.easydel_transform.get_float_dtype_by_name","title":"<code>get_float_dtype_by_name(dtype)</code>","text":"<p>The get_float_dtype_by_name function is a helper function that returns the JAX float dtype corresponding to the string name of a floating point type.  This is useful for converting between strings and JAX float types, which are used in many places throughout this codebase.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <p>Specify the type of data that is being passed into the function</p> required <p>Returns:</p> Type Description <p>The float dtype of the input string</p> Source code in <code>lib/python/EasyDel/transform/easydel_transform.py</code> <pre><code>def get_float_dtype_by_name(dtype):\n    \"\"\"\n    The get_float_dtype_by_name function is a helper function that returns the JAX float dtype\n    corresponding to the string name of a floating point type.  This is useful for converting\n    between strings and JAX float types, which are used in many places throughout this codebase.\n\n\n    :param dtype: Specify the type of data that is being passed into the function\n    :return: The float dtype of the input string\n\n    \"\"\"\n    return {\n        'bf16': jax.numpy.bfloat16,\n        'bfloat16': jax.numpy.bfloat16,\n        'fp16': jax.numpy.float16,\n        'float16': jax.numpy.float16,\n        'fp32': jax.numpy.float32,\n        'float32': jax.numpy.float32,\n        'fp64': jax.numpy.float64,\n        'float64': jax.numpy.float64,\n    }[dtype]\n</code></pre>"},{"location":"lib-python-EasyDel-transform-easydel_transform/#lib.python.EasyDel.transform.easydel_transform.huggingface_to_easydel","title":"<code>huggingface_to_easydel(state_dict, embedding_layer_name, device, dtype=jax.numpy.float16)</code>","text":"<p>The huggingface_to_easydel function takes a huggingface model's state_dict and converts it to an easydel model's flax_dict. The function is designed to be used in conjunction with the load_huggingface function, which loads a huggingface model from disk. The embedding layer name must be specified as well as the device on which the conversion will take place.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <p>Load the weights from a huggingface model</p> required <code>embedding_layer_name</code> <code>str</code> <p>str: Identify the embedding layer in the huggingface model</p> required <code>device</code> <p>Determine which device the model will be loaded on</p> required <code>dtype</code> <code>dtype</code> <p>jax.numpy.dtype: Specify the data type of the tensors</p> <code>float16</code> <p>Returns:</p> Type Description <p>A dictionary of the weights and biases in a format that can be used by flax (it's an UnFlattenDict)</p> Source code in <code>lib/python/EasyDel/transform/easydel_transform.py</code> <pre><code>def huggingface_to_easydel(\n        state_dict,\n        embedding_layer_name: str,\n        device,\n        dtype: jax.numpy.dtype = jax.numpy.float16\n):\n    \"\"\"\n    The huggingface_to_easydel function takes a huggingface model's state_dict and converts it to an easydel\n    model's flax_dict. The function is designed to be used in conjunction with the load_huggingface function, which\n    loads a huggingface model from disk. The embedding layer name must be specified as well as the device on which\n    the conversion will take place.\n\n    :param state_dict: Load the weights from a huggingface model\n    :param embedding_layer_name: str: Identify the embedding layer in the huggingface model\n    :param device: Determine which device the model will be loaded on\n    :param dtype: jax.numpy.dtype: Specify the data type of the tensors\n    :return: A dictionary of the weights and biases in a format that can be used by flax (it's an UnFlattenDict)\n\n    \"\"\"\n    _l = len('.weight')\n    with jax.default_device(device):\n        flax_dict = {}\n        for key, tensor in state_dict.items():\n            if embedding_layer_name in key:\n                # tensor = tensor.transpose(0, 1)\n                key = key[:-_l] + '.embedding'\n            elif match_keywords(key, ['kernel'], ['none']):\n                if len(tensor.shape) == 2:\n                    tensor = tensor.transpose(0, 1)\n                if key.endswith('.weight'):\n                    key = key[:-_l] + '.kernel'\n            key_tuple = key.split('.')\n            key_names = ()\n            tensor = tensor.detach().cpu().numpy()\n            for k in key_tuple:\n                key_names += k,\n            flax_dict[key_names] = tensor.astype(dtype)\n        return flax.traverse_util.unflatten_dict(flax_dict)\n</code></pre>"},{"location":"lib-python-EasyDel-transform-easydel_transform/#lib.python.EasyDel.transform.easydel_transform.match_keywords","title":"<code>match_keywords(string, ts, ns)</code>","text":"<p>The match_keywords function takes a string, and two lists of strings. The first list is the \"must-have\" keywords, and the second list is the \"not-allowed\" keywords. It returns True if all of the must-have keywords are in string, but none of not allowed are in it.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <p>Pass in the text that is being searched</p> required <code>ts</code> <p>Specify the required keywords and ns is used to specify the non-required keywords</p> required <code>ns</code> <p>Specify a list of negative keywords</p> required <p>Returns:</p> Type Description <p>True if all the keywords in ts are present and none of the</p> Source code in <code>lib/python/EasyDel/transform/easydel_transform.py</code> <pre><code>def match_keywords(string, ts, ns):\n    \"\"\"\n    The match_keywords function takes a string, and two lists of strings.\n    The first list is the &amp;quot;must-have&amp;quot; keywords, and the second list is the &amp;quot;not-allowed&amp;quot; keywords.\n    It returns True if all of the must-have keywords are in string, but none of not allowed are in it.\n\n    :param string: Pass in the text that is being searched\n    :param ts: Specify the required keywords and ns is used to specify the non-required keywords\n    :param ns: Specify a list of negative keywords\n    :return: True if all the keywords in ts are present and none of the\n\n    \"\"\"\n    for t in ts:\n        if t not in string:\n            return False\n    for n in ns:\n        if n in string:\n            return False\n    return True\n</code></pre>"},{"location":"lib-python-EasyDel-transform-easydel_transform/#lib.python.EasyDel.transform.easydel_transform.read_ckpt","title":"<code>read_ckpt(path, shard_fns=None, add_extra_past_fix=None)</code>","text":"<p>The read_ckpt function reads a checkpoint file and returns the tensors in it.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>[str, PathLike]</code> <p>[str: Specify the path to the checkpoint file</p> required <code>os.PathLike]</code> <p>Specify the path to the checkpoint file</p> required <code>shard_fns</code> <p>Shard the tensors</p> <code>None</code> <code>add_extra_past_fix</code> <code>list</code> <p>list: Add an extra past to the key</p> <code>None</code> <p>Returns:</p> Type Description <p>A dictionary of tensors</p> Source code in <code>lib/python/EasyDel/transform/easydel_transform.py</code> <pre><code>def read_ckpt(path: [str, os.PathLike], shard_fns=None, add_extra_past_fix: list = None):\n    \"\"\"\n    The read_ckpt function reads a checkpoint file and returns the tensors in it.\n\n    :param path: [str: Specify the path to the checkpoint file\n    :param os.PathLike]: Specify the path to the checkpoint file\n    :param shard_fns: Shard the tensors\n    :param add_extra_past_fix: list: Add an extra past to the key\n    :return: A dictionary of tensors\n\n    \"\"\"\n    tensors = {}\n    with open(path, 'rb') as stream:\n        unpacker = msgpack.Unpacker(stream, read_size=83886080, max_buffer_size=0)\n        for key, value in unpacker:\n            if add_extra_past_fix is not None:\n                key = add_extra_past_fix + key\n            key = tuple(key)\n            tensor = from_bytes(None, value)\n            if shard_fns is not None:\n                tensor = shard_fns[key](tensor)\n            tensors[key] = tensor\n    return tensors\n</code></pre>"},{"location":"lib-python-EasyDel-transform-easydel_transform/#lib.python.EasyDel.transform.easydel_transform.save_ckpt","title":"<code>save_ckpt(train_state, path, gather_fns=None, float_dtype=None)</code>","text":"<p>The save_ckpt function saves the state of a training run to disk.</p> <p>Parameters:</p> Name Type Description Default <code>train_state</code> <p>Store the current state of the training process</p> required <code>path</code> <p>Specify the location of the checkpoint file</p> required <code>gather_fns</code> <p>Specify a function that will be used to convert the tensor to bytes</p> <code>None</code> <code>float_dtype</code> <p>Convert the tensor to a specific dtype</p> <code>None</code> <p>Returns:</p> Type Description <p>Nothing</p> Source code in <code>lib/python/EasyDel/transform/easydel_transform.py</code> <pre><code>def save_ckpt(train_state, path, gather_fns=None, float_dtype=None):\n    \"\"\"\n    The save_ckpt function saves the state of a training run to disk.\n\n    :param train_state: Store the current state of the training process\n    :param path: Specify the location of the checkpoint file\n    :param gather_fns: Specify a function that will be used to convert the tensor to bytes\n    :param float_dtype: Convert the tensor to a specific dtype\n    :return: Nothing\n\n    \"\"\"\n\n    train_state = to_state_dict(train_state)\n    packer = msgpack.Packer()\n    flatten_train_state = flatten_dict(train_state)\n    if gather_fns is not None:\n        gather_fns = flatten_dict(to_state_dict(gather_fns))\n\n    with open(path, \"wb\") as stream:\n        for key, value in flatten_train_state.items():\n            if gather_fns is not None:\n                value = gather_fns[key](value)\n            value = float_tensor_to_dtype(value, float_dtype)\n            stream.write(packer.pack((key, to_bytes(value))))\n</code></pre>"},{"location":"lib-python-EasyDel-transform-falcon/","title":"transform.falcon","text":""},{"location":"lib-python-EasyDel-transform-falcon/#lib.python.EasyDel.transform.falcon.falcon_easydel_to_hf","title":"<code>falcon_easydel_to_hf(path, config)</code>","text":"<p>Takes path to easydel saved ckpt and return the model in pytorch (Transformers Huggingface)</p> Source code in <code>lib/python/EasyDel/transform/falcon.py</code> <pre><code>def falcon_easydel_to_hf(path, config: FalconConfig):\n    \"\"\"\n        Takes path to easydel saved ckpt and return the model in pytorch (Transformers Huggingface)\n    \"\"\"\n    torch_params = load_and_convert_checkpoint_to_torch(path)\n    edited_params = {}\n    for k, v in torch_params.items():\n        edited_params[k.replace('.kernel', '.weight').replace('.embedding', '.weight')] = v\n    model = FalconForCausalLM(config=config)\n    model.load_state_dict(edited_params)\n    return model\n</code></pre>"},{"location":"lib-python-EasyDel-transform-falcon/#lib.python.EasyDel.transform.falcon.falcon_from_pretrained","title":"<code>falcon_from_pretrained(model_id, device)</code>","text":"<p>return: Weight or Params for EasyDel Model , Config</p> Source code in <code>lib/python/EasyDel/transform/falcon.py</code> <pre><code>def falcon_from_pretrained(model_id, device):\n    \"\"\"\n    return: Weight or Params for EasyDel Model , Config\n    \"\"\"\n    # Requested By vwxyzjn at https://github.com/erfanzar/EasyDeL/issues/15#issue-1881044170\n    config = FalconConfig.from_pretrained(model_id)\n    model = FalconForCausalLM.from_pretrained(model_id)\n    easydel_wights = falcon_convert_pt_to_flax_7b(\n        state_dict=model.state_dict(),\n        config=config,\n        device=device\n    )\n    del model\n    gc.collect()\n    config.add_jax_args()\n    return easydel_wights, config\n</code></pre>"},{"location":"lib-python-EasyDel-transform-llama/","title":"transform.llama","text":""},{"location":"lib-python-EasyDel-transform-llama/#lib.python.EasyDel.transform.llama.llama_easydel_to_hf","title":"<code>llama_easydel_to_hf(path, config)</code>","text":"<p>Takes path to easydel saved ckpt and return the model in pytorch (Transformers Huggingface)</p> Source code in <code>lib/python/EasyDel/transform/llama.py</code> <pre><code>def llama_easydel_to_hf(path, config: LlamaConfig):\n    \"\"\"\n        Takes path to easydel saved ckpt and return the model in pytorch (Transformers Huggingface)\n    \"\"\"\n    torch_params = load_and_convert_checkpoint_to_torch(path)\n    edited_params = {}\n    for k, v in torch_params.items():\n        edited_params[k.replace('.kernel', '.weight').replace('.embedding', '.weight')] = v\n    model = LlamaForCausalLM(config=config)\n    model.load_state_dict(edited_params)\n    return model\n</code></pre>"},{"location":"lib-python-EasyDel-transform-llama/#lib.python.EasyDel.transform.llama.llama_from_pretrained","title":"<code>llama_from_pretrained(model_id, device)</code>","text":"<p>return: Weight or Params for EasyDel Model , Config</p> Source code in <code>lib/python/EasyDel/transform/llama.py</code> <pre><code>def llama_from_pretrained(model_id, device):\n    \"\"\"\n    return: Weight or Params for EasyDel Model , Config\n    \"\"\"\n    config = LlamaConfig.from_pretrained(model_id)\n    model = LlamaForCausalLM.from_pretrained(model_id)\n    easydel_wights = llama_convert_hf_to_flax(\n        state_dict=model.state_dict(),\n        config=config,\n        device=device\n    )\n    config.add_jax_args()\n\n    del model\n    gc.collect()\n    return easydel_wights, config\n</code></pre>"},{"location":"lib-python-EasyDel-transform-mistral/","title":"transform.mistral","text":""},{"location":"lib-python-EasyDel-transform-mistral/#lib.python.EasyDel.transform.mistral.mistral_easydel_to_hf","title":"<code>mistral_easydel_to_hf(path, config)</code>","text":"<p>Takes path to easydel saved ckpt and return the model in pytorch (Transformers Huggingface)</p> Source code in <code>lib/python/EasyDel/transform/mistral.py</code> <pre><code>def mistral_easydel_to_hf(path, config: MistralConfig):\n    \"\"\"\n    Takes path to easydel saved ckpt and return the model in pytorch (Transformers Huggingface)\n    \"\"\"\n    torch_params = load_and_convert_checkpoint_to_torch(path)\n    edited_params = {}\n    for k, v in torch_params.items():\n        edited_params[k.replace('.kernel', '.weight').replace('.embedding', '.weight')] = v\n    model = MistralForCausalLM(config=config)\n    model.load_state_dict(edited_params)\n    return model\n</code></pre>"},{"location":"lib-python-EasyDel-transform-mistral/#lib.python.EasyDel.transform.mistral.mistral_from_pretrained","title":"<code>mistral_from_pretrained(model_id, device)</code>","text":"<p>return: Weight or Params for EasyDel Model , Config</p> Source code in <code>lib/python/EasyDel/transform/mistral.py</code> <pre><code>def mistral_from_pretrained(model_id, device):\n    \"\"\"\n    return: Weight or Params for EasyDel Model , Config\n    \"\"\"\n    config = MistralConfig.from_pretrained(model_id)\n    config.vocab_size = 46080\n    model = MistralForCausalLM.from_pretrained(model_id)\n    model.resize_token_embeddings(46080)\n\n    easydel_wights = mistral_convert_hf_to_flax(\n        state_dict=model.state_dict(),\n        config=config,\n        device=device\n    )\n    config.add_jax_args()\n\n    del model\n    gc.collect()\n    return easydel_wights, config\n</code></pre>"},{"location":"lib-python-EasyDel-transform-mpt/","title":"transform.mpt","text":""},{"location":"lib-python-EasyDel-transform-mpt/#lib.python.EasyDel.transform.mpt.mpt_from_pretrained","title":"<code>mpt_from_pretrained(model_id, device, **kwargs)</code>","text":"<p>return: Weight or Params for EasyDel Model , Config</p> Source code in <code>lib/python/EasyDel/transform/mpt.py</code> <pre><code>def mpt_from_pretrained(model_id, device, **kwargs):\n    \"\"\"\n    return: Weight or Params for EasyDel Model , Config\n    \"\"\"\n    config = MptConfig.from_pretrained(model_id)\n    model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, **kwargs)\n\n    easydel_wights = mpt_convert_pt_to_flax_7b(\n        state_dict=model.state_dict(),\n        n_layers=config.num_hidden_layers if hasattr(config, 'num_hidden_layers') else config.n_layers,\n        device=device\n    )\n    config.add_jax_args()\n\n    del model\n    gc.collect()\n    return easydel_wights, config\n</code></pre>"},{"location":"lib-python-EasyDel-transform-utils/","title":"transform.utils","text":""},{"location":"lib-python-EasyDel-utils-checker/","title":"utils.checker","text":""},{"location":"lib-python-EasyDel-utils-prompters/","title":"utils.prompters","text":""},{"location":"lib-python-EasyDel-utils-prompters/#lib.python.EasyDel.utils.prompters.antitoxin_prompter","title":"<code>antitoxin_prompter(history, prompt, system=None)</code>","text":"<p>The antitoxin_prompter function takes in a history of user-assistant interactions, a prompt from the user, and optionally a system response. It returns an input string that can be fed into the antitoxin model to generate an assistant response.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>List[str]</code> <p>typing.List[str]: Pass in the history of the conversation</p> required <code>prompt</code> <code>str</code> <p>str: Pass the user's input to the assistant</p> required <code>system</code> <code>Optional[str]</code> <p>typing.Optional[str]: Pass the system's response to the prompt</p> <code>None</code> <code></code> <p>Store the history of user and assistant interaction</p> required <p>Returns:</p> Type Description <p>A string that contains the user's prompt,</p> Source code in <code>lib/python/EasyDel/utils/prompters.py</code> <pre><code>def antitoxin_prompter(\n        history: typing.List[str],\n        prompt: str,\n        system: typing.Optional[str] = None,\n):\n    \"\"\"\n    The antitoxin_prompter function takes in a history of user-assistant interactions,\n    a prompt from the user, and optionally a system response. It returns an input string\n    that can be fed into the antitoxin model to generate an assistant response.\n\n    :param history: typing.List[str]: Pass in the history of the conversation\n    :param prompt: str: Pass the user's input to the assistant\n    :param system: typing.Optional[str]: Pass the system's response to the prompt\n    :param : Store the history of user and assistant interaction\n    :return: A string that contains the user's prompt,\n\n    \"\"\"\n    sys_str = f\"&lt;|im_start|&gt;system\\n{system}&lt;|im_end|&gt;\\n\" if system is not None else \"\"\n    histories = ''\n    for user, assistance in history:\n        histories += f\"&lt;|im_start|&gt;user\\n{user}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n{assistance}&lt;|im_end|&gt;\\n\"\n    text = f\"&lt;|im_start|&gt;user\\n{prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n\"\n    return sys_str + histories + text\n</code></pre>"},{"location":"lib-python-EasyDel-utils-prompters/#lib.python.EasyDel.utils.prompters.antitoxin_prompter_chat_format","title":"<code>antitoxin_prompter_chat_format(history, system=None)</code>","text":"<p>The antitoxin_prompter_chat_format function takes a list of strings and returns a string. The input is the history of the chat, which is a list of tuples where each tuple contains two strings: the user's message and the assistant's response. The output is formatted as follows:</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>List[str]</code> <p>typing.List[str]: Pass in the history of user and assistant messages</p> required <code>system</code> <code>Optional[str]</code> <p>typing.Optional[str]: Pass in the system message</p> <code>None</code> <code></code> <p>Store the history of the conversation</p> required <p>Returns:</p> Type Description <p>A string that contains the system message and</p> Source code in <code>lib/python/EasyDel/utils/prompters.py</code> <pre><code>def antitoxin_prompter_chat_format(\n        history: typing.List[str],\n        system: typing.Optional[str] = None,\n):\n    \"\"\"\n    The antitoxin_prompter_chat_format function takes a list of strings and returns a string.\n    The input is the history of the chat, which is a list of tuples where each tuple contains two strings:\n    the user's message and the assistant's response. The output is formatted as follows:\n\n    :param history: typing.List[str]: Pass in the history of user and assistant messages\n    :param system: typing.Optional[str]: Pass in the system message\n    :param : Store the history of the conversation\n    :return: A string that contains the system message and\n\n    \"\"\"\n    sys_str = f\"&lt;|im_start|&gt;system\\n{system}&lt;|im_end|&gt;\\n\" if system is not None else \"\"\n    histories = ''\n    for user, assistance in history:\n        histories += f\"&lt;|im_start|&gt;user\\n{user}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n{assistance}&lt;|im_end|&gt;\\n\"\n    return sys_str + histories\n</code></pre>"},{"location":"lib-python-EasyDel-utils-prompters/#lib.python.EasyDel.utils.prompters.llama2_prompter","title":"<code>llama2_prompter(history, prompt, system=None)</code>","text":"<p>The llama2_prompter function takes a history of user-system interactions, a prompt for the next system response, and optionally a system response. It returns an LLAMA2 formatted string that can be used as input to the LLAMA2 model.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>List[str]</code> <p>typing.List[str]: Store the history of user input and system response</p> required <code>prompt</code> <code>str</code> <p>str: Specify the prompt to be displayed</p> required <code>system</code> <code>Optional[str]</code> <p>typing.Optional[str]: Indicate that the system is optional</p> <code>None</code> <code></code> <p>Specify the system's response</p> required <p>Returns:</p> Type Description <p>A string that is a concatenation of the</p> Source code in <code>lib/python/EasyDel/utils/prompters.py</code> <pre><code>def llama2_prompter(\n        history: typing.List[str],\n        prompt: str,\n        system: typing.Optional[str] = None,\n\n):\n    \"\"\"\n    The llama2_prompter function takes a history of user-system interactions,\n    a prompt for the next system response, and optionally a system response.\n    It returns an LLAMA2 formatted string that can be used as input to the LLAMA2 model.\n\n    :param history: typing.List[str]: Store the history of user input and system response\n    :param prompt: str: Specify the prompt to be displayed\n    :param system: typing.Optional[str]: Indicate that the system is optional\n    :param : Specify the system's response\n    :return: A string that is a concatenation of the\n\n    \"\"\"\n    do_strip = False\n    if system is not None:\n        texts = [f'&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\n{system}\\n&lt;&lt;/SYS&gt;&gt;\\n\\n']\n    else:\n        texts = [f'&lt;s&gt;[INST] ']\n    for user_input, response in history:\n        user_input = user_input.strip() if do_strip else user_input\n        do_strip = True\n        texts.append(f'{user_input} [/INST] {response.strip()} &lt;/s&gt;&lt;s&gt;[INST] ')\n    prompt = prompt.strip() if do_strip else prompt\n    texts.append(f'{prompt} [/INST]')\n    return ''.join(texts)\n</code></pre>"},{"location":"lib-python-EasyDel-utils-prompters/#lib.python.EasyDel.utils.prompters.llama2_prompter_chat_format","title":"<code>llama2_prompter_chat_format(system, messages)</code>","text":"<p>The llama2_prompter_chat_format function takes a system message and a list of messages, and returns the formatted string that can be used to create an LLAMA2 chat file. The system message is optional, and if it is not provided then the function will return only the user messages. The user messages are expected to be in pairs: one for each speaker (system or human).  The first element of each  pair should be the name of that speaker.</p> <p>Parameters:</p> Name Type Description Default <code>system</code> <code>str</code> <p>str: Store the system message</p> required <code>messages</code> <code>List[str]</code> <p>typing.List[str]: Pass in a list of strings</p> required <code></code> <p>Add the system message to the beginning of the chat</p> required <p>Returns:</p> Type Description <p>A string that is the</p> Source code in <code>lib/python/EasyDel/utils/prompters.py</code> <pre><code>def llama2_prompter_chat_format(\n        system: str,\n        messages: typing.List[str],\n):\n    \"\"\"\n    The llama2_prompter_chat_format function takes a system message and a list of messages,\n    and returns the formatted string that can be used to create an LLAMA2 chat file.\n    The system message is optional, and if it is not provided then the function will return only the user messages.\n    The user messages are expected to be in pairs: one for each speaker (system or human).  The first element of each\n     pair should be the name of that speaker.\n\n    :param system: str: Store the system message\n    :param messages: typing.List[str]: Pass in a list of strings\n    :param : Add the system message to the beginning of the chat\n    :return: A string that is the\n\n    \"\"\"\n    if system is not None:\n        string = [f'&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\n{system}\\n&lt;&lt;/SYS&gt;&gt;\\n\\n']\n    else:\n        string = [f'&lt;s&gt;[INST] ']\n    for index in range(0, len(messages), 2):\n        string.append(\n            f'{messages[index]} [/INST] {messages[index + 1].strip()} &lt;/s&gt;&lt;s&gt;[INST] ')\n    return ''.join(string).strip()\n</code></pre>"},{"location":"lib-python-EasyDel-utils-tensor_utils/","title":"utils.tensor_utils","text":""},{"location":"lib-python-EasyDel-utils-tensor_utils/#lib.python.EasyDel.utils.tensor_utils.np2jax","title":"<code>np2jax(array)</code>","text":"<p>Convert Numpy Array to JAX Array</p> Source code in <code>lib/python/EasyDel/utils/tensor_utils.py</code> <pre><code>def np2jax(array: np.array) -&gt; chex.Array:\n    \"\"\"\n        Convert Numpy Array to JAX Array\n        \"\"\"\n    return jnp.asarray(array)\n</code></pre>"},{"location":"lib-python-EasyDel-utils-tensor_utils/#lib.python.EasyDel.utils.tensor_utils.pt2jax","title":"<code>pt2jax(array)</code>","text":"<p>Convert Pytorch Array to JAX Array</p> Source code in <code>lib/python/EasyDel/utils/tensor_utils.py</code> <pre><code>def pt2jax(array: torch.Tensor) -&gt; chex.Array:\n    \"\"\"\n    Convert Pytorch Array to JAX Array\n    \"\"\"\n    return np2jax(pt2np(array))\n</code></pre>"},{"location":"lib-python-EasyDel-utils-tensor_utils/#lib.python.EasyDel.utils.tensor_utils.pt2np","title":"<code>pt2np(array)</code>","text":"<p>Convert Pytorch Array to Numpy Array</p> Source code in <code>lib/python/EasyDel/utils/tensor_utils.py</code> <pre><code>def pt2np(array: torch.Tensor) -&gt; np.array:\n    \"\"\"\n        Convert Pytorch Array to Numpy Array\n        \"\"\"\n    return array.detach().cpu().numpy()\n</code></pre>"},{"location":"lib-python-EasyDel-utils-utils/","title":"utils.utils","text":""},{"location":"lib-python-EasyDel-utils-utils/#lib.python.EasyDel.utils.utils.Timer","title":"<code>Timer</code>","text":"Source code in <code>lib/python/EasyDel/utils/utils.py</code> <pre><code>class Timer:\n\n    def __init__(self, name):\n        \"\"\"\n        The __init__ function is called when the class is instantiated.\n        It sets up the object with a name and initializes other variables.\n\n        :param self: Represent the instance of the class\n        :param name: Give the timer a name\n        :return: An instance of the class\n\n        \"\"\"\n        self.name_ = name\n        self.elapsed_ = 0.0\n        self.started_ = False\n        self.start_time = time.time()\n\n    def start(self):\n        \"\"\"\n        The start function starts the timer.\n                Args:\n                    None\n\n        :param self: Access the attributes and methods of the class in python\n        :return: Nothing\n\n        \"\"\"\n        assert not self.started_, \"timer has already been started\"\n        self.start_time = time.time()\n        self.started_ = True\n\n    def stop(self):\n        \"\"\"\n        The stop function stops the timer and adds the time elapsed since start was called to the total elapsed time.\n\n\n        :param self: Represent the instance of the class\n        :return: The time elapsed since the start function was called\n\n        \"\"\"\n        assert self.started_, \"timer is not started\"\n        self.elapsed_ += time.time() - self.start_time\n        self.started_ = False\n\n    def reset(self):\n        \"\"\"\n        The reset function sets the elapsed time to 0.0 and the started flag to False.\n\n        :param self: Represent the instance of the class\n        :return: True if the timer was running, false otherwise\n\n        \"\"\"\n        self.elapsed_ = 0.0\n        self.started_ = False\n\n    def elapsed(self, reset=True):\n        \"\"\"\n        The elapsed function returns the elapsed time in seconds since the timer was started.\n        If reset is True, then it also resets the timer to zero and restarts it.\n        If reset is False, then it leaves the timer running.\n\n        :param self: Represent the instance of the class\n        :param reset: Reset the timer\n        :return: The elapsed time in seconds\n\n        \"\"\"\n        started_ = self.started_\n        if self.started_:\n            self.stop()\n        elapsed_ = self.elapsed_\n        if reset:\n            self.reset()\n        if started_:\n            self.start()\n        return elapsed_\n</code></pre>"},{"location":"lib-python-EasyDel-utils-utils/#lib.python.EasyDel.utils.utils.Timer.__init__","title":"<code>__init__(name)</code>","text":"<p>The init function is called when the class is instantiated. It sets up the object with a name and initializes other variables.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>name</code> <p>Give the timer a name</p> required <p>Returns:</p> Type Description <p>An instance of the class</p> Source code in <code>lib/python/EasyDel/utils/utils.py</code> <pre><code>def __init__(self, name):\n    \"\"\"\n    The __init__ function is called when the class is instantiated.\n    It sets up the object with a name and initializes other variables.\n\n    :param self: Represent the instance of the class\n    :param name: Give the timer a name\n    :return: An instance of the class\n\n    \"\"\"\n    self.name_ = name\n    self.elapsed_ = 0.0\n    self.started_ = False\n    self.start_time = time.time()\n</code></pre>"},{"location":"lib-python-EasyDel-utils-utils/#lib.python.EasyDel.utils.utils.Timer.elapsed","title":"<code>elapsed(reset=True)</code>","text":"<p>The elapsed function returns the elapsed time in seconds since the timer was started. If reset is True, then it also resets the timer to zero and restarts it. If reset is False, then it leaves the timer running.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>reset</code> <p>Reset the timer</p> <code>True</code> <p>Returns:</p> Type Description <p>The elapsed time in seconds</p> Source code in <code>lib/python/EasyDel/utils/utils.py</code> <pre><code>def elapsed(self, reset=True):\n    \"\"\"\n    The elapsed function returns the elapsed time in seconds since the timer was started.\n    If reset is True, then it also resets the timer to zero and restarts it.\n    If reset is False, then it leaves the timer running.\n\n    :param self: Represent the instance of the class\n    :param reset: Reset the timer\n    :return: The elapsed time in seconds\n\n    \"\"\"\n    started_ = self.started_\n    if self.started_:\n        self.stop()\n    elapsed_ = self.elapsed_\n    if reset:\n        self.reset()\n    if started_:\n        self.start()\n    return elapsed_\n</code></pre>"},{"location":"lib-python-EasyDel-utils-utils/#lib.python.EasyDel.utils.utils.Timer.reset","title":"<code>reset()</code>","text":"<p>The reset function sets the elapsed time to 0.0 and the started flag to False.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>True if the timer was running, false otherwise</p> Source code in <code>lib/python/EasyDel/utils/utils.py</code> <pre><code>def reset(self):\n    \"\"\"\n    The reset function sets the elapsed time to 0.0 and the started flag to False.\n\n    :param self: Represent the instance of the class\n    :return: True if the timer was running, false otherwise\n\n    \"\"\"\n    self.elapsed_ = 0.0\n    self.started_ = False\n</code></pre>"},{"location":"lib-python-EasyDel-utils-utils/#lib.python.EasyDel.utils.utils.Timer.start","title":"<code>start()</code>","text":"<p>The start function starts the timer.         Args:             None</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Access the attributes and methods of the class in python</p> required <p>Returns:</p> Type Description <p>Nothing</p> Source code in <code>lib/python/EasyDel/utils/utils.py</code> <pre><code>def start(self):\n    \"\"\"\n    The start function starts the timer.\n            Args:\n                None\n\n    :param self: Access the attributes and methods of the class in python\n    :return: Nothing\n\n    \"\"\"\n    assert not self.started_, \"timer has already been started\"\n    self.start_time = time.time()\n    self.started_ = True\n</code></pre>"},{"location":"lib-python-EasyDel-utils-utils/#lib.python.EasyDel.utils.utils.Timer.stop","title":"<code>stop()</code>","text":"<p>The stop function stops the timer and adds the time elapsed since start was called to the total elapsed time.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <p>Returns:</p> Type Description <p>The time elapsed since the start function was called</p> Source code in <code>lib/python/EasyDel/utils/utils.py</code> <pre><code>def stop(self):\n    \"\"\"\n    The stop function stops the timer and adds the time elapsed since start was called to the total elapsed time.\n\n\n    :param self: Represent the instance of the class\n    :return: The time elapsed since the start function was called\n\n    \"\"\"\n    assert self.started_, \"timer is not started\"\n    self.elapsed_ += time.time() - self.start_time\n    self.started_ = False\n</code></pre>"},{"location":"lib-python-EasyDel-utils-utils/#lib.python.EasyDel.utils.utils.Timers","title":"<code>Timers</code>","text":"<p>Group of timers.</p> Source code in <code>lib/python/EasyDel/utils/utils.py</code> <pre><code>class Timers:\n    \"\"\"Group of timers.\"\"\"\n\n    def __init__(self, use_wandb, tensorboard_writer):\n        self.timers = {}\n        self.use_wandb = use_wandb\n        self.tensorboard_writer = tensorboard_writer\n\n    def __call__(self, name):\n        if name not in self.timers:\n            self.timers[name] = Timer(name)\n        return self.timers[name]\n\n    def write(self, names, iteration, normalizer=1.0, reset=False):\n\n        \"\"\"\n        The write function is used to write the elapsed time of a timer to Tensorboard and/or Weights &amp;amp; Biases.\n\n        :param self: Make the function a method of the class\n        :param names: Specify which timer(s) to write\n        :param iteration: Keep track of the number of iterations\n        :param normalizer: Normalize the time elapsed by a certain value\n        :param reset: Reset the timer after it has been written to tensorboard\n        :return: Nothing\n\n        \"\"\"\n        assert normalizer &gt; 0.0\n        for name in names:\n            value = self.timers[name].elapsed(reset=reset) / normalizer\n\n            if self.tensorboard_writer:\n                self.tensorboard_writer.add_scalar(f\"timers/{name}\", value, iteration)\n\n            if self.use_wandb:\n                wandb.log({f\"timers/{name}\": value}, step=iteration)\n\n    def log(self, names, normalizer=1.0, reset=True):\n        \"\"\"\n        The log function is used to print the time elapsed for a given function.\n\n        :param self: Represent the instance of the class\n        :param names: Specify the name of the timer that we want to log\n        :param normalizer: Normalize the time taken to run a function\n        :param reset: Reset the timer after logging\n        :return: The time taken for the given name\n\n        \"\"\"\n        assert normalizer &gt; 0.0\n\n        if isinstance(names, str):\n            names = [names]\n        for name in names:\n            elapsed_time = self.timers[name].elapsed(reset=reset) * 1000.0 / normalizer\n            string = prefix_str(f'Time For {name} (ms)', elapsed_time)\n\n            print(string, flush=True)\n</code></pre>"},{"location":"lib-python-EasyDel-utils-utils/#lib.python.EasyDel.utils.utils.Timers.log","title":"<code>log(names, normalizer=1.0, reset=True)</code>","text":"<p>The log function is used to print the time elapsed for a given function.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Represent the instance of the class</p> required <code>names</code> <p>Specify the name of the timer that we want to log</p> required <code>normalizer</code> <p>Normalize the time taken to run a function</p> <code>1.0</code> <code>reset</code> <p>Reset the timer after logging</p> <code>True</code> <p>Returns:</p> Type Description <p>The time taken for the given name</p> Source code in <code>lib/python/EasyDel/utils/utils.py</code> <pre><code>def log(self, names, normalizer=1.0, reset=True):\n    \"\"\"\n    The log function is used to print the time elapsed for a given function.\n\n    :param self: Represent the instance of the class\n    :param names: Specify the name of the timer that we want to log\n    :param normalizer: Normalize the time taken to run a function\n    :param reset: Reset the timer after logging\n    :return: The time taken for the given name\n\n    \"\"\"\n    assert normalizer &gt; 0.0\n\n    if isinstance(names, str):\n        names = [names]\n    for name in names:\n        elapsed_time = self.timers[name].elapsed(reset=reset) * 1000.0 / normalizer\n        string = prefix_str(f'Time For {name} (ms)', elapsed_time)\n\n        print(string, flush=True)\n</code></pre>"},{"location":"lib-python-EasyDel-utils-utils/#lib.python.EasyDel.utils.utils.Timers.write","title":"<code>write(names, iteration, normalizer=1.0, reset=False)</code>","text":"<p>The write function is used to write the elapsed time of a timer to Tensorboard and/or Weights &amp; Biases.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <p>Make the function a method of the class</p> required <code>names</code> <p>Specify which timer(s) to write</p> required <code>iteration</code> <p>Keep track of the number of iterations</p> required <code>normalizer</code> <p>Normalize the time elapsed by a certain value</p> <code>1.0</code> <code>reset</code> <p>Reset the timer after it has been written to tensorboard</p> <code>False</code> <p>Returns:</p> Type Description <p>Nothing</p> Source code in <code>lib/python/EasyDel/utils/utils.py</code> <pre><code>def write(self, names, iteration, normalizer=1.0, reset=False):\n\n    \"\"\"\n    The write function is used to write the elapsed time of a timer to Tensorboard and/or Weights &amp;amp; Biases.\n\n    :param self: Make the function a method of the class\n    :param names: Specify which timer(s) to write\n    :param iteration: Keep track of the number of iterations\n    :param normalizer: Normalize the time elapsed by a certain value\n    :param reset: Reset the timer after it has been written to tensorboard\n    :return: Nothing\n\n    \"\"\"\n    assert normalizer &gt; 0.0\n    for name in names:\n        value = self.timers[name].elapsed(reset=reset) / normalizer\n\n        if self.tensorboard_writer:\n            self.tensorboard_writer.add_scalar(f\"timers/{name}\", value, iteration)\n\n        if self.use_wandb:\n            wandb.log({f\"timers/{name}\": value}, step=iteration)\n</code></pre>"},{"location":"lib-python-EasyDel-utils-utils/#lib.python.EasyDel.utils.utils.get_mesh","title":"<code>get_mesh(shape=(1, -1, 1, 1), axis_names=('dp', 'fsdp', 'tp', 'mp'))</code>","text":"<p>The get_mesh function is a helper function that creates a JAX Mesh object.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>Sequence[int]</code> <p>typing.Sequence[int]: Specify the shape of the array that is used to create the mesh</p> <code>(1, -1, 1, 1)</code> <code>axis_names</code> <code>Sequence[str]</code> <p>typing.Sequence[int]: Specify the Axis Names in mesh</p> <code>('dp', 'fsdp', 'tp', 'mp')</code> <p>Returns:</p> Type Description <p>A mesh object</p> Source code in <code>lib/python/EasyDel/utils/utils.py</code> <pre><code>def get_mesh(\n        shape: typing.Sequence[int] = (1, -1, 1, 1),\n        axis_names: typing.Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"mp\")\n):\n    \"\"\"\n    The get_mesh function is a helper function that creates a JAX Mesh object.\n\n    :param shape: typing.Sequence[int]: Specify the shape of the array that is used to create the mesh\n    :param axis_names: typing.Sequence[int]: Specify the Axis Names in mesh\n    :return: A mesh object\n\n    \"\"\"\n    from jax.sharding import Mesh\n    from jax.experimental import mesh_utils\n    array = jnp.ones((len(jax.devices()), 1)).reshape(shape)\n    return Mesh(mesh_utils.create_device_mesh(array.shape), axis_names)\n</code></pre>"},{"location":"lib-python-EasyDel-utils-utils/#lib.python.EasyDel.utils.utils.get_names_from_partition_spec","title":"<code>get_names_from_partition_spec(partition_specs)</code>","text":"<p>The get_names_from_partition_spec function takes a partition_specs argument, which is either a dictionary or list. If it's a dictionary, the function converts it to a list of values. Then for each item in the partition_specs list:     If the item is None, continue (do nothing) and move on to next iteration of loop.     If the item is an instance of str (i.e., if it's just one string), add that string to names set and move on      to next iteration of loop.     Otherwise, (if not None or str), call get_names_from_partition_spec recurs</p> <p>Parameters:</p> Name Type Description Default <code>partition_specs</code> <p>Specify the partitioning of a table</p> required <p>Returns:</p> Type Description <p>A list of names from a partition spec</p> Source code in <code>lib/python/EasyDel/utils/utils.py</code> <pre><code>def get_names_from_partition_spec(partition_specs):\n    \"\"\"\n    The get_names_from_partition_spec function takes a partition_specs argument, which is either a dictionary or list.\n    If it's a dictionary, the function converts it to a list of values. Then for each item in the partition_specs list:\n        If the item is None, continue (do nothing) and move on to next iteration of loop.\n        If the item is an instance of str (i.e., if it's just one string), add that string to names set and move on \n        to next iteration of loop.\n        Otherwise, (if not None or str), call get_names_from_partition_spec recurs\n\n    :param partition_specs: Specify the partitioning of a table\n    :return: A list of names from a partition spec\n\n    \"\"\"\n    names = set()\n    if isinstance(partition_specs, dict):\n        partition_specs = partition_specs.values()\n    for item in partition_specs:\n        if item is None:\n            continue\n        elif isinstance(item, str):\n            names.add(item)\n        else:\n            names.update(get_names_from_partition_spec(item))\n\n    return list(names)\n</code></pre>"},{"location":"lib-python-EasyDel-utils-utils/#lib.python.EasyDel.utils.utils.make_shard_and_gather_fns","title":"<code>make_shard_and_gather_fns(partition_specs, dtype_specs=None)</code>","text":"<p>The make_shard_and_gather_fns function takes in a partition_specs and dtype_specs, and returns two functions: shard_fns and gather_fns. The shard function is used to shard the input tensor into the specified partitions, while the gather function is used to gather all of those shards back together. This allows us to use different data types for each partition (e.g., float16 for weights, float32 for activations)</p> <p>Parameters:</p> Name Type Description Default <code>partition_specs</code> <p>Specify the partitioning of each tensor in the model</p> required <code>dtype_specs</code> <p>Specify the dtype of the tensor</p> <code>None</code> <p>Returns:</p> Type Description <p>A tuple of two functions:</p> Source code in <code>lib/python/EasyDel/utils/utils.py</code> <pre><code>def make_shard_and_gather_fns(partition_specs, dtype_specs=None):\n    \"\"\"\n    The make_shard_and_gather_fns function takes in a partition_specs and dtype_specs,\n    and returns two functions: shard_fns and gather_fns. The shard function is used to\n    shard the input tensor into the specified partitions, while the gather function is used to\n    gather all of those shards back together. This allows us to use different data types for each\n    partition (e.g., float16 for weights, float32 for activations)\n\n    :param partition_specs: Specify the partitioning of each tensor in the model\n    :param dtype_specs: Specify the dtype of the tensor\n    :return: A tuple of two functions:\n\n    \"\"\"\n    float_dtypes = (jnp.bfloat16, jnp.float16, jnp.float32, jnp.float64)\n\n    def make_to_dtype_fn(dtype_spec):\n        def to_dtype(tensor):\n            if dtype_specs in float_dtypes and getattr(tensor, 'dtype', None) in float_dtypes:\n                return tensor.astype(dtype_specs)\n            elif hasattr(dtype_spec, 'dtype') and hasattr(tensor, 'dtype'):\n                return tensor.astype(dtype_spec.dtype)\n            return tensor\n\n        return to_dtype\n\n    def make_shard_fn(partition_spec, dtype_spec=None):\n        jax_shard_function = pjit(\n            make_to_dtype_fn(dtype_spec),\n            in_shardings=None,\n            out_shardings=partition_spec\n        )\n\n        def shard_fn(tensor):\n            return jax_shard_function(tensor).block_until_ready()\n\n        return shard_fn\n\n    def make_gather_fn(partition_spec, dtype_spec=None):\n        jax_gather_fn = pjit(\n            make_to_dtype_fn(dtype_spec),\n            in_shardings=partition_spec,\n            out_shardings=None\n        )\n\n        def gather_fn(tensor):\n            return jax.device_get(jax_gather_fn(tensor))\n\n        return gather_fn\n\n    if dtype_specs is None or dtype_specs in float_dtypes:\n        shard_fns = jax.tree_util.tree_map(make_shard_fn, partition_specs)\n        gather_fns = jax.tree_util.tree_map(make_gather_fn, partition_specs)\n    else:\n        shard_fns = jax.tree_util.tree_map(\n            make_shard_fn, partition_specs, dtype_specs\n        )\n        gather_fns = jax.tree_util.tree_map(\n            make_gather_fn, partition_specs, dtype_specs\n        )\n    return shard_fns, gather_fns\n</code></pre>"},{"location":"lib-python-EasyDel-utils-utils/#lib.python.EasyDel.utils.utils.names_in_mesh","title":"<code>names_in_mesh(*names)</code>","text":"<p>The names_in_mesh function is a decorator that can be used to check whether the names of the axes passed into a function are valid.  It will raise an exception if any of the axis names are not in the physical mesh.  For example, if you have a function that takes two axes as arguments, and you want to make sure they're both in your mesh:</p> <p>Parameters:</p> Name Type Description Default <code>*names</code> <p>Pass in a variable number of arguments</p> <code>()</code> <p>Returns:</p> Type Description <p>A boolean indicating whether all of the given names are in the physical mesh</p> Source code in <code>lib/python/EasyDel/utils/utils.py</code> <pre><code>def names_in_mesh(*names):\n    \"\"\"\n    The names_in_mesh function is a decorator that can be used to check whether\n    the names of the axes passed into a function are valid.  It will raise an\n    exception if any of the axis names are not in the physical mesh.  For example,\n    if you have a function that takes two axes as arguments, and you want to make sure they're both in your mesh:\n\n    :param *names: Pass in a variable number of arguments\n    :return: A boolean indicating whether all of the given names are in the physical mesh\n\n    \"\"\"\n    return set(names) &lt;= set(pxla.thread_resources.env.physical_mesh.axis_names)\n</code></pre>"},{"location":"lib-python-EasyDel-utils-utils/#lib.python.EasyDel.utils.utils.with_sharding_constraint","title":"<code>with_sharding_constraint(x, partition_specs)</code>","text":"<p>The with_sharding_constraint function is used to ensure that the sharding of a tensor is consistent with the sharding of its inputs.  This function should be called on any tensor which has been created by an operation which does not automatically handle this, such as tf.concat or tf.split.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Pass in the tensor that is to be sharded</p> required <code>partition_specs</code> <p>Specify the axis names and partition sizes</p> required <p>Returns:</p> Type Description <p>The same value as the original function</p> Source code in <code>lib/python/EasyDel/utils/utils.py</code> <pre><code>def with_sharding_constraint(x, partition_specs):\n    \"\"\"\n    The with_sharding_constraint function is used to ensure that the sharding of a tensor\n    is consistent with the sharding of its inputs.  This function should be called on any\n    tensor which has been created by an operation which does not automatically handle this,\n    such as tf.concat or tf.split.\n\n    :param x: Pass in the tensor that is to be sharded\n    :param partition_specs: Specify the axis names and partition sizes\n    :return: The same value as the original function\n\n    \"\"\"\n    axis_names = get_names_from_partition_spec(partition_specs)\n    if names_in_mesh(*axis_names):\n        x = wsc(x, partition_specs)\n    return x\n</code></pre>"}]}